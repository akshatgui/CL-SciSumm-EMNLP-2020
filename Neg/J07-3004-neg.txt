For the identification and labeling steps, we train a maximum entropy classifier (Berger et al, 1996) over sections 02-21 of a version of the CCGbank corpus (Hockenmaier and Steedman, 2007) that has been augmented by projecting the Propbank semantic annotations (Boxwell and White, 2008). $$$$$ The missing entries that would be required for the remaining 6% of tokens fall into two classes: 1,728, or 3.8%, correspond to completely unknown words that do not appear at all in section 02–21, whereas the other 2.2% of tokens do appear in the training set, but not with the categories required in section 00.
For the identification and labeling steps, we train a maximum entropy classifier (Berger et al, 1996) over sections 02-21 of a version of the CCGbank corpus (Hockenmaier and Steedman, 2007) that has been augmented by projecting the Propbank semantic annotations (Boxwell and White, 2008). $$$$$ The lexicon extracted from sections 02–21 contains the necessary categories (as determined by our translation algorithm) for 94.0% of all tokens in section 00 (42,707 out of 45,422).
For the identification and labeling steps, we train a maximum entropy classifier (Berger et al, 1996) over sections 02-21 of a version of the CCGbank corpus (Hockenmaier and Steedman, 2007) that has been augmented by projecting the Propbank semantic annotations (Boxwell and White, 2008). $$$$$ Their treatment is discussed in Sections 6.2 and 6.3.
For the identification and labeling steps, we train a maximum entropy classifier (Berger et al, 1996) over sections 02-21 of a version of the CCGbank corpus (Hockenmaier and Steedman, 2007) that has been augmented by projecting the Propbank semantic annotations (Boxwell and White, 2008). $$$$$ This is especially true for formalisms with an extended domain of locality, such as TAG or CCG, where a single elementary tree or lexical category may contain information that is distributed over a number of distinct phrase-structure rules.

We used the CCGbank-style dependency output of the parser (Hockenmaier and Steedman,2007), which is a directed graph of head-child relations labelled with the head's lexical category and the argument slot filled by the child. $$$$$ The Wall Street Journal subcorpus of the Penn Treebank contains about 50,000 sentences, or 1 million words, annotated with part-of-speech tags and phrase-structure trees: These trees are relatively flat: modals and auxiliaries introduce a new VP level, whereas verb modifiers and arguments typically appear all at the same level, as sisters of the main verb.
We used the CCGbank-style dependency output of the parser (Hockenmaier and Steedman,2007), which is a directed graph of head-child relations labelled with the head's lexical category and the argument slot filled by the child. $$$$$ In the Treebank, the surface subject of a passive sentence is coindexed with a ∗ null element in direct object position: Our translation algorithm uses the presence of the ∗ null element to identify passive mode, but ignores it otherwise, assigning the CCG category S[pss]\NP to noted.13 The dependency between the subject and the participial is mediated through the lexical category of the copula, (S[dcl]\NPi)/(S[pss]\NPi) (with the standard semantics apax.px).14 In order to reduce lexical ambiguity and deal with data sparseness, we treat optional by-PPs which contain the “logical” subject (NP-LGS) as adjuncts rather than arguments of the passive participle.15 Here is the resulting CCG derivation, together with its dependency structure: 13 In the case of verbs like pay for, which take a PP argument, the null element appears within the PP.
We used the CCGbank-style dependency output of the parser (Hockenmaier and Steedman,2007), which is a directed graph of head-child relations labelled with the head's lexical category and the argument slot filled by the child. $$$$$ The resulting corpus, CCGbank, includes 99.4% of the sentences in the Penn Treebank.

CCGbank (Hockenmaier and Steedman, 2007) is a corpus of CCG derivations that was semiautomatically converted from the Wall Street Journal section of the Penn treebank. $$$$$ In order to obtain linguistically adequate CCG analyses, and to eliminate noise and inconsistencies in the original annotation, an extensive analysis of the constructions and annotations in the Penn Treebank was called for, and a substantial number of changes to the Treebank were necessary.
CCGbank (Hockenmaier and Steedman, 2007) is a corpus of CCG derivations that was semiautomatically converted from the Wall Street Journal section of the Penn treebank. $$$$$ English transitive verbs have the category (S\NP)/NP: they take an (object) NP to their right to yield a verb phrase (S\NP), which in turn takes a (subject) NP to its left to form a sentence S. Each syntactic category also has a corresponding semantic interpretation (here given as a A-expression).
CCGbank (Hockenmaier and Steedman, 2007) is a corpus of CCG derivations that was semiautomatically converted from the Wall Street Journal section of the Penn treebank. $$$$$ Compound nouns are often inherently ambiguous, and in most cases, the Treebank does not specify their internal structure: In order to obtain the correct analysis, manual re-annotation would be required.
CCGbank (Hockenmaier and Steedman, 2007) is a corpus of CCG derivations that was semiautomatically converted from the Wall Street Journal section of the Penn treebank. $$$$$ It is available from the Linguistic Data Consortium, and has been used to train widecoverage statistical parsers that obtain state-of-the-art rates of dependency recovery.

For example, declarative sentences are S [dcl], wh-questions are S [wq] and sentence fragments are S [frg] (Hockenmaier and Steedman, 2007). $$$$$ This requires a syntactic representation which is transparent to the underlying semantics, making the local and long-range dependencies between heads, arguments, and modifiers explicit.
For example, declarative sentences are S [dcl], wh-questions are S [wq] and sentence fragments are S [frg] (Hockenmaier and Steedman, 2007). $$$$$ There are 1,286 lexical category types in sections 02–21.
For example, declarative sentences are S [dcl], wh-questions are S [wq] and sentence fragments are S [frg] (Hockenmaier and Steedman, 2007). $$$$$ Typical examples of rare but correct and necessary categories are relative pronouns in pied-piping constructions, or verbs which take expletive subjects.
For example, declarative sentences are S [dcl], wh-questions are S [wq] and sentence fragments are S [frg] (Hockenmaier and Steedman, 2007). $$$$$ This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word–word dependencies.

Bos et al (2009) created a CCGbank from an Italian dependency tree bank by converting dependency trees into phrase structure trees and then applying an algorithm similar to Hockenmaier and Steedman (2007). $$$$$ We discuss the implications of our findings for the extraction of other linguistically expressive grammars from the Treebank, and for the design offuture treebanks.
Bos et al (2009) created a CCGbank from an Italian dependency tree bank by converting dependency trees into phrase structure trees and then applying an algorithm similar to Hockenmaier and Steedman (2007). $$$$$ Details are in the CCGbank manual.
Bos et al (2009) created a CCGbank from an Italian dependency tree bank by converting dependency trees into phrase structure trees and then applying an algorithm similar to Hockenmaier and Steedman (2007). $$$$$ For type-logical grammars (Moortgat 1988; Morrill 1994; Moot 2003), this is little more than a matter of transducing the syntactic types for the lexicon into the appropriate notation.
Bos et al (2009) created a CCGbank from an Italian dependency tree bank by converting dependency trees into phrase structure trees and then applying an algorithm similar to Hockenmaier and Steedman (2007). $$$$$ This includes small clauses, as well as pied-piping, subject extraction from embedded sentences and argument cluster coordination (discussed in Section 6).

The resource used for building wide-coverage CCG parsers of English is CCGbank (Hockenmaier and Steedman, 2007), a version of the Penn Treebank in which each phrase-structure tree has been transformed into a normal-form CCG derivation. $$$$$ NPs are flat as well, with all complex modifiers appearing at the same NP level, and compound nouns typically lacking any internal structure.
The resource used for building wide-coverage CCG parsers of English is CCGbank (Hockenmaier and Steedman, 2007), a version of the Penn Treebank in which each phrase-structure tree has been transformed into a normal-form CCG derivation. $$$$$ Although certain decisions taken by the builders of the original Penn Treebank mean that the syntactic derivations that can be obtained from the Penn Treebank are not always semantically correct (as we will discuss), subsequent work by Bos et al. (2004) and Bos (2005) has demonstrated that the output of parsers trained on CCGbank can also be directly translated into logical forms such as Discourse Representation Theory structures (Kamp and Reyle 1993), which can then be used as input to a theorem prover in applications like question answering and textual entailment recognition.
The resource used for building wide-coverage CCG parsers of English is CCGbank (Hockenmaier and Steedman, 2007), a version of the Penn Treebank in which each phrase-structure tree has been transformed into a normal-form CCG derivation. $$$$$ 4.3.5 The Final Derivation.

Our experiments were performed using CCGBank (Hockenmaier and Steedman, 2007), which was split into three subsets for training (Sections 02-21), development testing (Section 00) and the final test (Section 23). $$$$$ Even though certain mismatches between the syntactic annotations in the Penn Treebank and the underlying semantics remain, and will affect any similar attempt to obtain expressive grammars from the Treebank, we believe that CCGbank, the resulting corpus, will be of use to the computational linguistics community in the following ways.
Our experiments were performed using CCGBank (Hockenmaier and Steedman, 2007), which was split into three subsets for training (Sections 02-21), development testing (Section 00) and the final test (Section 23). $$$$$ A similarly flat annotation style is adopted at the sentence level.
Our experiments were performed using CCGBank (Hockenmaier and Steedman, 2007), which was split into three subsets for training (Sections 02-21), development testing (Section 00) and the final test (Section 23). $$$$$ We will begin by ignoring null elements, and assume that Penn Treebank trees are entirely consistent with CCG analyses.

CCGbank was derived from the PTB, and so it might be considered that converting back to the PTB would be a relatively easy task, by essentially reversing the mapping Hockenmaier and Steedman (2007) used to create CCGbank. $$$$$ Such rules induce additional derivational ambiguity, even in canonical sentences like (4).
CCGbank was derived from the PTB, and so it might be considered that converting back to the PTB would be a relatively easy task, by essentially reversing the mapping Hockenmaier and Steedman (2007) used to create CCGbank. $$$$$ Because this distinction requires manual inspection on a caseby-case basis, we were unable to modify the Treebank analysis.
CCGbank was derived from the PTB, and so it might be considered that converting back to the PTB would be a relatively easy task, by essentially reversing the mapping Hockenmaier and Steedman (2007) used to create CCGbank. $$$$$ The other case where small clauses are used in the Treebank includes absolute with and though constructions (with the limit in effect).

In our experience, trying to add number and animacy agreement constraints to a grammar induced from the CCGbank (Hockenmaier and Steedman, 2007) turned out to be surprisingly difficult, as hard constraints often ended up breaking examples that were working without such constraints, due to exceptions, sub-regularities and acceptable variation in the data. $$$$$ In both cases, the missing information has to be added before a Treebank tree can be translated into CCG.
In our experience, trying to add number and animacy agreement constraints to a grammar induced from the CCGbank (Hockenmaier and Steedman, 2007) turned out to be surprisingly difficult, as hard constraints often ended up breaking examples that were working without such constraints, due to exceptions, sub-regularities and acceptable variation in the data. $$$$$ Although these dependencies are only an approximation of the full semantic interpretation that can in principle be obtained from a CCG, they may prove useful for tasks such as summarization and question answering (Clark, Steedman, and Curran 2004).
In our experience, trying to add number and animacy agreement constraints to a grammar induced from the CCGbank (Hockenmaier and Steedman, 2007) turned out to be surprisingly difficult, as hard constraints often ended up breaking examples that were working without such constraints, due to exceptions, sub-regularities and acceptable variation in the data. $$$$$ MJS acknowledges support from the Scottish Enterprise Edinburgh–Stanford Link (NSF IIS-041628 (R39058)) and EU IST grant PACOPLUS (FP6-2004-IST-4-27657).
In our experience, trying to add number and animacy agreement constraints to a grammar induced from the CCGbank (Hockenmaier and Steedman, 2007) turned out to be surprisingly difficult, as hard constraints often ended up breaking examples that were working without such constraints, due to exceptions, sub-regularities and acceptable variation in the data. $$$$$ In order to obtain linguistically adequate CCG analyses, and to eliminate noise and inconsistencies in the original annotation, an extensive analysis of the constructions and annotations in the Penn Treebank was called for, and a substantial number of changes to the Treebank were necessary.

For example, CCGbank (Hockenmaier and Steedman, 2007) contains 1241 distinct supertags (lexical categories) and the most ambiguous word has 126 supertags. $$$$$ The reasons for this shift away from linguistic adequacy are easy to trace.
For example, CCGbank (Hockenmaier and Steedman, 2007) contains 1241 distinct supertags (lexical categories) and the most ambiguous word has 126 supertags. $$$$$ Despite the fact that these occur in a large number of sentences, they affect only a small number of words, and have thus a small impact on overall dependency recovery.
For example, CCGbank (Hockenmaier and Steedman, 2007) contains 1241 distinct supertags (lexical categories) and the most ambiguous word has 126 supertags. $$$$$ MJS acknowledges support from the Scottish Enterprise Edinburgh–Stanford Link (NSF IIS-041628 (R39058)) and EU IST grant PACOPLUS (FP6-2004-IST-4-27657).

CCGbank was created by semiautomatically converting the Penn Treebank to CCG derivations (Hockenmaier and Steedman, 2007). $$$$$ 4.3.3 Head and Adjunct.
CCGbank was created by semiautomatically converting the Penn Treebank to CCG derivations (Hockenmaier and Steedman, 2007). $$$$$ JH also acknowledges support by an EPSRC studentship and the Edinburgh Language Technology Group, and by NSF ITR grant 0205456 at the University of Pennsylvania.
CCGbank was created by semiautomatically converting the Penn Treebank to CCG derivations (Hockenmaier and Steedman, 2007). $$$$$ JH also acknowledges support by an EPSRC studentship and the Edinburgh Language Technology Group, and by NSF ITR grant 0205456 at the University of Pennsylvania.
CCGbank was created by semiautomatically converting the Penn Treebank to CCG derivations (Hockenmaier and Steedman, 2007). $$$$$ Remaining problems are discussed in Section 7.

By automatically transforming the constituent structure trees annotated in PTB to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (Hockenmaier and Steedman, 2007), many syntactic parser other than the CFG formalism were also developed. $$$$$ We would also like to thank the Linguistic Data Consortium for their help in publishing CCGbank, and the Computational Linguistics reviewers for their extensive comments on earlier versions of this paper.
By automatically transforming the constituent structure trees annotated in PTB to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (Hockenmaier and Steedman, 2007), many syntactic parser other than the CFG formalism were also developed. $$$$$ Similarly, if C is of the form X\$, all outermost backward arguments \$ (and syntactic features) are stripped off from C to obtain C'.
By automatically transforming the constituent structure trees annotated in PTB to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (Hockenmaier and Steedman, 2007), many syntactic parser other than the CFG formalism were also developed. $$$$$ article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word–word dependencies.

We agree; however, our ultimate motivation is to use this work to tackle bootstrapping from very small tag dictionaries or dictionaries obtained from linguists or resources other than a corpus, and for tag sets that are more ambiguous (e.g., super tagging for CCGbank (Hockenmaier and Steedman, 2007)). $$$$$ MJS acknowledges support from the Scottish Enterprise Edinburgh–Stanford Link (NSF IIS-041628 (R39058)) and EU IST grant PACOPLUS (FP6-2004-IST-4-27657).
We agree; however, our ultimate motivation is to use this work to tackle bootstrapping from very small tag dictionaries or dictionaries obtained from linguists or resources other than a corpus, and for tag sets that are more ambiguous (e.g., super tagging for CCGbank (Hockenmaier and Steedman, 2007)). $$$$$ In order to obtain linguistically adequate CCG analyses, and to eliminate noise and inconsistencies in the original annotation, an extensive analysis of the constructions and annotations in the Penn Treebank was called for, and a substantial number of changes to the Treebank were necessary.
We agree; however, our ultimate motivation is to use this work to tackle bootstrapping from very small tag dictionaries or dictionaries obtained from linguists or resources other than a corpus, and for tag sets that are more ambiguous (e.g., super tagging for CCGbank (Hockenmaier and Steedman, 2007)). $$$$$ This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word–word dependencies.
We agree; however, our ultimate motivation is to use this work to tackle bootstrapping from very small tag dictionaries or dictionaries obtained from linguists or resources other than a corpus, and for tag sets that are more ambiguous (e.g., super tagging for CCGbank (Hockenmaier and Steedman, 2007)). $$$$$ This includes small clauses, as well as pied-piping, subject extraction from embedded sentences and argument cluster coordination (discussed in Section 6).

This logical form can be expressed in many ways; we will focus on the dependency representation used in CCGbank (Hockenmaier and Steedman, 2007). $$$$$ However, our translation algorithm yields normal form derivations (Hepple and Morrill 1989; Wittenburg and Wall 1991; K¨onig 1994; Eisner 1996), which use composition and type-raising only when syntactically necessary.
This logical form can be expressed in many ways; we will focus on the dependency representation used in CCGbank (Hockenmaier and Steedman, 2007). $$$$$ Although some non-terminal nodes carry additional function tags, such as -SBJ (subject) or -TMP (temporal modifier), truly problematic cases such as prepositional phrases are often marked with tags such as -CLR (“closely related”) or -DIR (“direction”), which are not always reliable or consistent indicators that a constituent is a modifier or an argument.
This logical form can be expressed in many ways; we will focus on the dependency representation used in CCGbank (Hockenmaier and Steedman, 2007). $$$$$ The set of categories that project such dependencies is not acquired automatically, but is given (as a list of category templates) to the algorithm which creates the actual dependency structures.
This logical form can be expressed in many ways; we will focus on the dependency representation used in CCGbank (Hockenmaier and Steedman, 2007). $$$$$ Create copies of coordinated argument clusters that correspond to the CCG analysis. determineConstituentTypes: For each node, determine its constituent type (head, complement, adjunct, conjunction, a constituent that is coindexed with a *RNR* trace, spurious null element, or argument cluster). makeBinary: Binarize the tree. percolateTraces: Determine the CCG category of *T* and *RNR* traces in complement position, and percolate them up to the appropriate level in the tree. assignCategories: Assign CCG categories to nodes in the tree, starting at the root node.

All experiments were conducted on CCGBank (Hockenmaier and Steedman, 2007), a right-most normal-form CCG version of the Penn Treebank. $$$$$ In order to understand a newspaper article, or any other piece of text, it is necessary to construct a representation of its meaning that is amenable to some form of inference.
All experiments were conducted on CCGBank (Hockenmaier and Steedman, 2007), a right-most normal-form CCG version of the Penn Treebank. $$$$$ Because the POS tag of passing is VBG, the CCG category of the complement VP is S[ng]\NP (present participle) and the lexical category of is is therefore (S[dcl]\NP)/(S[ng]\NP): is just passing the buck to young people Other VP features include [to] (to infinitival), [b] (bare infinitival), S[pt] (past participle), [pss] (passive), or [ng] (present participle).
All experiments were conducted on CCGBank (Hockenmaier and Steedman, 2007), a right-most normal-form CCG version of the Penn Treebank. $$$$$ Combinatory Categorial Grammar (CCG) was originally developed as a “near-contextfree” theory of natural language grammar, with a very free definition of derivational structure adapted to the analysis of coordination and unbounded dependency without movement or deletion transformations.

The grammar is automatically extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) with Propbank (Palmer et al, 2005) roles projected onto it (Boxwell and White, 2008). $$$$$ Figure 4 examines the growth of the number of lexical category types as a function of the amount of data translated into CCG.
The grammar is automatically extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) with Propbank (Palmer et al, 2005) roles projected onto it (Boxwell and White, 2008). $$$$$ This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word–word dependencies.
The grammar is automatically extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) with Propbank (Palmer et al, 2005) roles projected onto it (Boxwell and White, 2008). $$$$$ An important conclusion that follows for the builders of future treebanks is that the tradition established by the Penn Treebank of including all linguistically relevant dependencies should be continued, with if anything even closer adherence to semantically informed linguistic insights into predicate–argument structural relations.

Hockenmaier and Steedman (2007) showed that a CCG corpus could be created by adapting the Penn Treebank (Marcus et al, 1993). $$$$$ 4.3.2 Head and Complement.
Hockenmaier and Steedman (2007) showed that a CCG corpus could be created by adapting the Penn Treebank (Marcus et al, 1993). $$$$$ Lexical Coverage on Unseen Data.
Hockenmaier and Steedman (2007) showed that a CCG corpus could be created by adapting the Penn Treebank (Marcus et al, 1993). $$$$$ For formalisms like LTAG, the relation is more complex, but the work of Joshi and Kulick (1996), who “unfold” CCG categories into TAG elementary trees via partial proof trees, and Shen and Joshi (2005), who define LTAG “spines” that resemble categories, suggest that this is possible.

We use CCGBank (Hockenmaier and Steedman, 2007) for experimental data. $$$$$ Long-range dependencies represented in the Penn Treebank by traces such as *T* and *RNR* require extensions to the basic algorithm, which result in derivations that make use of typeraising, composition, and (occasionally) substitution rules like those in (5) wherever syntactically necessary.
We use CCGBank (Hockenmaier and Steedman, 2007) for experimental data. $$$$$ Multi-Word Expressions.
We use CCGBank (Hockenmaier and Steedman, 2007) for experimental data. $$$$$ Nodes that are coindexed with *RNR* traces receive the category of the corresponding traces.
We use CCGBank (Hockenmaier and Steedman, 2007) for experimental data. $$$$$ Although implementational details will differ across formalisms, similar problems and questions to those that arose in our work will be encountered in any attempt to extract expressive grammars from annotated corpora.

A well known work is transforming Penn Treebank into resources for various deep linguistic processing, including LTAG (Xia, 1999), CCG (Hockenmaier and Steedman, 2007), HPSG (Miyao et al, 2004) and LFG (Cahill et al, 2002). $$$$$ The growth of rule instantiations is shown in Figure 4.
A well known work is transforming Penn Treebank into resources for various deep linguistic processing, including LTAG (Xia, 1999), CCG (Hockenmaier and Steedman, 2007), HPSG (Miyao et al, 2004) and LFG (Cahill et al, 2002). $$$$$ It is available from the Linguistic Data Consortium, and has been used to train widecoverage statistical parsers that obtain state-of-the-art rates of dependency recovery.
A well known work is transforming Penn Treebank into resources for various deep linguistic processing, including LTAG (Xia, 1999), CCG (Hockenmaier and Steedman, 2007), HPSG (Miyao et al, 2004) and LFG (Cahill et al, 2002). $$$$$ NPs are flat as well, with all complex modifiers appearing at the same NP level, and compound nouns typically lacking any internal structure.
A well known work is transforming Penn Treebank into resources for various deep linguistic processing, including LTAG (Xia, 1999), CCG (Hockenmaier and Steedman, 2007), HPSG (Miyao et al, 2004) and LFG (Cahill et al, 2002). $$$$$ MJS acknowledges support from the Scottish Enterprise Edinburgh–Stanford Link (NSF IIS-041628 (R39058)) and EU IST grant PACOPLUS (FP6-2004-IST-4-27657).

CCGbank (Hockenmaier and Steedman, 2007) extends this grammar with a set of type-changing rules, designed to strike a better balance between sparsity in the category set and ambiguity in the grammar. $$$$$ article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word–word dependencies.
CCGbank (Hockenmaier and Steedman, 2007) extends this grammar with a set of type-changing rules, designed to strike a better balance between sparsity in the category set and ambiguity in the grammar. $$$$$ Their treatment is discussed in Sections 6.2 and 6.3.
CCGbank (Hockenmaier and Steedman, 2007) extends this grammar with a set of type-changing rules, designed to strike a better balance between sparsity in the category set and ambiguity in the grammar. $$$$$ This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word–word dependencies.
