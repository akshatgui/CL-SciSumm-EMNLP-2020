Pado et al. (2009) uses Textual Entailment features extracted from the Standford Entailment Recognizer (MacCartney et al, 2006). $$$$$ An advantage to the use of statistical classifiers is that they can be configured to output a probability distribution over possible answers rather than just the most likely answer.
Pado et al. (2009) uses Textual Entailment features extracted from the Standford Entailment Recognizer (MacCartney et al, 2006). $$$$$ In these rules, heads of constituents are first identified using a modified version of the Collins head rules that favor semantic heads (such as lexical verbs rather than auxiliaries), and dependents of heads are typed using tregex patterns (Levy and Andrew, 2006), an extension of the tgrep pattern language.

Some authors have already designed similar matching techniques, such as the ones described in (MacCartney et al, 2006) and (Snow et al, 2006). $$$$$ We argue that there are significant weaknesses in this approach, including flawed assumptions of monotonicity and locality.
Some authors have already designed similar matching techniques, such as the ones described in (MacCartney et al, 2006) and (Snow et al, 2006). $$$$$ In this formulation, sentences are represented as normalized syntactic dependency graphs (like the one shown in figure 1) and entailment is approximated with an alignment between the graph representing the hypothesis and a portion of the corresponding graph(s) representing the text.
Some authors have already designed similar matching techniques, such as the ones described in (MacCartney et al, 2006) and (Snow et al, 2006). $$$$$ This paper advocates a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment.

Marsi and Krahmer (2005) and MacCartney et al (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al (2006) and Hickl and Bensley (2007). $$$$$ We convert the phrase structure trees to typed dependency graphs using a set of deterministic handcoded rules (de Marneffe et al., 2006).
Marsi and Krahmer (2005) and MacCartney et al (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al (2006) and Hickl and Bensley (2007). $$$$$ Such models have serious limitations: semantic overlap is typically a symmetric relation, whereas entailment is clearly not, and, because overlap models do not account for syntactic or semantic structure, they are easily fooled by examples like ID 2081.
Marsi and Krahmer (2005) and MacCartney et al (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al (2006) and Hickl and Bensley (2007). $$$$$ Some have used simple measures of semantic overlap, but the more interesting work has largely converged on a graphalignment approach, operating on semantic graphs derived from syntactic dependency parses, and using a locally-decomposable alignment score as a proxy for strength of entailment.
Marsi and Krahmer (2005) and MacCartney et al (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al (2006) and Hickl and Bensley (2007). $$$$$ Like raw accuracy, it lies in the interval [0, 1], but it will exceed raw accuracy to the degree that predictions are well-calibrated.

We have previously emphasized (MacCartney et al, 2006) that there is more to inferential validity than close lexical or structural correspondence $$$$$ Current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text, using a locally decomposable matching score.
We have previously emphasized (MacCartney et al, 2006) that there is more to inferential validity than close lexical or structural correspondence $$$$$ We argue that there are significant weaknesses in this approach, including flawed assumptions of monotonicity and locality.
We have previously emphasized (MacCartney et al, 2006) that there is more to inferential validity than close lexical or structural correspondence $$$$$ “Hand-tuning” describes experiments in which all features are on, but no training occurs; rather, weights are set by hand, according to human intuition.

Our system is based on the stage architecture of the Stanford RTE system (MacCartney et al, 2006), but adds a stage for event coreference decision. $$$$$ Instead we propose a pipelined approach where alignment is followed by a classification step, in which we extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classifier trained on development data.
Our system is based on the stage architecture of the Stanford RTE system (MacCartney et al, 2006), but adds a stage for event coreference decision. $$$$$ Our goal in this stage is to compute linguistic representations of the text and hypothesis that contain as much information as possible about their semantic content.
Our system is based on the stage architecture of the Stanford RTE system (MacCartney et al, 2006), but adds a stage for event coreference decision. $$$$$ Unfortunately, there is no polynomial time algorithm for finding the exact best alignment.

Based on this representation, we apply a two stage entailment process similar to MacCartney et al (2006) developed for textual entailment $$$$$ We report results on data from the 2005 Pascal RTE Challenge which surpass previously reported results for alignment-based systems.
Based on this representation, we apply a two stage entailment process similar to MacCartney et al (2006) developed for textual entailment $$$$$ Current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text, using a locally decomposable matching score.
Based on this representation, we apply a two stage entailment process similar to MacCartney et al (2006) developed for textual entailment $$$$$ Such models have serious limitations: semantic overlap is typically a symmetric relation, whereas entailment is clearly not, and, because overlap models do not account for syntactic or semantic structure, they are easily fooled by examples like ID 2081.
Based on this representation, we apply a two stage entailment process similar to MacCartney et al (2006) developed for textual entailment $$$$$ ⇒ weak no Factivity features.

Given the clause representation, we follow the idea similar to MacCartney et al (2006), and predict the entailment decision in two stages of processing $$$$$ This allows us to get confidence estimates for computing a confidence weighted score (see section 5).
Given the clause representation, we follow the idea similar to MacCartney et al (2006), and predict the entailment decision in two stages of processing $$$$$ “Hand-tuning” describes experiments in which all features are on, but no training occurs; rather, weights are set by hand, according to human intuition.
Given the clause representation, we follow the idea similar to MacCartney et al (2006), and predict the entailment decision in two stages of processing $$$$$ The last issue arising in the graph matching approaches is the inherent confounding of alignment and entailment determination.
Given the clause representation, we follow the idea similar to MacCartney et al (2006), and predict the entailment decision in two stages of processing $$$$$ We thank Anna Rafferty, Josh Ainslie, and particularly Roger Grosse for contributions to the ideas and system reported here.

We base our experiments on the Stanford RTE system which uses a staged architecture (MacCartney et al, 2006). $$$$$ That is, the standard is not whether the hypothesis is logically entailed, but whether it can reasonably be inferred.
We base our experiments on the Stanford RTE system which uses a staged architecture (MacCartney et al, 2006). $$$$$ But a graphmatching system will to try to get non-entailment by making the matching cost between civilians and members of law enforcement agencies be very high.
We base our experiments on the Stanford RTE system which uses a staged architecture (MacCartney et al, 2006). $$$$$ Instead we propose a pipelined approach where alignment is followed by a classification step, in which we extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classifier trained on development data.
We base our experiments on the Stanford RTE system which uses a staged architecture (MacCartney et al, 2006). $$$$$ Much work remains in improving the entailment features, many of which may be seen as rough approximations to a formal monotonicity calculus.

Later systems that include more linguistic features extracted from resources such as WordNet have enjoyed more success (MacCartney et al, 2006). $$$$$ This pattern of entailment, like others, can be reversed by negative polarity markers (The gangster managed to escape |= The gangster escaped while The gangster didn’t manage to escape 6|= The gangster escaped).
Later systems that include more linguistic features extracted from resources such as WordNet have enjoyed more success (MacCartney et al, 2006). $$$$$ We address this by keeping the feature dimensionality small, and using high regularization penalties in training.
Later systems that include more linguistic features extracted from resources such as WordNet have enjoyed more success (MacCartney et al, 2006). $$$$$ We argue that there are significant weaknesses in this approach, including flawed assumptions of monotonicity and locality.
Later systems that include more linguistic features extracted from resources such as WordNet have enjoyed more success (MacCartney et al, 2006). $$$$$ Although developed independently, the same division between alignment and classification has also been proposed by Marsi and Krahmer (2005), whose textual system is developed and evaluated on parallel translations into Dutch.

Many of these features are inspired by MacCartney et al (2006) and Snow et al (2006), but not as sophisticated. $$$$$ The CWS is computed as follows: for each positive integer k up to the size of the test set, we compute accuracy over the k most confident predictions.
Many of these features are inspired by MacCartney et al (2006) and Snow et al (2006), but not as sophisticated. $$$$$ (2005), is to translate dependency parses into neo-Davidsonian-style quasilogical forms, and to perform weighted abductive theorem proving in the tradition of (Hobbs et al., 1988).
Many of these features are inspired by MacCartney et al (2006) and Snow et al (2006), but not as sophisticated. $$$$$ We have argued that such models suffer from three crucial limitations: an assumption of monotonicity, an assumption of locality, and a confounding of alignment and entailment determination.

The Stanford Entailment Recognizer (MacCartney et al, 2006) is a stochastic model that computes match and mismatch features for each premise hypothesis pair. $$$$$ Instead we propose a pipelined approach where alignment is followed by a classification step, in which we extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classifier trained on development data.
The Stanford Entailment Recognizer (MacCartney et al, 2006) is a stochastic model that computes match and mismatch features for each premise hypothesis pair. $$$$$ Because full, accurate, open-domain natural language understanding lies far beyond current capabilities, nearly all efforts in this area have sought to extract the maximum mileage from quite limited semantic representations.
The Stanford Entailment Recognizer (MacCartney et al, 2006) is a stochastic model that computes match and mismatch features for each premise hypothesis pair. $$$$$ We argue that there are significant weaknesses in this approach, including flawed assumptions of monotonicity and locality.

It has a three-stage architecture similar to the RTE system of MacCartney et al (2006). $$$$$ For example, in ID 59, the hypothesis aligns well with the text, but the addition of in Iraq indicates non-entailment.
It has a three-stage architecture similar to the RTE system of MacCartney et al (2006). $$$$$ We thank Anna Rafferty, Josh Ainslie, and particularly Roger Grosse for contributions to the ideas and system reported here.
It has a three-stage architecture similar to the RTE system of MacCartney et al (2006). $$$$$ We thank Anna Rafferty, Josh Ainslie, and particularly Roger Grosse for contributions to the ideas and system reported here.
It has a three-stage architecture similar to the RTE system of MacCartney et al (2006). $$$$$ This paper advocates a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment.

MacCartney et al (2006) describe a system for doing robust textual inference. $$$$$ Instead we propose a pipelined approach where alignment is followed by a classification step, in which we extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classifier trained on development data.
MacCartney et al (2006) describe a system for doing robust textual inference. $$$$$ An alignment consists of a mapping from each node (word) in the hypothesis graph to a single node in the text graph, or to null.3 Figure 1 gives the alignment for ID 971.
MacCartney et al (2006) describe a system for doing robust textual inference. $$$$$ If the parent is not in the list, we only check whether the embedding text is an affirmative context or a negative one.
