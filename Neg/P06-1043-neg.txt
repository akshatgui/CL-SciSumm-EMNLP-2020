 $$$$$ Much of this improvement, however, is based upon an ever-increasing number of features to be trained on (typithe data.
 $$$$$ Each division of the corpus consists of sentences from all available genres.
 $$$$$ Statistical parsers trained and tested on the Wall Street Journal treebank have shown vast improvements over the last 10 years.
 $$$$$ We do not see the same improvement (Figure 1) but this is likely due to differences in the parsers.

 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
 $$$$$ The first of these, parse-reranking (Collins, 2000; Charniak and Johnson, 2005) starts with a “standard” generative parser, but uses it to generate the n-best parses rather than a single parse.
 $$$$$ Adding in 1,000k sentences from NANC as well, we saw a further increase to 86.3%.

 $$$$$ While this is the goal of the majority of parsing researchers, it is not quite universal.
 $$$$$ Gildea evaluates on sentences of length ≤ 40, Bacchiani on all sentences. ent parsers such as voting schemes and parse selection can improve performance on biomedical data.
 $$$$$ But the lack of corpora has led to a situation where much of the current work on parsing is performed on a single domain using training data from that domain — the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al., 1993).
 $$$$$ While (McClosky et al., 2006) showed that this technique was effective when testing on WSJ, the true distribution was closer to WSJ so it made sense to emphasize it.

 $$$$$ Here we show that it also helps the father-afield Brown data.
 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
 $$$$$ We have demonstrated that rerankers and selftrained models can work well across domains.

 $$$$$ They find that techniques which combine differBrown test corpora using different WSJ and Brown training sets.
 $$$$$ Tuning on BROWN helps the parser, but not for the reranking parser. ment.
 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
 $$$$$ The results of this test are shown in Table 8.

 $$$$$ The final estimated model is shown in Table 9.
 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
 $$$$$ He also notes that different domains have very different structures by looking at frequent grammar productions.

In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. $$$$$ This may be due to the smaller size of the BROWN training set or because the feature schemas for the reranker were developed on WSJ data.
In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. $$$$$ While this is the goal of the majority of parsing researchers, it is not quite universal.
In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. $$$$$ Unfortunately, the state of the art in parser portability (i.e. using a parser trained on one domain to parse a different domain) is not good.

This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ The standard “Charniak parser” checks in at a labeled precisionof 89.7% on the Penn set, but only 82.9% on the test set from the Brown treebank corpus.
This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ The unlabeled data is the North American News Corpus, NANC (Graff, 1995), which is approximately 24 million unlabeled sentences from various news sources.
This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ The reranking parser generally sees an improvement, but it does not appear to be significant.
This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ We would like to thank the BLLIP team for their comments.

(McClosky et al, 2006) presents a successful instance of parsing with self-training by using a re-ranker. $$$$$ The next line shows what happens when parsing Brown using a WSJ-trained parser.
(McClosky et al, 2006) presents a successful instance of parsing with self-training by using a re-ranker. $$$$$ Tuning on BROWN helps the parser, but not for the reranking parser. ment.
(McClosky et al, 2006) presents a successful instance of parsing with self-training by using a re-ranker. $$$$$ Such worries have merit.
(McClosky et al, 2006) presents a successful instance of parsing with self-training by using a re-ranker. $$$$$ Statistical parsers trained and tested on the Wall Street Journal treebank have shown vast improvements over the last 10 years.

 $$$$$ We conducted randomization tests for the significance of the difference in corpus f-score, based on the randomization version of the paired sample ttest described by Cohen (1995).
 $$$$$ Both systems use a “model-merging” (Bacchiani et al., 2006) approach.
 $$$$$ Recent work, (McClosky et al., 2006), has shown that adding many millions of words of machine parsed and reranked LA Times articles does, in fact, improve performance of the parser on the closely related WSJ data.
 $$$$$ However, suppose one does self-training on a biology textbook instead of the LA Times.

Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and self training and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al, 2006a; McClosky et al, 2006b). $$$$$ It is interesting to compare this to our results for a completely Brown trained system (i.e. one in which the first-phase parser is trained on just Brown training data, and the second-phase reranker is trained on Brown 50-best lists).
Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and self training and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al, 2006a; McClosky et al, 2006b). $$$$$ Gildea evaluates on sentences of length ≤ 40, Bacchiani on all sentences. ent parsers such as voting schemes and parse selection can improve performance on biomedical data.
Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and self training and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al, 2006a; McClosky et al, 2006b). $$$$$ We would like to thank the BLLIP team for their comments.

The syntactic parser is the version that is self trained using 2,500,000 sentences from NANC, and where the starting version is trained only on WSJ data (McClosky et al, 2006b). $$$$$ For these reasons he takes the position that we should, instead, simply create treebanks for a large number of domains.
The syntactic parser is the version that is self trained using 2,500,000 sentences from NANC, and where the starting version is trained only on WSJ data (McClosky et al, 2006b). $$$$$ In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%.
The syntactic parser is the version that is self trained using 2,500,000 sentences from NANC, and where the starting version is trained only on WSJ data (McClosky et al, 2006b). $$$$$ Clegg and Shepherd (2005) provide an extensive side-by-side performance analysis of several modern statistical parsers when faced with such data.

 $$$$$ The “Charniak parser” has a labeled precision-recall f-measure of 89.7% on WSJ but a lowly 82.9% on the test set from the Brown corpus treebank.
 $$$$$ “BROWN tuned” indicates that BROWN training data was used to tune the parameters (since the normal held-out section was being used for testing).
 $$$$$ One might hope that such a text will split the difference between more “normal” newspaper articles and the specialized medical text.
 $$$$$ We concentrate particularly on the work of (Gildea, 2001; Bacchiani et al., 2006) as they provide results which are directly comparable to those presented in this paper.

 $$$$$ The standard “Charniak parser” checks in at a labeled precisionof 89.7% on the Penn set, but only 82.9% on the test set from the Brown treebank corpus.
 $$$$$ Looking at Table 1, the first line shows us the standard training and testing on WSJ — both parsers perform in the 86-87% range.
 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.

 $$$$$ The 87.8% f-score on Brown represents a 24% error reduction on the corpus.
 $$$$$ Unless mentioned otherwise, we use the WSJ-trained reranker (as opposed to a BROWN-trained reranker).
 $$$$$ Recent work, (McClosky et al., 2006), has shown that adding many millions of words of machine parsed and reranked LA Times articles does, in fact, improve performance of the parser on the closely related WSJ data.

McClosky et al (2006b) used self-training and corpus weighting to adapt their parser trained on WSJ corpus to Browncorpus. $$$$$ Next, the 50-best parses are reordered by the reranker.
McClosky et al (2006b) used self-training and corpus weighting to adapt their parser trained on WSJ corpus to Browncorpus. $$$$$ In particular we note the effects of two comparatively recent techniques for parser improvement.
McClosky et al (2006b) used self-training and corpus weighting to adapt their parser trained on WSJ corpus to Browncorpus. $$$$$ This paper should allay these fears.

Recently, McClosky et al (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al, 2006b). $$$$$ We would like to thank the BLLIP team for their comments.
Recently, McClosky et al (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al, 2006b). $$$$$ At first blush one might think that gathering even more fine-grained features from a WSJ treebank would not help adaptation.
Recently, McClosky et al (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al, 2006b). $$$$$ We would like to thank the BLLIP team for their comments.

Note that our approach is different from the self-training technique proposed in (McClosky et al, 2006a), although both methods belong to semi-supervised training category. $$$$$ The second technique is self-training — parsing unlabeled data and adding it to the training corpus.
Note that our approach is different from the self-training technique proposed in (McClosky et al, 2006a), although both methods belong to semi-supervised training category. $$$$$ We would like to thank the BLLIP team for their comments.
Note that our approach is different from the self-training technique proposed in (McClosky et al, 2006a), although both methods belong to semi-supervised training category. $$$$$ Additionally, the reranker provides further benefit and adds an absolute 1-2% to the fscore.
Note that our approach is different from the self-training technique proposed in (McClosky et al, 2006a), although both methods belong to semi-supervised training category. $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.

Note that the results of our model are not directly comparable with previous parsing results shown in (McClosky et al, 2006a), since the parsing accuracy is measured in terms of dependency relations while their results are f-score of the bracketings implied in the phrase structure. $$$$$ The second technique is self-training — parsing unlabeled data and adding it to the training corpus.
Note that the results of our model are not directly comparable with previous parsing results shown in (McClosky et al, 2006a), since the parsing accuracy is measured in terms of dependency relations while their results are f-score of the bracketings implied in the phrase structure. $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
Note that the results of our model are not directly comparable with previous parsing results shown in (McClosky et al, 2006a), since the parsing accuracy is measured in terms of dependency relations while their results are f-score of the bracketings implied in the phrase structure. $$$$$ The improvements appear to be orthogonal, as our best performance is reached when we use the reranker and add 2,500k self-trained sentences from NANC. training data on parsing performance. f-scores for the parser with and without the WSJ reranker are shown when evaluating on BROWN development.
Note that the results of our model are not directly comparable with previous parsing results shown in (McClosky et al, 2006a), since the parsing accuracy is measured in terms of dependency relations while their results are f-score of the bracketings implied in the phrase structure. $$$$$ Of course, as corpora differences go, Brown is relatively close to WSJ.

 $$$$$ Here we show that it also helps the father-afield Brown data.
 $$$$$ One intriguing idea is what we call “self-trained bridging-corpora.” We have not yet experimented with medical text but we expect that the “best” WSJ+NANC parser will not perform very well.
 $$$$$ Thus, we have labeled (WSJ) and unlabeled (NANC) out-of-domain data and labeled in-domain data (BROWN).
 $$$$$ The table shows that the BROWN reranker is not significantly different from the WSJ reranker.
