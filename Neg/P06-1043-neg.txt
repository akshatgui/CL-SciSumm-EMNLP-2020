 $$$$$ However, (Bacchiani et al., 2006) achieve a larger gain by weighting the in-domain (Brown) data more heavily than the out-of-domain WSJ data.
 $$$$$ Because we are mainly interested in exploring techniques with self-trained models rather than optimizing performance, we only consider weighting each corpus with a relative weight of one for this paper.
 $$$$$ As further evidence, we present the results of applying the WSJ model to the Switchboard corpus — a domain much less similar to WSJ than BROWN.

 $$$$$ Models self-trained on WSJ appear to be better parsing models in general, the benefits of which are not limited to the WSJ domain.
 $$$$$ For these reasons he takes the position that we should, instead, simply create treebanks for a large number of domains.
 $$$$$ One intriguing idea is what we call “self-trained bridging-corpora.” We have not yet experimented with medical text but we expect that the “best” WSJ+NANC parser will not perform very well.
 $$$$$ NANC contains news articles from various news sources including the Wall Street Journal, though for this paper, we only use articles from the LA Times portion.

 $$$$$ We have demonstrated that rerankers and selftrained models can work well across domains.
 $$$$$ They find that techniques which combine differBrown test corpora using different WSJ and Brown training sets.
 $$$$$ We use the Charniak and Johnson (2005) reranking parser in our experiments.
 $$$$$ Work in parser adaptation is premised on the assumption that one wants a single parser that can handle a wide variety of domains.

 $$$$$ Unfortunately, the state of the art in parser portability (i.e. using a parser trained on one domain to parse a different domain) is not good.
 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.

 $$$$$ The standard “Charniak parser” checks in at a labeled precisionof 89.7% on the Penn set, but only 82.9% on the test set from the Brown treebank corpus.
 $$$$$ Lease and Charniak (2005) use the Charniak parser for biomedical data and find that the use of out-of-domain trees and in-domain vocabulary information can considerably improve performance.
 $$$$$ We would like to thank the BLLIP team for their comments.
 $$$$$ Naturally, one of the goals of statistical parsing is to produce a broad-coverage parser which is relatively insensitive to textual domain.

 $$$$$ Naturally, one of the goals of statistical parsing is to produce a broad-coverage parser which is relatively insensitive to textual domain.
 $$$$$ Bridging these larger gaps is still for the future.
 $$$$$ Thus, the WSJ+NANC model has better oracle rates than the WSJ model (McClosky et al., 2006) for both the WSJ and BROWN domains.
 $$$$$ Given the aforementioned costs, it is unlikely that many significant treebanks will be created for new genres.

In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. $$$$$ The reranking parser generally sees an improvement, but it does not appear to be significant.
In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. $$$$$ The different corpora are, in effect, concatenated together.
In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. $$$$$ The results are summarized in Table 3.

This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ We assume that these sets of data must be in similar domains (e.g. news articles) though the effectiveness of self-training across domains is currently an open question.
This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ “best” WSJ-parser-reranker improves performance on the Switchboard corpus, it starts from a much lower base (74.0%), and achieves a much less significant improvement (3% absolute, 11% error reduction).
This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ They find that techniques which combine differBrown test corpora using different WSJ and Brown training sets.

(McClosky et al, 2006) presents a successful instance of parsing with self-training by using a re-ranker. $$$$$ Adding it improves performance yet-again, this time from 85.2% to 87.8%, for a net error reduction of 28%.
(McClosky et al, 2006) presents a successful instance of parsing with self-training by using a re-ranker. $$$$$ For these reasons he takes the position that we should, instead, simply create treebanks for a large number of domains.
(McClosky et al, 2006) presents a successful instance of parsing with self-training by using a re-ranker. $$$$$ We use the standard divisions: Sections 2 through 21 are used for training, section 24 for held-out development, and section 23 for final testing.
(McClosky et al, 2006) presents a successful instance of parsing with self-training by using a re-ranker. $$$$$ Of these 2,049 sentences, there were 983 parse pairs with the same sentence-level f-score.

 $$$$$ We would like to thank the BLLIP team for their comments.
 $$$$$ The standard “Charniak parser” checks in at a labeled precisionof 89.7% on the Penn set, but only 82.9% on the test set from the Brown treebank corpus.

Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and self training and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al, 2006a; McClosky et al, 2006b). $$$$$ While the WSJ parser has relatively low f-scores, adding NANC data results in a parser with comparable oracle scores as the parser trained from BROWN training.
Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and self training and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al, 2006a; McClosky et al, 2006b). $$$$$ In this paper, we present some encouraging results on parser adaptation without any in-domain data.
Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and self training and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al, 2006a; McClosky et al, 2006b). $$$$$ We perform a similar experiment, using our WSJtrained reranking parser to parse BROWN train and testing on BROWN development.
Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and self training and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al, 2006a; McClosky et al, 2006b). $$$$$ The unlabeled data is the North American News Corpus, NANC (Graff, 1995), which is approximately 24 million unlabeled sentences from various news sources.

The syntactic parser is the version that is self trained using 2,500,000 sentences from NANC, and where the starting version is trained only on WSJ data (McClosky et al, 2006b). $$$$$ It shows that the WSJ+NANC/WSJ parser becomes more likely to have a higher f-score than the BROWN/BROWN parser as the number of prepositions in the sentence increases, and that the BROWN/BROWN parser is more likely to have a higher f-score on Brown sections K, N, P, G and L (these are the general fiction, adventure and western fiction, romance and love story, letters and memories, and mystery sections of the Brown corpus, respectively).
The syntactic parser is the version that is self trained using 2,500,000 sentences from NANC, and where the starting version is trained only on WSJ data (McClosky et al, 2006b). $$$$$ (Though we also present results with indomain data as a reference point.)

 $$$$$ We would like to thank the BLLIP team for their comments.
 $$$$$ Gildea evaluates on sentences of length ≤ 40, Bacchiani on all sentences. ent parsers such as voting schemes and parse selection can improve performance on biomedical data.
 $$$$$ Of course, as corpora differences go, Brown is relatively close to WSJ.
 $$$$$ The standard “Charniak parser” checks in at a labeled precisionof 89.7% on the Penn set, but only 82.9% on the test set from the Brown treebank corpus.

 $$$$$ We would like to thank the BLLIP team for their comments.
 $$$$$ Statistical parsers trained and tested on the Wall Street Journal treebank have shown vast improvements over the last 10 years.
 $$$$$ Lacking alternatives, both (Gildea, 2001) and (Bacchiani et al., 2006) give up on adapting a pure WSJ trained system, instead looking at the issue of how much of an improvement one gets over a pure Brown system by adding WSJ data (as seen in the last two lines of Table 1).
 $$$$$ Such worries have merit.

 $$$$$ To use the data from NANC, we use self-training (McClosky et al., 2006).
 $$$$$ The first of these, parse-reranking (Collins, 2000; Charniak and Johnson, 2005) starts with a “standard” generative parser, but uses it to generate the n-best parses rather than a single parse.
 $$$$$ We use evalb to calculate how similar the two sets of output are on a bracket level.
 $$$$$ Then a reranking phase uses more detailed features, features which would (mostly) be impossible to incorporate in the initial phase, to reorder the list and pick a possibly different best parse.

McClosky et al (2006b) used self-training and corpus weighting to adapt their parser trained on WSJ corpus to Browncorpus. $$$$$ Given the aforementioned costs, it is unlikely that many significant treebanks will be created for new genres.
McClosky et al (2006b) used self-training and corpus weighting to adapt their parser trained on WSJ corpus to Browncorpus. $$$$$ One intriguing idea is what we call “self-trained bridging-corpora.” We have not yet experimented with medical text but we expect that the “best” WSJ+NANC parser will not perform very well.
McClosky et al (2006b) used self-training and corpus weighting to adapt their parser trained on WSJ corpus to Browncorpus. $$$$$ The standard “Charniak parser” checks in at a labeled precisionof 89.7% on the Penn set, but only 82.9% on the test set from the Brown treebank corpus.
McClosky et al (2006b) used self-training and corpus weighting to adapt their parser trained on WSJ corpus to Browncorpus. $$$$$ Considering they were created from different corpora, this seems like a high level of agreement.

Recently, McClosky et al (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al, 2006b). $$$$$ Finally, the 1-best parses after reranking are combined with the WSJ training set to retrain the firststage parser.1 McClosky et al. (2006) find that the self-trained models help considerably when parsing WSJ.
Recently, McClosky et al (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al, 2006b). $$$$$ While both WSJ and BROWN models benefit from a small amount of NANC data, adding more than 250k NANC sentences to the BROWN or combined models causes their performance to drop.
Recently, McClosky et al (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al, 2006b). $$$$$ However, we do see a similar improvement for parsing accuracy once NANC data has been added.

Note that our approach is different from the self-training technique proposed in (McClosky et al, 2006a), although both methods belong to semi-supervised training category. $$$$$ In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%.
Note that our approach is different from the self-training technique proposed in (McClosky et al, 2006a), although both methods belong to semi-supervised training category. $$$$$ Examples of the sections annotated include science fiction, humor, romance, mystery, adventure, and “popular lore.” We use the same divisions as Bacchiani et al. (2006), who base their divisions on Gildea (2001).
Note that our approach is different from the self-training technique proposed in (McClosky et al, 2006a), although both methods belong to semi-supervised training category. $$$$$ Steedman et al. (2003) apply cotraining to parser adaptation and find that cotraining can work across domains.
Note that our approach is different from the self-training technique proposed in (McClosky et al, 2006a), although both methods belong to semi-supervised training category. $$$$$ (Though we also present results with indomain data as a reference point.)

Note that the results of our model are not directly comparable with previous parsing results shown in (McClosky et al, 2006a), since the parsing accuracy is measured in terms of dependency relations while their results are f-score of the bracketings implied in the phrase structure. $$$$$ The reranking parser used the WSJ-trained reranker model.
Note that the results of our model are not directly comparable with previous parsing results shown in (McClosky et al, 2006a), since the parsing accuracy is measured in terms of dependency relations while their results are f-score of the bracketings implied in the phrase structure. $$$$$ In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%.
Note that the results of our model are not directly comparable with previous parsing results shown in (McClosky et al, 2006a), since the parsing accuracy is measured in terms of dependency relations while their results are f-score of the bracketings implied in the phrase structure. $$$$$ To use the data from NANC, we use self-training (McClosky et al., 2006).

 $$$$$ While the full corpus consists of fiction and nonfiction domains, the sections that have been annotated in Treebank II bracketing are primarily those containing fiction.
 $$$$$ This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres.
