Next, it looks promising to try to estimate the dictionary word frequencies using a search engine instead of text corpus, as proposed by Lapata and Keller (2004). $$$$$ Lauer (1995) is the first to propose and evaluate an unsupervised probabilistic model of compound noun interpretation for domain independent text.
Next, it looks promising to try to estimate the dictionary word frequencies using a search engine instead of text corpus, as proposed by Lapata and Keller (2004). $$$$$ Its performance is not statistically different from that of the best Altavista model.
Next, it looks promising to try to estimate the dictionary word frequencies using a search engine instead of text corpus, as proposed by Lapata and Keller (2004). $$$$$ For example, in order to interpret war story, one needs to find in a corpus related paraphrases: story about the war, story of the war, story in the war, etc.
Next, it looks promising to try to estimate the dictionary word frequencies using a search engine instead of text corpus, as proposed by Lapata and Keller (2004). $$$$$ An average accuracy of 81.50% was obtained.

Lapata and Keller (2004) uses the number of page hits as the web-count of the queried n gram (which is problematic according to Kilgarriff (2007)). $$$$$ The BNC counts were retrieved using the Gsearch corpus query tool (Corley et al., 2001); the morphological query expansion was the same as for web queries; the NEAR operator was simulated by assuming a window of five words to the left and five to the right.
Lapata and Keller (2004) uses the number of page hits as the web-count of the queried n gram (which is problematic according to Kilgarriff (2007)). $$$$$ The second analysis task we consider is the semantic interpretation of compound nouns.
Lapata and Keller (2004) uses the number of page hits as the web-count of the queried n gram (which is problematic according to Kilgarriff (2007)). $$$$$ The web counts were gathered using inflected queries involving the verb, a determiner, and the object (see Section 2).

While it is possible to exploit search engine queries for various NLP tasks (Lapata and Keller, 2004), for applications which use corpora as unsupervised training material downloadable base data is essential. $$$$$ A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) and performs similar to Golding and Schabes (1996).
While it is possible to exploit search engine queries for various NLP tasks (Lapata and Keller, 2004), for applications which use corpora as unsupervised training material downloadable base data is essential. $$$$$ Table 13 shows that both the Altavista model and BNC model significantly outperform the baseline (relative frequency of the majority class on the gold-standard data).
While it is possible to exploit search engine queries for various NLP tasks (Lapata and Keller, 2004), for applications which use corpora as unsupervised training material downloadable base data is essential. $$$$$ The BNC is a static 100M word corpus of British English, which is about 1000 times smaller than the web (Keller and Lapata, 2003).

This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004). $$$$$ A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) and performs similar to Golding and Schabes (1996).
This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004). $$$$$ We wanted to test if the web-based approach extends from lexicalized compounds to productive syntactic units for which dictionary entries do not exist.
This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004). $$$$$ For example, in order to interpret war story, one needs to find in a corpus related paraphrases: story about the war, story of the war, story in the war, etc.
This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004). $$$$$ By recasting the interpretation problem in terms of paraphrasing, Lauer assumes that the semantic relations of compound heads and modifiers can be expressed via prepositions that (in contrast to abstract semantic relations) can be found in a corpus.

Lapata and Keller (2004) first used web-based co-occurrence counts for the bracketing of NCs. $$$$$ For each word in the confusion set, we used the web to estimate how frequently it co-occurs with a word or a pair of words immediately to its left or right.
Lapata and Keller (2004) first used web-based co-occurrence counts for the bracketing of NCs. $$$$$ This problem is limited to unigrams, which were used in some of the models detailed below.
Lapata and Keller (2004) first used web-based co-occurrence counts for the bracketing of NCs. $$$$$ This paper aims to address these questions.
Lapata and Keller (2004) first used web-based co-occurrence counts for the bracketing of NCs. $$$$$ This baseline is more realistic than baselines obtained from standard corpora; it is generally harder to beat, as our comparisons with the BNC baseline throughout this paper have shown.

Aside from counting bigrams, various tasks are attainable using web based models $$$$$ The task is to infer which word in a confusion set is the correct one in a given context.
Aside from counting bigrams, various tasks are attainable using web based models $$$$$ Ties are resolved by comparing the unigram frequencies of the words in the confusion set and defaulting to the word with the highest one.
Aside from counting bigrams, various tasks are attainable using web based models $$$$$ Table 6 shows that both the best Altavista model and the best BNC model outperform their respective baselines.
Aside from counting bigrams, various tasks are attainable using web based models $$$$$ In all cases, we propose a simple, unsupervised n-gram based model whose parameters are estimated using web counts.

Therefore, it can be used easily as a baseline, as suggested by (Lapata and Keller, 2004). $$$$$ Web counts were obtained by submitting literal queries to Altavista (see Section 2).
Therefore, it can be used easily as a baseline, as suggested by (Lapata and Keller, 2004). $$$$$ Lauer (1995) proposes an unsupervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus.
Therefore, it can be used easily as a baseline, as suggested by (Lapata and Keller, 2004). $$$$$ Keller and Lapata (2003) investigated the validity of web counts for a range of predicate-argument bigrams (verbobject, adjective-noun, and noun-noun bigrams).

The results are compared against two state of the art approaches $$$$$ We start by using web counts for two generation tasks for which the use of large data sets has shown promising results: (a) target language candidate selection for machine translation (Grefenstette, 1998) and (b) context sensitive spelling correction (Banko and Brill, 2001a,b).
The results are compared against two state of the art approaches $$$$$ So far, only two generation tasks (candidate selection for machine translation and confusion-set disambiguation) have been tested using web-scale data sets.
The results are compared against two state of the art approaches $$$$$ He observes that this approach suffers from an acute data sparseness problem and goes on to obtain counts for candidate compounds through web searches, thus achieving a translation accuracy of 86–87%.
The results are compared against two state of the art approaches $$$$$ So far, only two generation tasks (candidate selection for machine translation and confusion-set disambiguation) have been tested using web-scale data sets.

More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. $$$$$ Lauer (1995) tested both the adjacency and dependency models on 244 compounds extracted from Grolier’s encyclopedia, a corpus of 8 million words.
More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. $$$$$ Our method uses a very small feature set, it relies only on co-occurrence frequencies and does not have access to POS information (the latter has been shown to have an improvement on confusion sets whose words belong to different parts of speech).
More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. $$$$$ Table 4 lists Prescher et al.’s results for the two corpora and for both models together with a random baseline (select a target noun at random) and a frequency baseline (select the most frequent target noun).
More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. $$$$$ Words are classified into four classes: countable, uncountable, bipartite (e.g., trousers), and plural only (e.g., goods).

We have experimented with the support vector machines (SVM) model and compared the results against two state-of-the-art models $$$$$ The adjacency model compares [n1 n2] against [n2 n3] and adopts a right branching analysis if [n2 n3] is more likely than [n1 n2].
We have experimented with the support vector machines (SVM) model and compared the results against two state-of-the-art models $$$$$ Web counts were retrieved for all possible verb-object translations; the most likely one was selected using either co-occurrence frequency (f(v,n)) or conditional probability (f (v,n)/f (n)).

(Lapata and Keller, 2004)'s web-based unsupervised model classifies noun noun instances based on Lauer's list of 8 prepositions and uses the web as training corpus. $$$$$ Keller and Lapata’s (2003) results suggest that webbased frequencies can be a viable alternative to bigram frequencies obtained from smaller corpora or recreated using smoothing.
(Lapata and Keller, 2004)'s web-based unsupervised model classifies noun noun instances based on Lauer's list of 8 prepositions and uses the web as training corpus. $$$$$ For the ordering task we restricted ourselves to the direct evidence strategy which simply chooses the adjective order with the highest frequency or probability (see Table 7).
(Lapata and Keller, 2004)'s web-based unsupervised model classifies noun noun instances based on Lauer's list of 8 prepositions and uses the web as training corpus. $$$$$ Lauer (1995) tested both the adjacency and dependency models on 244 compounds extracted from Grolier’s encyclopedia, a corpus of 8 million words.
(Lapata and Keller, 2004)'s web-based unsupervised model classifies noun noun instances based on Lauer's list of 8 prepositions and uses the web as training corpus. $$$$$ The ordering of prenominal modifiers is important for natural language generation systems where the text must be both fluent and grammatical.

Although (Lapata and Keller, 2004) used Altavista in their experiments, they showed there is almost no difference between the correlations achieved using Google and Altavista counts. $$$$$ For both the high ambiguity and the low ambiguity data set, we find that the performance of the best Altavista model is not significantly different from that of the best BNC model.
Although (Lapata and Keller, 2004) used Altavista in their experiments, they showed there is almost no difference between the correlations achieved using Google and Altavista counts. $$$$$ The present paper investigates if these results generalize to tasks covering both syntax and semantics, both generation and analysis, and a larger of For the majority of tasks, we find that simple, unsupervised models perform when frequencies are obtained from the web rather than from a large corpus.
Although (Lapata and Keller, 2004) used Altavista in their experiments, they showed there is almost no difference between the correlations achieved using Google and Altavista counts. $$$$$ The reason for this seems to be that the web is much larger than the BNC (about 1000 times); the size seems to compensate for the fact that simple heuristics were used to obtain web counts, and for the noise inherent in web data.
Although (Lapata and Keller, 2004) used Altavista in their experiments, they showed there is almost no difference between the correlations achieved using Google and Altavista counts. $$$$$ The latter were obtained using inflected queries (see Section 2) and Altavista’s NEAR operator.

They then later propose using Web counts as a baseline unsupervised method for many NLP tasks (Lapata and Keller, 2004). $$$$$ The results obtained by a variety of classification methods are given in Table 6.
They then later propose using Web counts as a baseline unsupervised method for many NLP tasks (Lapata and Keller, 2004). $$$$$ In order to show this, web counts would have to be applied to a diverse range of NLP tasks, both syntactic and semantic, involving analysis (e.g., disambiguation) and generation (e.g., selection among competing outputs).
They then later propose using Web counts as a baseline unsupervised method for many NLP tasks (Lapata and Keller, 2004). $$$$$ We used the same 263,838 adjective pairs that Malouf extracted from the BNC.

Lapata and Keller (2004) achieved their best accuracy (78.68%) with the dependency model and the simple symmetric score #(wi ,wj). $$$$$ However, in most cases, web-based models fail to outperform more sophisticated state-of-theart models trained on small corpora.
Lapata and Keller (2004) achieved their best accuracy (78.68%) with the dependency model and the simple symmetric score #(wi ,wj). $$$$$ In order to show this, web counts would have to be applied to a diverse range of NLP tasks, both syntactic and semantic, involving analysis (e.g., disambiguation) and generation (e.g., selection among competing outputs).
Lapata and Keller (2004) achieved their best accuracy (78.68%) with the dependency model and the simple symmetric score #(wi ,wj). $$$$$ The ordering of prenominal adjectives has sparked a great deal of theoretical debate (see Shaw and Hatzivassiloglou 1999 for an overview) and efforts have concentrated on defining rules based on semantic criteria that account for different orders (e.g., age � color, value � dimension).
Lapata and Keller (2004) achieved their best accuracy (78.68%) with the dependency model and the simple symmetric score #(wi ,wj). $$$$$ For example, in order to interpret war story, one needs to find in a corpus related paraphrases: story about the war, story of the war, story in the war, etc.

This is confirmed by the adjacency model experiments in (Lapata and Keller, 2004) on Lauer's NC set. $$$$$ To gauge the performance of the web-based models we compared them against their BNC-based alternatives; the performance of the best Altavista model was significantly higher than that of the best BNC model (see Table 8).
This is confirmed by the adjacency model experiments in (Lapata and Keller, 2004) on Lauer's NC set. $$$$$ Ties were resolved by defaulting to the most frequent analysis (i.e., left-branching).
This is confirmed by the adjacency model experiments in (Lapata and Keller, 2004) on Lauer's NC set. $$$$$ A number of machine learning methods have been proposed for context-sensitive spelling correction.
This is confirmed by the adjacency model experiments in (Lapata and Keller, 2004) on Lauer's NC set. $$$$$ For all of our tasks, we have to select either the best of several possible models or the best parameter setting for a single model.

Lapata and Keller (2004) derived their statistics from the Web and achieved results close to Lauer's using simple lexical models. $$$$$ Then we investigate the generality of the web-based approach by applying it to a range of analysis and generations tasks, involving both syntactic and semantic knowledge: (c) ordering of prenominal adjectives, (d) compound noun bracketing, (e) compound noun interpretation, and (f) noun countability detection.
Lapata and Keller (2004) derived their statistics from the Web and achieved results close to Lauer's using simple lexical models. $$$$$ We wanted to test if the web-based approach extends from lexicalized compounds to productive syntactic units for which dictionary entries do not exist.
Lapata and Keller (2004) derived their statistics from the Web and achieved results close to Lauer's using simple lexical models. $$$$$ Given that submitting 26,271 queries to Altavista would be fairly timeconsuming, a random sample of 1000 sequences was obtained from the test corpus and the web frequencies of these pairs were retrieved.

Table 3 compares our results to those of Lauer (1995) and of Lapata and Keller (2004). $$$$$ Finally, he uses memory-based learning as a means to encode morphological and semantic similarities among different adjective orders.
Table 3 compares our results to those of Lauer (1995) and of Lapata and Keller (2004). $$$$$ Then we investigate the generality of the web-based approach by applying it to a range of analysis and generations tasks, involving both syntactic and semantic knowledge: (c) ordering of prenominal adjectives, (d) compound noun bracketing, (e) compound noun interpretation, and (f) noun countability detection.
Table 3 compares our results to those of Lauer (1995) and of Lapata and Keller (2004). $$$$$ Given that submitting 26,271 queries to Altavista would be fairly timeconsuming, a random sample of 1000 sequences was obtained from the test corpus and the web frequencies of these pairs were retrieved.
Table 3 compares our results to those of Lauer (1995) and of Lapata and Keller (2004). $$$$$ All search terms were submitted to the search engine in lower case.

We have extended and improved upon the state-of-the-art approaches to NC bracketing using an unsupervised method that is more robust than Lauer (1995) and more accurate than Lapata and Keller (2004). $$$$$ This holds for both the high and low ambiguity data sets.
We have extended and improved upon the state-of-the-art approaches to NC bracketing using an unsupervised method that is more robust than Lauer (1995) and more accurate than Lapata and Keller (2004). $$$$$ As an alternative to (6), he proposes the model in (7) which combines the probability of the modifier given a certain preposition with the probability of the head given the same preposition, and assumes that these two probabilities are independent.
We have extended and improved upon the state-of-the-art approaches to NC bracketing using an unsupervised method that is more robust than Lauer (1995) and more accurate than Lapata and Keller (2004). $$$$$ Then we investigate the generality of the web-based approach by applying it to a range of analysis and generations tasks, involving both syntactic and semantic knowledge: (c) ordering of prenominal adjectives, (d) compound noun bracketing, (e) compound noun interpretation, and (f) noun countability detection.

 $$$$$ For example, the sequence big fat Greek wedding is perfectly acceptable, whereas fat Greek big wedding sounds odd.
 $$$$$ The task is illustrated in (1) where there are five translation alternatives for the German noun Geschichte listed in curly brackets, the first being the correct one.
 $$$$$ He observes that this approach suffers from an acute data sparseness problem and goes on to obtain counts for candidate compounds through web searches, thus achieving a translation accuracy of 86–87%.
 $$$$$ Our results therefore indicate that large data sets such as those obtained from the web are not the panacea that they are claimed to be (at least implicitly) by authors such as Grefenstette (1998) and Keller and Lapata (2003).

The vast size of the Web has been demonstrated to combat the data sparseness problem, for example, in Lapata and Keller (2004). $$$$$ Once the set of translation candidates is generated, statistical information gathered from target language corpora is used to select the most appropriate alternative (Dagan and Itai, 1994).
The vast size of the Web has been demonstrated to combat the data sparseness problem, for example, in Lapata and Keller (2004). $$$$$ This paper aims to address these questions.
The vast size of the Web has been demonstrated to combat the data sparseness problem, for example, in Lapata and Keller (2004). $$$$$ The present paper investigates if these results generalize to tasks covering both syntax and semantics, both generation and analysis, and a larger of For the majority of tasks, we find that simple, unsupervised models perform when frequencies are obtained from the web rather than from a large corpus.
