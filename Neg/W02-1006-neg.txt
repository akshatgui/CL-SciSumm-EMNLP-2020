In this paper, we used the WSD program reported in (Lee and Ng, 2002). $$$$$ The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms.
In this paper, we used the WSD program reported in (Lee and Ng, 2002). $$$$$ A test example is classified by traversing the learned decision tree.
In this paper, we used the WSD program reported in (Lee and Ng, 2002). $$$$$ WSD is a fundamental problem of natural language processing.

More precisely, we follow (Lee and Ng, 2002), a reference work for WSD, by adopting a Support Vector Machines (SVM) classifier with a linear kernel and three kinds of features for characterizing each considered occurrence in a text of the reference word. $$$$$ However, all of these research efforts concentrate only on evaluating different learning algorithms, without systematically considering their interaction with knowledge sources.
More precisely, we follow (Lee and Ng, 2002), a reference work for WSD, by adopting a Support Vector Machines (SVM) classifier with a linear kernel and three kinds of features for characterizing each considered occurrence in a text of the reference word. $$$$$ There are 8,611 training instances and 4,328 test instances tagged with WORDNET senses.
More precisely, we follow (Lee and Ng, 2002), a reference work for WSD, by adopting a Support Vector Machines (SVM) classifier with a linear kernel and three kinds of features for characterizing each considered occurrence in a text of the reference word. $$$$$ Another approach involves the use of dictionary or thesaurus to perform WSD.
More precisely, we follow (Lee and Ng, 2002), a reference work for WSD, by adopting a Support Vector Machines (SVM) classifier with a linear kernel and three kinds of features for characterizing each considered occurrence in a text of the reference word. $$$$$ Each training (or test) context of generates one training (or test) feature vector.

Only features based on syntactic relations are not taken from (Lee and Ng, 2002) since their use would have not been coherent with the window based approach of the building of our initial thesaurus. $$$$$ We evaluated four supervised learning algorithms: Support Vector Machines (SVM), AdaBoost with decision stumps (AdB), Naive Bayes (NB), and decision trees (DT).
Only features based on syntactic relations are not taken from (Lee and Ng, 2002) since their use would have not been coherent with the window based approach of the building of our initial thesaurus. $$$$$ In this approach, to disambiguate a word , we first collect training texts in which instances of occur.
Only features based on syntactic relations are not taken from (Lee and Ng, 2002) since their use would have not been coherent with the window based approach of the building of our initial thesaurus. $$$$$ We then train a WSD classifier based on these sample texts, such that the trained classifier is able to assign the sense of in a new context.

We adopt the feature design used by Lee and Ng (2002), which consists of the following four types: (1) Local context: n-grams of nearby words (position sensitive); (2) Global context: all the words (excluding stop words) in the given context (position-insensitive; a bag of words); (3) POS: parts-of-speech n-grams of nearby words; (4) Syntactic relations: syntactic information obtained from parser output. $$$$$ In this paper, we conduct a systematic evaluation of the various knowledge sources and supervised learning algorithms on the English lexical sample data sets of both SENSEVALs.
We adopt the feature design used by Lee and Ng (2002), which consists of the following four types: (1) Local context: n-grams of nearby words (position sensitive); (2) Global context: all the words (excluding stop words) in the given context (position-insensitive; a bag of words); (3) POS: parts-of-speech n-grams of nearby words; (4) Syntactic relations: syntactic information obtained from parser output. $$$$$ However, all of these research efforts concentrate only on evaluating different learning algorithms, without systematically considering their interaction with knowledge sources.
We adopt the feature design used by Lee and Ng (2002), which consists of the following four types: (1) Local context: n-grams of nearby words (position sensitive); (2) Global context: all the words (excluding stop words) in the given context (position-insensitive; a bag of words); (3) POS: parts-of-speech n-grams of nearby words; (4) Syntactic relations: syntactic information obtained from parser output. $$$$$ Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.
We adopt the feature design used by Lee and Ng (2002), which consists of the following four types: (1) Local context: n-grams of nearby words (position sensitive); (2) Global context: all the words (excluding stop words) in the given context (position-insensitive; a bag of words); (3) POS: parts-of-speech n-grams of nearby words; (4) Syntactic relations: syntactic information obtained from parser output. $$$$$ We then train a WSD classifier based on these sample texts, such that the trained classifier is able to assign the sense of in a new context.

Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM. $$$$$ During testing, if appears in a phrasal word form, the classifier for that phrasal word form is used.
Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM. $$$$$ In SENSEVAL-2, the various Duluth systems (Pedersen, 2001b) attempted to investigate whether features or learning algorithms are more important.
Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM. $$$$$ Otherwise, the classifier for is used.
Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM. $$$$$ In SENSEVAL-2, the various Duluth systems (Pedersen, 2001b) attempted to investigate whether features or learning algorithms are more important.

Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD. $$$$$ During testing, if appears in a phrasal word form, the classifier for that phrasal word form is used.
Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD. $$$$$ There is a large body of prior research on WSD.
Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD. $$$$$ For these words, we first used a POS tagger (Ratnaparkhi, 1996) to determine the correct POS.
Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD. $$$$$ We also base our evaluation on both SENSEVAL-2 and SENSEVAL-1 official test data sets, and compare with the official scores of participating systems.

For every preposition, a baseline maxent model is trained using a set of features reported in the state-of-the-art WSD system of LeeandNg (2002). $$$$$ The top three systems for SENSEVAL-1 are: hopkins (s1) (Yarowsky, 2000), ets-pu (s2) (Chodorow et al., 2000), and tilburg (s3) (Veenstra et al., 2000).
For every preposition, a baseline maxent model is trained using a set of features reported in the state-of-the-art WSD system of LeeandNg (2002). $$$$$ A word can have multiple meanings (or senses).
For every preposition, a baseline maxent model is trained using a set of features reported in the state-of-the-art WSD system of LeeandNg (2002). $$$$$ For SENSEVAL-1, 4 trainable words belong to the indeterminate category, i.e., the POS is not provided.
For every preposition, a baseline maxent model is trained using a set of features reported in the state-of-the-art WSD system of LeeandNg (2002). $$$$$ For a word that may occur in phrasal word form (eg, the verb “turn” and the phrasal form “turn down”), we train a separate classifier for each phrasal word form.

Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation. $$$$$ In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data.
Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation. $$$$$ All remaining tokens from all training contexts provided for are gathered.
Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation. $$$$$ Such an approach would be highly laborintensive, with questionable scalability.

 $$$$$ Note that we are able to obtain state-of-the-art results using a single learning algorithm (SVM), without resorting to combining multiple learning algorithms.
 $$$$$ Early research efforts on comparing different learning algorithms (Mooney, 1996; Pedersen and Bruce, 1997) tend to base their comparison on only one word or at most a dozen words.
 $$$$$ However, they do not evaluate their method on a common benchmark data set, and there is no exploration on the interaction of knowledge sources with different learning algorithms.

For each ambiguous word in ELS task of SENSEVAL-3, we used three types of features to capture contextual information: part-of-speech of neighboring words with position information, unordered single words in topical context, and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations). $$$$$ However, relative contribution of knowledge sources was not reported and only two main types of algorithms (Naive Bayes and decision tree) were tested.
For each ambiguous word in ELS task of SENSEVAL-3, we used three types of features to capture contextual information: part-of-speech of neighboring words with position information, unordered single words in topical context, and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations). $$$$$ We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms.
For each ambiguous word in ELS task of SENSEVAL-3, we used three types of features to capture contextual information: part-of-speech of neighboring words with position information, unordered single words in topical context, and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations). $$$$$ Following the description in (Cabezas et al., 2001), our own re-implementation of UMD-SST gives a recall of 58.6%, close to their reported figure of 56.8%.
For each ambiguous word in ELS task of SENSEVAL-3, we used three types of features to capture contextual information: part-of-speech of neighboring words with position information, unordered single words in topical context, and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations). $$$$$ Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.

Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002). $$$$$ In contrast, in this paper, we systematically vary both knowledge sources and learning algorithms, and investigate the interaction between them.
Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002). $$$$$ We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms.
Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002). $$$$$ A test example is classified depending on the side of the hyperplane it lies in.

Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002). $$$$$ We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms.
Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002). $$$$$ However, all of these research efforts concentrate only on evaluating different learning algorithms, without systematically considering their interaction with knowledge sources.
Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002). $$$$$ For a word that may occur in phrasal word form (eg, the verb “turn” and the phrasal form “turn down”), we train a separate classifier for each phrasal word form.
Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002). $$$$$ The Naive Bayes classifier (Duda and Hart, 1973) assumes the features are independent given the class.

These feature types have been widely used in WSD algorithms (see Lee and Ng (2002) for an evaluation of their effectiveness). $$$$$ For a word that may occur in phrasal word form (eg, the verb “turn” and the phrasal form “turn down”), we train a separate classifier for each phrasal word form.
These feature types have been widely used in WSD algorithms (see Lee and Ng (2002) for an evaluation of their effectiveness). $$$$$ The comparison between our learning algorithms and the top three participating systems is given in Table 5.

These knowledge sources were effectively used to build a state-of-the-art WSD pro gram in one of our prior work (Lee and Ng, 2002). $$$$$ Due to space constraints, we will only highlight prior research efforts that have investigated (1) contribution of various knowledge sources, or (2) relative performance of different learning algorithms.
These knowledge sources were effectively used to build a state-of-the-art WSD pro gram in one of our prior work (Lee and Ng, 2002). $$$$$ All the experimental results reported in this paper are obtained using the implementation of these algorithms in WEKA (Witten and Frank, 2000).
These knowledge sources were effectively used to build a state-of-the-art WSD pro gram in one of our prior work (Lee and Ng, 2002). $$$$$ If the training examples are nonseparable, a regularization parameter ( by default) can be used to control the trade-off between achieving a large margin and a low training error.
These knowledge sources were effectively used to build a state-of-the-art WSD pro gram in one of our prior work (Lee and Ng, 2002). $$$$$ In addition, different learning algorithms benefit differently from feature selection.

Similar to our previous work (Chan and Ng, 2005b), we used the supervised WSD approach described in (Lee and Ng, 2002) for our experiments, using the naive Bayes algorithm as our classifier. $$$$$ For a word that may occur in phrasal word form (eg, the verb “turn” and the phrasal form “turn down”), we train a separate classifier for each phrasal word form.
Similar to our previous work (Chan and Ng, 2005b), we used the supervised WSD approach described in (Lee and Ng, 2002) for our experiments, using the naive Bayes algorithm as our classifier. $$$$$ Each occurrence of is manually tagged with the correct sense.

For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local col locations, parts-of-speech (POS), and surrounding words. $$$$$ WEKA implements AdaBoost.M1.
For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local col locations, parts-of-speech (POS), and surrounding words. $$$$$ On the SENSEVAL-2 test set, SVM achieves 65.4% (all 4 knowledge sources), 64.8% (remove syntactic relations), 61.8% (further remove POS), and 60.5% (only collocations) as knowledge sources are removed one at a time.
For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local col locations, parts-of-speech (POS), and surrounding words. $$$$$ However, they do not evaluate their method on a common benchmark data set, and there is no exploration on the interaction of knowledge sources with different learning algorithms.
For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local col locations, parts-of-speech (POS), and surrounding words. $$$$$ Some examples are shown in Table 1.

The features used in these systems usually include local features, such as part-of-speech (POS) of neighboring words, local collocations, syntactic patterns and global features such as single words in the surrounding context (bag-of-words) (Lee and Ng, 2002). $$$$$ SVM performs best without feature selection, whereas NB performs best with some feature selection ( ).
The features used in these systems usually include local features, such as part-of-speech (POS) of neighboring words, local collocations, syntactic patterns and global features such as single words in the surrounding context (bag-of-words) (Lee and Ng, 2002). $$$$$ In this paper, we focus on a corpus-based, supervised learning approach.
The features used in these systems usually include local features, such as part-of-speech (POS) of neighboring words, local collocations, syntactic patterns and global features such as single words in the surrounding context (bag-of-words) (Lee and Ng, 2002). $$$$$ The nine columns correspond to: (i) using only POS of neighboring words (ii) using only single words in the surrounding context with feature selection ( ) (iii) same as (ii) but without feature selection ( ) (iv) using only local collocations with feature selection ( ) (v) same as (iv) but without feature selection ( ) (vi) using only syntactic relations with feature selection on words ( ) (vii) same as (vi) but without feature selection ( ) (viii) combining all four knowledge sources with feature selection (ix) combining all four knowledge sources without feature selection.

 $$$$$ Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.
 $$$$$ In SENSEVAL-2, JHU used a combination of various learning algorithms (decision lists, cosinebased vector models, and Bayesian models) with various knowledge sources such as surrounding words, local collocations, syntactic relations, and morphological information.

We adopt the same syntactic relations as (Lee and Ng, 2002). $$$$$ During testing, if appears in a phrasal word form, the classifier for that phrasal word form is used.
We adopt the same syntactic relations as (Lee and Ng, 2002). $$$$$ Ng (1997) compared two learning algorithms, k-nearest neighbor and Naive Bayes, on the DSO corpus (191 words).
We adopt the same syntactic relations as (Lee and Ng, 2002). $$$$$ It repeats this process recursively for each partition until all examples in each partition belong to one class.
We adopt the same syntactic relations as (Lee and Ng, 2002). $$$$$ Note that we can only compare macroaveraged recall for SENSEVAL-1 systems, since the sense of each individual test instance output by the SENSEVAL-1 participating systems is not available.

In the SVM (Vapnik, 1995) approach, we first form a training and a testing file using all standard features for each sense following (Lee and Ng, 2002) (one classifier per sense). $$$$$ AdaBoost (Freund and Schapire, 1996) is a method of training an ensemble of weak learners such that the performance of the whole ensemble is higher than its constituents.
In the SVM (Vapnik, 1995) approach, we first form a training and a testing file using all standard features for each sense following (Lee and Ng, 2002) (one classifier per sense). $$$$$ In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data.
In the SVM (Vapnik, 1995) approach, we first form a training and a testing file using all standard features for each sense following (Lee and Ng, 2002) (one classifier per sense). $$$$$ Otherwise, the classifier for is used.
