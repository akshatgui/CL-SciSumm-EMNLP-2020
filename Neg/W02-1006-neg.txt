In this paper, we used the WSD program reported in (Lee and Ng, 2002). $$$$$ We use different types of syntactic relations, depending on the POS of .
In this paper, we used the WSD program reported in (Lee and Ng, 2002). $$$$$ For example, local collocations contribute the most for SVM, while parts-of-speech (POS) contribute the most for NB.
In this paper, we used the WSD program reported in (Lee and Ng, 2002). $$$$$ There is a large body of prior research on WSD.
In this paper, we used the WSD program reported in (Lee and Ng, 2002). $$$$$ We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms.

More precisely, we follow (Lee and Ng, 2002), a reference work for WSD, by adopting a Support Vector Machines (SVM) classifier with a linear kernel and three kinds of features for characterizing each considered occurrence in a text of the reference word. $$$$$ “ ” or “ ” means our rithms can already handle multi-class problems, and we denote runs using the original AdB, NB, and DT algorithms as “normal” in Table 2 and Table 3.
More precisely, we follow (Lee and Ng, 2002), a reference work for WSD, by adopting a Support Vector Machines (SVM) classifier with a linear kernel and three kinds of features for characterizing each considered occurrence in a text of the reference word. $$$$$ We will investigate the effect of more elaborate feature selection schemes on the performance of different learning algorithms for WSD in future work.
More precisely, we follow (Lee and Ng, 2002), a reference work for WSD, by adopting a Support Vector Machines (SVM) classifier with a linear kernel and three kinds of features for characterizing each considered occurrence in a text of the reference word. $$$$$ The recent work of Pedersen (2001a) and Zavrel et al. (2000) evaluated a variety of learning algorithms on the SENSEVAL1 data set.

Only features based on syntactic relations are not taken from (Lee and Ng, 2002) since their use would have not been coherent with the window based approach of the building of our initial thesaurus. $$$$$ To extract the feature values of the collocation feature , we first collect all possible collocation strings (converted into lower case) corresponding to in all training contexts of .
Only features based on syntactic relations are not taken from (Lee and Ng, 2002) since their use would have not been coherent with the window based approach of the building of our initial thesaurus. $$$$$ The t statistic of the difference between each pair of recall figures (between each test instance pair for micro-averaging and between each word task pair for macro-averaging) is computed, giving rise to a p value.

We adopt the feature design used by Lee and Ng (2002), which consists of the following four types $$$$$ However, they do not evaluate their method on a common benchmark data set, and there is no exploration on the interaction of knowledge sources with different learning algorithms.
We adopt the feature design used by Lee and Ng (2002), which consists of the following four types $$$$$ We use a sentence segmentation program (Reynar and Ratnaparkhi, 1997) and a POS tagger (Ratnaparkhi, 1996) to segment the tokens surrounding into sentences and assign POS tags to these tokens.
We adopt the feature design used by Lee and Ng (2002), which consists of the following four types $$$$$ We will investigate the effect of more elaborate feature selection schemes on the performance of different learning algorithms for WSD in future work.
We adopt the feature design used by Lee and Ng (2002), which consists of the following four types $$$$$ If a nominal feature takes the th feature value, then the th binary feature is set to 1 and all the other binary features are set to 0.

Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM. $$$$$ Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.
Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM. $$$$$ For these words, we first used a POS tagger (Ratnaparkhi, 1996) to determine the correct POS.
Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM. $$$$$ However, they do not evaluate their method on a common benchmark data set, and there is no exploration on the interaction of knowledge sources with different learning algorithms.
Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM. $$$$$ All the experimental results reported in this paper are obtained using the implementation of these algorithms in WEKA (Witten and Frank, 2000).

Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD. $$$$$ Our reported results in this paper used the linear kernel.
Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD. $$$$$ Input features can be mapped into high dimensional space before performing the optimization and classification.
Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD. $$$$$ For example, local collocations contribute the most for SVM, while parts-of-speech (POS) contribute the most for NB.
Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD. $$$$$ We also base our evaluation on both SENSEVAL-2 and SENSEVAL-1 official test data sets, and compare with the official scores of participating systems.

For every preposition, a baseline maxent model is trained using a set of features reported in the state-of-the-art WSD system of LeeandNg (2002). $$$$$ In addition, different learning algorithms benefit differently from feature selection.
For every preposition, a baseline maxent model is trained using a set of features reported in the state-of-the-art WSD system of LeeandNg (2002). $$$$$ Such an approach would be highly laborintensive, with questionable scalability.
For every preposition, a baseline maxent model is trained using a set of features reported in the state-of-the-art WSD system of LeeandNg (2002). $$$$$ The recent work of Pedersen (2001a) and Zavrel et al. (2000) evaluated a variety of learning algorithms on the SENSEVAL1 data set.

Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation. $$$$$ In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data.
Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation. $$$$$ The nine columns correspond to: (i) using only POS of neighboring words (ii) using only single words in the surrounding context with feature selection ( ) (iii) same as (ii) but without feature selection ( ) (iv) using only local collocations with feature selection ( ) (v) same as (iv) but without feature selection ( ) (vi) using only syntactic relations with feature selection on words ( ) (vii) same as (vi) but without feature selection ( ) (viii) combining all four knowledge sources with feature selection (ix) combining all four knowledge sources without feature selection.
Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation. $$$$$ For these words, we first used a POS tagger (Ratnaparkhi, 1996) to determine the correct POS.
Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation. $$$$$ In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data.

 $$$$$ We also base our evaluation on both SENSEVAL-2 and SENSEVAL-1 official test data sets, and compare with the official scores of participating systems.
 $$$$$ As shown in Table 2 and Table 3, the best microaveraged recall for SENSEVAL-2 (SENSEVAL-1) is 65.4% (79.2%), obtained by combining all knowledge sources (without feature selection) and using SVM as the learning algorithm.
 $$$$$ “ ” or “ ” means our rithms can already handle multi-class problems, and we denote runs using the original AdB, NB, and DT algorithms as “normal” in Table 2 and Table 3.

For each ambiguous word in ELS task of SENSEVAL-3, we used three types of features to capture contextual information $$$$$ We tried higher order polynomial kernels, but they gave poorer results.
For each ambiguous word in ELS task of SENSEVAL-3, we used three types of features to capture contextual information $$$$$ In this paper, we focus on a corpus-based, supervised learning approach.
For each ambiguous word in ELS task of SENSEVAL-3, we used three types of features to capture contextual information $$$$$ All learning parameters use the default values in WEKA unless otherwise stated.
For each ambiguous word in ELS task of SENSEVAL-3, we used three types of features to capture contextual information $$$$$ We will investigate the effect of more elaborate feature selection schemes on the performance of different learning algorithms for WSD in future work.

Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002). $$$$$ Input features can be mapped into high dimensional space before performing the optimization and classification.
Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002). $$$$$ Due to space constraints, we will only highlight prior research efforts that have investigated (1) contribution of various knowledge sources, or (2) relative performance of different learning algorithms.
Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002). $$$$$ In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data.
Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002). $$$$$ There are 8,611 training instances and 4,328 test instances tagged with WORDNET senses.

Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002). $$$$$ NB even outperforms SVM if only POS is used.
Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002). $$$$$ In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data.
Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002). $$$$$ Due to space constraints, we will only highlight prior research efforts that have investigated (1) contribution of various knowledge sources, or (2) relative performance of different learning algorithms.
Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002). $$$$$ WSD is a fundamental problem of natural language processing.

These feature types have been widely used in WSD algorithms (see Lee and Ng (2002) for an evaluation of their effectiveness). $$$$$ One could envisage building a WSD system using handcrafted rules or knowledge obtained from linguists.
These feature types have been widely used in WSD algorithms (see Lee and Ng (2002) for an evaluation of their effectiveness). $$$$$ Some examples are shown in Table 1.
These feature types have been widely used in WSD algorithms (see Lee and Ng (2002) for an evaluation of their effectiveness). $$$$$ All remaining tokens from all training contexts provided for are gathered.
These feature types have been widely used in WSD algorithms (see Lee and Ng (2002) for an evaluation of their effectiveness). $$$$$ We will investigate the effect of more elaborate feature selection schemes on the performance of different learning algorithms for WSD in future work.

These knowledge sources were effectively used to build a state-of-the-art WSD pro gram in one of our prior work (Lee and Ng, 2002). $$$$$ However, all of these research efforts concentrate only on evaluating different learning algorithms, without systematically considering their interaction with knowledge sources.
These knowledge sources were effectively used to build a state-of-the-art WSD pro gram in one of our prior work (Lee and Ng, 2002). $$$$$ Following the description in (Cabezas et al., 2001), our own re-implementation of UMD-SST gives a recall of 58.6%, close to their reported figure of 56.8%.
These knowledge sources were effectively used to build a state-of-the-art WSD pro gram in one of our prior work (Lee and Ng, 2002). $$$$$ Early research efforts on comparing different learning algorithms (Mooney, 1996; Pedersen and Bruce, 1997) tend to base their comparison on only one word or at most a dozen words.
These knowledge sources were effectively used to build a state-of-the-art WSD pro gram in one of our prior work (Lee and Ng, 2002). $$$$$ Ng and Lee (1996) reported the relative contribution of different knowledge sources, but on only one word “interest”.

Similar to our previous work (Chan and Ng, 2005b), we used the supervised WSD approach described in (Lee and Ng, 2002) for our experiments, using the naive Bayes algorithm as our classifier. $$$$$ Each occurrence of is manually tagged with the correct sense.
Similar to our previous work (Chan and Ng, 2005b), we used the supervised WSD approach described in (Lee and Ng, 2002) for our experiments, using the naive Bayes algorithm as our classifier. $$$$$ We also tabulate analogous figures for the top three participating systems for both SENSEVALs.
Similar to our previous work (Chan and Ng, 2005b), we used the supervised WSD approach described in (Lee and Ng, 2002) for our experiments, using the naive Bayes algorithm as our classifier. $$$$$ Similarly, the word me also points to the parent headword saw.
Similar to our previous work (Chan and Ng, 2005b), we used the supervised WSD approach described in (Lee and Ng, 2002) for our experiments, using the naive Bayes algorithm as our classifier. $$$$$ In contrast, in this paper, we systematically vary both knowledge sources and learning algorithms, and investigate the interaction between them.

For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local col locations, parts-of-speech (POS), and surrounding words. $$$$$ Several top SENSEVAL-2 participating systems have attempted the combination of classifiers using different learning algorithms.
For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local col locations, parts-of-speech (POS), and surrounding words. $$$$$ We also tabulate analogous figures for the top three participating systems for both SENSEVALs.
For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local col locations, parts-of-speech (POS), and surrounding words. $$$$$ There are 13,845 training instances1 for these trainable words, and 7,446 test instances.
For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local col locations, parts-of-speech (POS), and surrounding words. $$$$$ We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms.

The features used in these systems usually include local features, such as part-of-speech (POS) of neighboring words, local collocations, syntactic patterns and global features such as single words in the surrounding context (bag-of-words) (Lee and Ng, 2002). $$$$$ For example, if is the word bars and the set of selected unigrams is chocolate, iron, beer , the feature vector for the sentence “Reid saw me looking at the iron bars .” is 0, 1, 0 .
The features used in these systems usually include local features, such as part-of-speech (POS) of neighboring words, local collocations, syntactic patterns and global features such as single words in the surrounding context (bag-of-words) (Lee and Ng, 2002). $$$$$ The performance drop from 61.8% may be due to the different collocations used in the two systems.
The features used in these systems usually include local features, such as part-of-speech (POS) of neighboring words, local collocations, syntactic patterns and global features such as single words in the surrounding context (bag-of-words) (Lee and Ng, 2002). $$$$$ The t statistic of the difference between each pair of recall figures (between each test instance pair for micro-averaging and between each word task pair for macro-averaging) is computed, giving rise to a p value.
The features used in these systems usually include local features, such as part-of-speech (POS) of neighboring words, local collocations, syntactic patterns and global features such as single words in the surrounding context (bag-of-words) (Lee and Ng, 2002). $$$$$ In this paper, we focus on a corpus-based, supervised learning approach.

 $$$$$ However, they do not evaluate their method on a common benchmark data set, and there is no exploration on the interaction of knowledge sources with different learning algorithms.
 $$$$$ In particular, using all of these knowledge sources and SVM (i.e., a single learning algorithm) achieves accuracy higher than the best official scores on both SENSEVAL-2 and SENSEVAL-1 test data.
 $$$$$ In contrast, our implementation of SVM using the two knowledge sources of surrounding words and local collocations achieves recall of 61.8%.
 $$$$$ All the experimental results reported in this paper are obtained using the implementation of these algorithms in WEKA (Witten and Frank, 2000).

We adopt the same syntactic relations as (Lee and Ng, 2002). $$$$$ For example, if is the word bars and suppose the set of selected collocations for is a chocolate, the wine, the iron , then the feature value for collocation in the sentence “Reid saw me looking at the iron bars .” is the iron.
We adopt the same syntactic relations as (Lee and Ng, 2002). $$$$$ For SENSEVAL-1, we used the 36 trainable words for our evaluation.
We adopt the same syntactic relations as (Lee and Ng, 2002). $$$$$ The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms.

In the SVM (Vapnik, 1995) approach, we first form a training and a testing file using all standard features for each sense following (Lee and Ng, 2002) (one classifier per sense). $$$$$ A large p value indicates that the two systems are not significantly different from each other.
In the SVM (Vapnik, 1995) approach, we first form a training and a testing file using all standard features for each sense following (Lee and Ng, 2002) (one classifier per sense). $$$$$ For SENSEVAL-1, 4 trainable words belong to the indeterminate category, i.e., the POS is not provided.
In the SVM (Vapnik, 1995) approach, we first form a training and a testing file using all standard features for each sense following (Lee and Ng, 2002) (one classifier per sense). $$$$$ We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms.
In the SVM (Vapnik, 1995) approach, we first form a training and a testing file using all standard features for each sense following (Lee and Ng, 2002) (one classifier per sense). $$$$$ All the experimental results reported in this paper are obtained using the implementation of these algorithms in WEKA (Witten and Frank, 2000).
