 $$$$$ This allows us to train on smaller examples, focusing training more on the truly informative parts of the corpus.
 $$$$$ In our system, we measure D separately for each word, and use the average entropy over the word sequence as a measurement of disagreement for the example.
 $$$$$ Figure 1 presents the results of comparing the several selection methods against each other.
 $$$$$ In order to instantiate the general algorithm for larger committees, we need to define (i) a measure for disagreement (Step 3), and (ii) a selection criterion (Step 4).

Active learning has been successfully applied to a number of natural language oriented tasks, including text categorization (Lewis and Gale, 1994) and part-of-speech tagging (Engelson and Dagan, 1996). $$$$$ We then generalize the selection scheme, allowing more options to adapt and tune the approach for specific tasks.
Active learning has been successfully applied to a number of natural language oriented tasks, including text categorization (Lewis and Gale, 1994) and part-of-speech tagging (Engelson and Dagan, 1996). $$$$$ Cornmon features are the words involved in the attachment, such as the head verb or noun, the preposition, and the head word of the PP.
Active learning has been successfully applied to a number of natural language oriented tasks, including text categorization (Lewis and Gale, 1994) and part-of-speech tagging (Engelson and Dagan, 1996). $$$$$ This paper extends our previous on sample selection for probabilistic classifiers.
Active learning has been successfully applied to a number of natural language oriented tasks, including text categorization (Lewis and Gale, 1994) and part-of-speech tagging (Engelson and Dagan, 1996). $$$$$ Note that this type of uncertainty regarding the identity of the appropriate classification, is different than uncertainty regarding the correctness of the classification itself.

Engelson and Dagan (1996) experimented with QBC using HMMs for POS tagging and found that selective sampling of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies. $$$$$ Our experimental study of variants of the selection method suggests several practical conclusions.
Engelson and Dagan (1996) experimented with QBC using HMMs for POS tagging and found that selective sampling of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies. $$$$$ This allows us to train on smaller examples, focusing training more on the truly informative parts of the corpus.
Engelson and Dagan (1996) experimented with QBC using HMMs for POS tagging and found that selective sampling of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies. $$$$$ During training, the values of the parameters are estimated from a set of statistics, S, extracted from a training set of annotated examples.
Engelson and Dagan (1996) experimented with QBC using HMMs for POS tagging and found that selective sampling of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies. $$$$$ Applying committee-based selection to supervised training for such tasks can be done analogously to its application in the current papers.

In all experiments, the agreement among the decision committee members is quantified by the Vote Entropy measure (Engelson and Dagan, 1996). $$$$$ As the graphs show, the sample selection method achieves the same accuracy as complete training with fewer lexical and bigram counts.
In all experiments, the agreement among the decision committee members is quantified by the Vote Entropy measure (Engelson and Dagan, 1996). $$$$$ Committee-based selection thus addresses properties 1 and 2 simultaneously: it acquires statistics just when uncertainty in current parameter estimates entails uncertainty regarding the appropriate classification of the example.
In all experiments, the agreement among the decision committee members is quantified by the Vote Entropy measure (Engelson and Dagan, 1996). $$$$$ A more general algorithm results from allowing (i) a larger number of committee members, k, in order to sample P(MIS) more precisely, and (ii) more refined example selection criteria.
In all experiments, the agreement among the decision committee members is quantified by the Vote Entropy measure (Engelson and Dagan, 1996). $$$$$ We find that all variants achieve a significant reduction in annotation cost, though their computational efficiency differs.

Active learning also has been applied to many NLP applications, including POS tagging (Engelson and Dagan, 1996) and parsing (Baldridge and Osborne, 2003). $$$$$ The implicit modeling of uncertainty makes the selection system generally applicable and quite simple to implement.
Active learning also has been applied to many NLP applications, including POS tagging (Engelson and Dagan, 1996) and parsing (Baldridge and Osborne, 2003). $$$$$ Acknowledgments.
Active learning also has been applied to many NLP applications, including POS tagging (Engelson and Dagan, 1996) and parsing (Baldridge and Osborne, 2003). $$$$$ Note that this type of uncertainty regarding the identity of the appropriate classification, is different than uncertainty regarding the correctness of the classification itself.

This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). $$$$$ In Figure 2 we investigate further the properties of batch selection.
This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). $$$$$ To illustrate the generation of committeemembers, consider a model containing a single binomial parameter a (the probability of a success), with estimated value a.
This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). $$$$$ For example, the maximum likelihood estimate for a is a = k, giving the model M = {a} = {kJ.
This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). $$$$$ First, we describe the 'simplest' committee-based selection algorithm, which has no parameters to tune.

A common metric to estimate the disagreement within an ensemble is the so-called vote entropy, the entropy of the distribution of labels li assigned to an example e by the ensemble of k classifiers (Engelson and Dagan, 1996). $$$$$ We denote a particular model by M = {ai}, where each ai is a specific value for the corresponding ai.
A common metric to estimate the disagreement within an ensemble is the so-called vote entropy, the entropy of the distribution of labels li assigned to an example e by the ensemble of k classifiers (Engelson and Dagan, 1996). $$$$$ We propose reducing this cost significantly using committee7The use of a single model is also criticized in (Cohn, Atlas, and Ladner, 1994). based sample selection, which reduces redundant annotation of examples that contribute little new information.
A common metric to estimate the disagreement within an ensemble is the so-called vote entropy, the entropy of the distribution of labels li assigned to an example e by the ensemble of k classifiers (Engelson and Dagan, 1996). $$$$$ The other alternative is randomized selection, in which an example is selected for annotation based on the flip of a coin biased according to the vote entropy—a higher vote entropy entailing a higher probability of selection.
A common metric to estimate the disagreement within an ensemble is the so-called vote entropy, the entropy of the distribution of labels li assigned to an example e by the ensemble of k classifiers (Engelson and Dagan, 1996). $$$$$ Most simply, we can use a committee of size two and select an example when the two models disagree on its classification.

AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). $$$$$ For example, the maximum likelihood estimate for a is a = k, giving the model M = {a} = {kJ.
AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). $$$$$ In particular, we found that the simplest version of the method achieves a significant reduction in annotation cost, comparable to that of other versions.
AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). $$$$$ This section presents the framework and terminology assumed for probabilistic classification, as well as its instantiation for stochastic bigram part-ofspeech tagging.
AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). $$$$$ We approximate the posterior P(ai = ai IS) by first assuming that the multinomial is a collection of independent binomials, each of which corresponds to a single value ui of the multinomial; we then normalize the values so that they sum to 1.

Engelson and Dagan (1996) investigated several plausible approaches to the selection function but were unable to find significant differences among them. $$$$$ We also evaluate the computational efficiency of the different variants, and the number of unlabeled examples they consume.
Engelson and Dagan (1996) investigated several plausible approaches to the selection function but were unable to find significant differences among them. $$$$$ This avoids redundantly annotating examples that contribute little new information.
Engelson and Dagan (1996) investigated several plausible approaches to the selection function but were unable to find significant differences among them. $$$$$ Here we present and compare results for batch, randomized, thresholded, and two member committee-based selection.
Engelson and Dagan (1996) investigated several plausible approaches to the selection function but were unable to find significant differences among them. $$$$$ The posterior probability P(a, = adS) for the multinomial is given exactly by the Dirichlet distribution (Johnson, 1972) (which reduces to the Beta distribution in the binomial case).

QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelson and Dagan, 1996). $$$$$ Finally, we study the effect of sample selection on the size of the model acquired by the learner.
QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelson and Dagan, 1996). $$$$$ Consider the properties of those examples that are selected for training.
QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelson and Dagan, 1996). $$$$$ On the other hand, as N decreases, batch selection becomes closer to sequential selection.
QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelson and Dagan, 1996). $$$$$ The other alternative is randomized selection, in which an example is selected for annotation based on the flip of a coin biased according to the vote entropy—a higher vote entropy entailing a higher probability of selection.

This is measured by the vote entropy (Engelson and Dagan, 1996), i.e., the entropy of the distribution of classifications assigned to an example by the classifiers. $$$$$ In Figure 2 we investigate further the properties of batch selection.
This is measured by the vote entropy (Engelson and Dagan, 1996), i.e., the entropy of the distribution of classifications assigned to an example by the classifiers. $$$$$ Previous research in sample selection has used either sequential selection (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Dagan and Engelson, 1995), or batch selection (Lewis and Catlett, 1994; Lewis and Gale, 1994).

Engelson and Dagan (1996) confirm this observation that, in general, different (and even more refined) selection methods still yield similar results. $$$$$ For bigram tagging, comparative evaluation of the different variants of the method showed similar large reductions in annotation cost, suggesting the robustness of the committee-based approach.
Engelson and Dagan (1996) confirm this observation that, in general, different (and even more refined) selection methods still yield similar results. $$$$$ Let { ui} denote the set of possible values of a given multinomial variable, and let S = Ind denote a set of statistics extracted from the training set for that variable, where ni is the number of times that the value ui appears in the training set for the variable, defining N = Ei ni.
Engelson and Dagan (1996) confirm this observation that, in general, different (and even more refined) selection methods still yield similar results. $$$$$ Each multinomial random variable corresponds to a conditioning event and its values are given by the corresponding set of conditioned events.

 $$$$$ In general, a selected training example will contribute data to several statistics, which in turn will improve the estimates of several parameter values.
 $$$$$ When the statistics for a parameter are insufficient, the variance of the posterior distribution of the estimates is large, and hence there will be large differences in the values of the parameter chosen for different committee members.
 $$$$$ This problem is solved by considering each sentence as an individual example.
