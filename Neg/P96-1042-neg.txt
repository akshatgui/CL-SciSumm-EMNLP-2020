 $$$$$ In other words, the more statistics there are for estimating the parameter, the more similar are the parameter values used by different committee members.
 $$$$$ As the graphs show, the sample selection method achieves the same accuracy as complete training with fewer lexical and bigram counts.
 $$$$$ Note that this type of uncertainty regarding the identity of the appropriate classification, is different than uncertainty regarding the correctness of the classification itself.
 $$$$$ Vote entropy is maximized when all committee members disagree, and is zero when they all agree.

Active learning has been successfully applied to a number of natural language oriented tasks, including text categorization (Lewis and Gale, 1994) and part-of-speech tagging (Engelson and Dagan, 1996). $$$$$ Indeed, fully unsupervised training may not be feasible for certain tasks.
Active learning has been successfully applied to a number of natural language oriented tasks, including text categorization (Lewis and Gale, 1994) and part-of-speech tagging (Engelson and Dagan, 1996). $$$$$ Figure 1(a) shows the advantage that sample selection gives with regard to annotation cost.
Active learning has been successfully applied to a number of natural language oriented tasks, including text categorization (Lewis and Gale, 1994) and part-of-speech tagging (Engelson and Dagan, 1996). $$$$$ Our work focuses on sample selection for training probabilistic classifiers.
Active learning has been successfully applied to a number of natural language oriented tasks, including text categorization (Lewis and Gale, 1994) and part-of-speech tagging (Engelson and Dagan, 1996). $$$$$ Committee members are then generated by drawing models randomly from POI IS).

Engelson and Dagan (1996) experimented with QBC using HMMs for POS tagging and found that selective sampling of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies. $$$$$ In this paper, we investigate and extend the committee-based sample selection approach to minimizing training cost (Dagan and Engelson, 1995).
Engelson and Dagan (1996) experimented with QBC using HMMs for POS tagging and found that selective sampling of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies. $$$$$ This basic algorithm needs no parameters.
Engelson and Dagan (1996) experimented with QBC using HMMs for POS tagging and found that selective sampling of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies. $$$$$ Within the committee-based paradigm there exist different methods for selecting informative examples.
Engelson and Dagan (1996) experimented with QBC using HMMs for POS tagging and found that selective sampling of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies. $$$$$ For example, complete training requires annotated examples containing 98,000 ambiguous words to achieve a 92.6% accuracy (beyond the scale of the graph), while the selective methods require only 18,000-25,000 ambiguous words to achieve this accuracy.

In all experiments, the agreement among the decision committee members is quantified by the Vote Entropy measure (Engelson and Dagan, 1996). $$$$$ When training a bigram model (indeed, any HMM), this is not true, as each word is dependent on that before it.
In all experiments, the agreement among the decision committee members is quantified by the Vote Entropy measure (Engelson and Dagan, 1996). $$$$$ In practice, this estimator is usually smoothed in some way to compensate for data sparseness.
In all experiments, the agreement among the decision committee members is quantified by the Vote Entropy measure (Engelson and Dagan, 1996). $$$$$ The probabilistic model M, and thus the score function Fm, are defined by a set of parameters, {a}.
In all experiments, the agreement among the decision committee members is quantified by the Vote Entropy measure (Engelson and Dagan, 1996). $$$$$ Model parameters for which acquiring additional statistics is most beneficial can be characterized by the following three properties: ters that affect only few examples have low overall utility.

Active learning also has been applied to many NLP applications, including POS tagging (Engelson and Dagan, 1996) and parsing (Baldridge and Osborne, 2003). $$$$$ Furthermore, as batch size increases, computational efficiency, in terms of the number of examples examined to attain a given accuracy, decreases tremendously (Figure 2(b)).
Active learning also has been applied to many NLP applications, including POS tagging (Engelson and Dagan, 1996) and parsing (Baldridge and Osborne, 2003). $$$$$ Here we present and compare results for batch, randomized, thresholded, and two member committee-based selection.
Active learning also has been applied to many NLP applications, including POS tagging (Engelson and Dagan, 1996) and parsing (Baldridge and Osborne, 2003). $$$$$ We approximate the posterior P(ai = ai IS) by first assuming that the multinomial is a collection of independent binomials, each of which corresponds to a single value ui of the multinomial; we then normalize the values so that they sum to 1.
Active learning also has been applied to many NLP applications, including POS tagging (Engelson and Dagan, 1996) and parsing (Baldridge and Osborne, 2003). $$$$$ This basic algorithm needs no parameters.

This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). $$$$$ The statistics S for such a model are given by N, the number of trials, and x, the number of successes in those trials.
This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). $$$$$ Furthermore, committee-based selection may be attempted also for training non-probabilistic classifiers, where explicit modeling of information gain is typically impossible.
This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). $$$$$ Our experimental study of variants of the selection method suggests several practical conclusions.
This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). $$$$$ We describe a family of methods for committee-based sample selection, and report experimental results for the task of stochastic part-ofspeech tagging.

A common metric to estimate the disagreement within an ensemble is the so-called vote entropy, the entropy of the distribution of labels li assigned to an example e by the ensemble of k classifiers (Engelson and Dagan, 1996). $$$$$ This paper investigates methods for reducing annotacost by selection. this approach, during training the learning program examines many unlabeled examples and selects for labeling (annotation) only those that are most informative at each stage.
A common metric to estimate the disagreement within an ensemble is the so-called vote entropy, the entropy of the distribution of labels li assigned to an example e by the ensemble of k classifiers (Engelson and Dagan, 1996). $$$$$ For example, a transition probability parameter P(tiâ€”+t3) has conditioning event ti and conditioned event ti.
A common metric to estimate the disagreement within an ensemble is the so-called vote entropy, the entropy of the distribution of labels li assigned to an example e by the ensemble of k classifiers (Engelson and Dagan, 1996). $$$$$ We first review the basic approach of committeebased sample selection and its application to partof-speech tagging.
A common metric to estimate the disagreement within an ensemble is the so-called vote entropy, the entropy of the distribution of labels li assigned to an example e by the ensemble of k classifiers (Engelson and Dagan, 1996). $$$$$ The batch selection algorithm, executed for each batch B of N examples, is as follows: This procedure is repeated sequentially for successive batches of N examples, returning to the start of the corpus at the end.

AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). $$$$$ In stochastic part-of-speech tagging, the model assumed is a Hidden Markov Model (HMM), and input examples are sentences.
AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). $$$$$ Fm is determined by a probabilistic model M. In many applications, Fm is the conditional probability function, Pm (cle), specifying the probability of each class given the example, but other score functions that correlate with the likelihood of the class are often used.
AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). $$$$$ The ultimate reduction in annotation cost is achieved by unsupervised training methods, which do not require an annotated corpus at all (Kupiec, 1992; Merialdo, 1994; Elworthy, 1994).
AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). $$$$$ For each such binomial, we approximate P(ai = ai IS) as a truncated normal distribution (restricted to [0,1]), with estimated mean /./.

Engelson and Dagan (1996) investigated several plausible approaches to the selection function but were unable to find significant differences among them. $$$$$ Furthermore, committee-based selection may be attempted also for training non-probabilistic classifiers, where explicit modeling of information gain is typically impossible.
Engelson and Dagan (1996) investigated several plausible approaches to the selection function but were unable to find significant differences among them. $$$$$ Fm is determined by a probabilistic model M. In many applications, Fm is the conditional probability function, Pm (cle), specifying the probability of each class given the example, but other score functions that correlate with the likelihood of the class are often used.
Engelson and Dagan (1996) investigated several plausible approaches to the selection function but were unable to find significant differences among them. $$$$$ We see that while all selective methods are less efficient in terms of examples examined than complete training, they are comparable to each other.
Engelson and Dagan (1996) investigated several plausible approaches to the selection function but were unable to find significant differences among them. $$$$$ Both types of the parameters we sample have the form of multinomial distributions.

QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelson and Dagan, 1996). $$$$$ In this work we assumed a uniform prior distribution for each model parameter; we have not addressed the question of how to best choose a prior for this problem.
QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelson and Dagan, 1996). $$$$$ In particular, we found that the simplest version of the method achieves a significant reduction in annotation cost, comparable to that of other versions.
QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelson and Dagan, 1996). $$$$$ Furthermore, as batch size increases, computational efficiency, in terms of the number of examples examined to attain a given accuracy, decreases tremendously (Figure 2(b)).
QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelson and Dagan, 1996). $$$$$ Rather than evaluating examples individually for their informativeness a large batch of examples is examined, and the m best are selected for annotation.

This is measured by the vote entropy (Engelson and Dagan, 1996), i.e., the entropy of the distribution of classifications assigned to an example by the classifiers. $$$$$ When the statistics for a parameter are insufficient, the variance of the posterior distribution of the estimates is large, and hence there will be large differences in the values of the parameter chosen for different committee members.
This is measured by the vote entropy (Engelson and Dagan, 1996), i.e., the entropy of the distribution of classifications assigned to an example by the classifiers. $$$$$ The classifier then assigns the example to the class with the highest score.
This is measured by the vote entropy (Engelson and Dagan, 1996), i.e., the entropy of the distribution of classifications assigned to an example by the classifiers. $$$$$ An example is selected for labeling if the committee members largely disagree on its classification.
This is measured by the vote entropy (Engelson and Dagan, 1996), i.e., the entropy of the distribution of classifications assigned to an example by the classifiers. $$$$$ The tagger then assigns the sentence to the tag sequence which is most probable according to the HMM.

Engelson and Dagan (1996) confirm this observation that, in general, different (and even more refined) selection methods still yield similar results. $$$$$ A single model can be used to estimate only the second type of uncertainty, which does not correlate directly with the utility of additional training.
Engelson and Dagan (1996) confirm this observation that, in general, different (and even more refined) selection methods still yield similar results. $$$$$ The method can be applied in a semiinteractive process, in which the system selects several new examples for annotation at a time and updates its statistics after receiving their labels from the user.
Engelson and Dagan (1996) confirm this observation that, in general, different (and even more refined) selection methods still yield similar results. $$$$$ The reasoning is that if an example's classification is uncertain given current training data then the example is likely to contain unknown information useful for classifying similar examples in the future.
Engelson and Dagan (1996) confirm this observation that, in general, different (and even more refined) selection methods still yield similar results. $$$$$ A probabilistic classifier classifies input examples e by classes c E C, where C is a known set of possible classes.

 $$$$$ First, we describe the 'simplest' committee-based selection algorithm, which has no parameters to tune.
 $$$$$ This section presents the framework and terminology assumed for probabilistic classification, as well as its instantiation for stochastic bigram part-ofspeech tagging.
 $$$$$ The ultimate reduction in annotation cost is achieved by unsupervised training methods, which do not require an annotated corpus at all (Kupiec, 1992; Merialdo, 1994; Elworthy, 1994).
 $$$$$ We consider two alternative selection criteria (for Step 4).
