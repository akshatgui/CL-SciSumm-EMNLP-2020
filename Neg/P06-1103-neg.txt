Klementiev and Roth (2006) explore the use of a perceptron-based ranking model for the purpose of finding name transliterations across comparable corpora. $$$$$ If a dictionary is available, transliteration candidate lists on line 6 are augmented with translations.
Klementiev and Roth (2006) explore the use of a perceptron-based ranking model for the purpose of finding name transliterations across comparable corpora. $$$$$ The ultimate goal of this work is to automatically tag NEs so that they can be used for training of an NER system for a new language.
Klementiev and Roth (2006) explore the use of a perceptron-based ranking model for the purpose of finding name transliterations across comparable corpora. $$$$$ However, when initialized with the set of size 5, the algorithm never manages to improve.
Klementiev and Roth (2006) explore the use of a perceptron-based ranking model for the purpose of finding name transliterations across comparable corpora. $$$$$ For time sequence matching, we used a scoring metric novel in this domain.

 $$$$$ Note that although we expect that better use of language specific knowledge would improve the results, it would defeat one of the goals of this work.
 $$$$$ However, they focus on the classification stage of already segmented entities, and make use of contextual and morphological clues that require knowledge of the language beyond the level we want to assume with respect to the target language.
 $$$$$ We have proposed a novel algorithm for cross lingual multi-word NE discovery in a bilingual weakly temporally aligned corpus.

The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al, 2006) and train a discriminative classifier. $$$$$ NEs have similar time distributions across such corpora, and often some of the tokens in a multi-word NE are transliterated.
The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al, 2006) and train a discriminative classifier. $$$$$ (Shinyama and Sekine, 2004) used the idea to discover NEs, but in a single language, English, across two news sources.
The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al, 2006) and train a discriminative classifier. $$$$$ We have demonstrated that using two independent sources of information (transliteration and temporal similarity) together to guide NE extraction gives better performance than using either of them alone (see Figure 3).

 $$$$$ We thank Richard Sproat, ChengXiang Zhai, and Kevin Small for their useful feedback during this work, and the anonymous referees for their helpful comments.
 $$$$$ That is not required, however, as the model only needs to be good enough to place the correct transliteration anywhere in the candidate list.
 $$$$$ Named Entity recognition (NER) is an important part of many natural language processing tasks.

Our initial feature extraction method follows the one presented in (Klementiev and Roth, 2006a), in which the feature space consists of n-gram pairs from the two languages. $$$$$ We develop an algorithm that exploits both observations iteratively.
Our initial feature extraction method follows the one presented in (Klementiev and Roth, 2006a), in which the feature space consists of n-gram pairs from the two languages. $$$$$ Current approaches often employ machine learning techniques and require supervised data.
Our initial feature extraction method follows the one presented in (Klementiev and Roth, 2006a), in which the feature space consists of n-gram pairs from the two languages. $$$$$ Approaches that attempt to use these two characteristics separately to identify NEs across languages would have significant shortcomings.
Our initial feature extraction method follows the one presented in (Klementiev and Roth, 2006a), in which the feature space consists of n-gram pairs from the two languages. $$$$$ For evaluation, random 727 of the total of 978 NEs were matched to correct transliterations by a language expert (partly due to the fact that some of the English NEs were not mentioned in the Russian side of the corpus).

As stated by (Klementiev and Roth, 2006), the projection of NER tags is easier in comparison to projecting other types of annotations such as POS-tags and BPC. $$$$$ We have demonstrated that using two independent sources of information (transliteration and temporal similarity) together to guide NE extraction gives better performance than using either of them alone (see Figure 3).
As stated by (Klementiev and Roth, 2006), the projection of NER tags is easier in comparison to projecting other types of annotations such as POS-tags and BPC. $$$$$ We have proposed a novel algorithm for cross lingual multi-word NE discovery in a bilingual weakly temporally aligned corpus.
As stated by (Klementiev and Roth, 2006), the projection of NER tags is easier in comparison to projecting other types of annotations such as POS-tags and BPC. $$$$$ We expect that more language specific knowledge used to discover accurate equivalence classes would result in performance improvements.
As stated by (Klementiev and Roth, 2006), the projection of NER tags is easier in comparison to projecting other types of annotations such as POS-tags and BPC. $$$$$ We have demonstrated that using two independent sources of information (transliteration and temporal similarity) together to guide NE extraction gives better performance than using either of them alone (see Figure 3).

 $$$$$ We expect that more language specific knowledge used to discover accurate equivalence classes would result in performance improvements.
 $$$$$ We thank Richard Sproat, ChengXiang Zhai, and Kevin Small for their useful feedback during this work, and the anonymous referees for their helpful comments.
 $$$$$ In keeping with our objective to provide as little language knowledge as possible, we introduced a simplistic approach to identifying transliteration equivalence classes, which sometimes produced erroneous groupings (e.g. an equivalence class for NE congolese in Russian included both congo and congolese on Figure 6).
 $$$$$ We develop an algorithm that exploits both observations iteratively.

The iterative training algorithm described above is adopted from Klementiev and Roth (2006). $$$$$ We thank Richard Sproat, ChengXiang Zhai, and Kevin Small for their useful feedback during this work, and the anonymous referees for their helpful comments.
The iterative training algorithm described above is adopted from Klementiev and Roth (2006). $$$$$ Seeded with a small number of transliteration pairs, our algorithm discovers multi-word NEs, and takes advantage of a dictionary (if one exists) to account for translated or partially translated NEs.
The iterative training algorithm described above is adopted from Klementiev and Roth (2006). $$$$$ The algorithm can be naturally extended to comparable corpora of more than two languages.
The iterative training algorithm described above is adopted from Klementiev and Roth (2006). $$$$$ We thank Richard Sproat, ChengXiang Zhai, and Kevin Small for their useful feedback during this work, and the anonymous referees for their helpful comments.

(Klementiev and Roth, 2006) bootstrap with a classifier used interchangeably with an unsupervised temporal alignment method. $$$$$ We thank Richard Sproat, ChengXiang Zhai, and Kevin Small for their useful feedback during this work, and the anonymous referees for their helpful comments.
(Klementiev and Roth, 2006) bootstrap with a classifier used interchangeably with an unsupervised temporal alignment method. $$$$$ The few examples in the initial training set produce features corresponding to substring pairs characteristic for English-Russian transliterations.
(Klementiev and Roth, 2006) bootstrap with a classifier used interchangeably with an unsupervised temporal alignment method. $$$$$ The algorithm can be naturally extended to comparable corpora of more than two languages.
(Klementiev and Roth, 2006) bootstrap with a classifier used interchangeably with an unsupervised temporal alignment method. $$$$$ This research is supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program and a DOI grant under the Reflex program.

Our initial feature extraction scheme follows the one presented in (Klementiev and Roth, 2006), in which the feature space consists of n-gram pairs from the two languages. $$$$$ However, many languages lack such resources.
Our initial feature extraction scheme follows the one presented in (Klementiev and Roth, 2006), in which the feature space consists of n-gram pairs from the two languages. $$$$$ To this end, we would like to compare the performance of an NER system trained on a corpus tagged using this approach to one trained on a hand-tagged corpus.
Our initial feature extraction scheme follows the one presented in (Klementiev and Roth, 2006), in which the feature space consists of n-gram pairs from the two languages. $$$$$ This research is supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program and a DOI grant under the Reflex program.
Our initial feature extraction scheme follows the one presented in (Klementiev and Roth, 2006), in which the feature space consists of n-gram pairs from the two languages. $$$$$ NEs have similar time distributions across such corpora, and often some of the tokens in a multi-word NE are transliterated.

We evaluated our approach in two settings; first, we compared our system to a baseline system described in (Klementiev and Roth, 2006). $$$$$ We thank Richard Sproat, ChengXiang Zhai, and Kevin Small for their useful feedback during this work, and the anonymous referees for their helpful comments.
We evaluated our approach in two settings; first, we compared our system to a baseline system described in (Klementiev and Roth, 2006). $$$$$ Not all correct transliterations make it to the top of the candidates list (transliteration model by itself is never as accurate as the complete algorithm on Figure 3).
We evaluated our approach in two settings; first, we compared our system to a baseline system described in (Klementiev and Roth, 2006). $$$$$ We provided experimental evidence that this metric outperforms other scoring metrics traditionally used.

Note that one of the models proposed in (Klementiev and Roth, 2006b) takes advantage of the temporal information. $$$$$ This research is supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program and a DOI grant under the Reflex program.
Note that one of the models proposed in (Klementiev and Roth, 2006b) takes advantage of the temporal information. $$$$$ To this end, we would like to compare the performance of an NER system trained on a corpus tagged using this approach to one trained on a hand-tagged corpus.
Note that one of the models proposed in (Klementiev and Roth, 2006b) takes advantage of the temporal information. $$$$$ If a dictionary is available, each candidate list is augmented with translations (if they exist).

Our best model, the unsupervised learning with all constraints, outperforms both models in (Klementiev and Roth, 2006b), even though we do not use any temporal information. $$$$$ In order to keep to our objective of requiring as little language knowledge as possible, we took a rather simplistic approach for both languages of our corpus.
Our best model, the unsupervised learning with all constraints, outperforms both models in (Klementiev and Roth, 2006b), even though we do not use any temporal information. $$$$$ We expect that more language specific knowledge used to discover accurate equivalence classes would result in performance improvements.
Our best model, the unsupervised learning with all constraints, outperforms both models in (Klementiev and Roth, 2006b), even though we do not use any temporal information. $$$$$ The ultimate goal of this work is to automatically tag NEs so that they can be used for training of an NER system for a new language.
Our best model, the unsupervised learning with all constraints, outperforms both models in (Klementiev and Roth, 2006b), even though we do not use any temporal information. $$$$$ In essence, the algorithm we present uses temporal alignment as a supervision signal to iteratively train a transliteration model.

The Russian data set, originally introduced in (Klementiev and Roth, 2006b), is comprised of temporally aligned news articles. $$$$$ We developed a linear discriminative transliteration model, and presented a method to automatically generate features.
The Russian data set, originally introduced in (Klementiev and Roth, 2006b), is comprised of temporally aligned news articles. $$$$$ For time sequence matching, we used a scoring metric novel in this domain.
The Russian data set, originally introduced in (Klementiev and Roth, 2006b), is comprised of temporally aligned news articles. $$$$$ We tried various (sliding) window size for a perturbed corpus with (Table 2).

 $$$$$ This research is supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program and a DOI grant under the Reflex program.
 $$$$$ In our experiments, we paired substrings if their positions in their respective words differed by -1, 0, or 1.
 $$$$$ In order to keep to our objective of requiring as little language knowledge as possible, we took a rather simplistic approach for both languages of our corpus.

For Russian, we compare to the model presented in (Klementiev and Roth, 2006b), a weakly supervised algorithm that uses both phonetic information and temporal information. $$$$$ In the multi-word NE experiment, 282 random multi-word (2 or more) NEs and their transliterations/translations discovered by the algorithm were verified by a language expert.
For Russian, we compare to the model presented in (Klementiev and Roth, 2006b), a weakly supervised algorithm that uses both phonetic information and temporal information. $$$$$ However, when initialized with the set of size 5, the algorithm never manages to improve.

We compared our algorithm to two models described in (Klementiev and Roth, 2006b) one uses only phonetic similarity and the second also considers temporal co-occurrence similarity when ranking the transliteration candidates. $$$$$ On each iteration, it selects a list of top ranked transliteration candidates for each NE according to the current model (line 6).
We compared our algorithm to two models described in (Klementiev and Roth, 2006b) one uses only phonetic similarity and the second also considers temporal co-occurrence similarity when ranking the transliteration candidates. $$$$$ On each iteration, it selects a list of top ranked transliteration candidates for each NE according to the current model (line 6).
We compared our algorithm to two models described in (Klementiev and Roth, 2006b) one uses only phonetic similarity and the second also considers temporal co-occurrence similarity when ranking the transliteration candidates. $$$$$ Indeed, the weaker the temporal supervision the more we need to endow the model so that it can select cleaner candidates in the early iterations.
We compared our algorithm to two models described in (Klementiev and Roth, 2006b) one uses only phonetic similarity and the second also considers temporal co-occurrence similarity when ranking the transliteration candidates. $$$$$ The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).

This configuration is equivalent to the model used in (Klementiev and Roth, 2006b). $$$$$ (Knight and Graehl, 1997) build a generative model for backward transliteration from Japanese to English.
This configuration is equivalent to the model used in (Klementiev and Roth, 2006b). $$$$$ Finally, we verify that the NE is actually contained in the target corpus.
This configuration is equivalent to the model used in (Klementiev and Roth, 2006b). $$$$$ However, many languages lack such resources.

The extraction proceeds either iteratively by starting from a few seed extraction rules (Collins and Singer, 1999), or by mining named entities from comparable news articles (Shinyama and Sekine, 2004) or from multilingual corpora (Klementiev and Roth, 2006). $$$$$ While generative models are often robust, they tend to make independence assumptions that do not hold in data.
The extraction proceeds either iteratively by starting from a few seed extraction rules (Collins and Singer, 1999), or by mining named entities from comparable news articles (Shinyama and Sekine, 2004) or from multilingual corpora (Klementiev and Roth, 2006). $$$$$ The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).
The extraction proceeds either iteratively by starting from a few seed extraction rules (Collins and Singer, 1999), or by mining named entities from comparable news articles (Shinyama and Sekine, 2004) or from multilingual corpora (Klementiev and Roth, 2006). $$$$$ Positive examples were constructed by pairing an NE with the common stem of its transliteration equivalence class.

More recently, Klementiev and Roth (2006) also use F-index (Hetland, 2004), a score using DFT, to calculate the time distribution similarity. $$$$$ It then uses temporal alignment (with thresholding) to re-rank the list and select the best transliteration candidate for the next round of training (lines 8, and 9).
More recently, Klementiev and Roth (2006) also use F-index (Hetland, 2004), a score using DFT, to calculate the time distribution similarity. $$$$$ We thank Richard Sproat, ChengXiang Zhai, and Kevin Small for their useful feedback during this work, and the anonymous referees for their helpful comments.
More recently, Klementiev and Roth (2006) also use F-index (Hetland, 2004), a score using DFT, to calculate the time distribution similarity. $$$$$ The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).
More recently, Klementiev and Roth (2006) also use F-index (Hetland, 2004), a score using DFT, to calculate the time distribution similarity. $$$$$ For time sequence matching, we used a scoring metric novel in this domain.
