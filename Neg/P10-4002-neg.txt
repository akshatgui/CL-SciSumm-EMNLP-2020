For translation experiments, we used cdec (Dyer et al, 2010), a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system. $$$$$ present an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars.
For translation experiments, we used cdec (Dyer et al, 2010), a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system. $$$$$ Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the sponsors.
For translation experiments, we used cdec (Dyer et al, 2010), a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system. $$$$$ In word-based models, a single node may derive multiple different source language coverages since word based models impose no requirements on covering all words in the input.

Our implementation is mostly in Python on top of the cdec system (Dyer et al, 2010) via the pycdec interface (Chahuneau et al, 2012). $$$$$ From this unified representation, the decoder can not only the 1or translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques.
Our implementation is mostly in Python on top of the cdec system (Dyer et al, 2010) via the pycdec interface (Chahuneau et al, 2012). $$$$$ Additionally, each algorithm takes a weight function that maps from hypergraph edges to a value in K, making it possible to use many different semirings without altering the underlying hypergraph.
Our implementation is mostly in Python on top of the cdec system (Dyer et al, 2010) via the pycdec interface (Chahuneau et al, 2012). $$$$$ The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models.

The weights of the log-linear translation models were tuned towards the BLEU metric on development data using cdec's (Dyer et al, 2010) implementation of MERT (Och, 2003). $$$$$ This work was partially supported by the GALE program of the Defense Advanced Research Projects Agency, Contract No.
The weights of the log-linear translation models were tuned towards the BLEU metric on development data using cdec's (Dyer et al, 2010) implementation of MERT (Och, 2003). $$$$$ Second, existing open source decoders were designed with the traditional phrase-based parameterization using a very small number of dense features (typically less than 10). cdec has been designed from the ground up to support any parameterization, from those with a handful of dense features up to models with millions of sparse features (Blunsom et al., 2008; Chiang et al., 2009).
The weights of the log-linear translation models were tuned towards the BLEU metric on development data using cdec's (Dyer et al, 2010) implementation of MERT (Och, 2003). $$$$$ In these models, the translation model is trained to maximize conditional log likelihood of the training data under a specified grammar.

We implement Linear CP (LCP) on top of Cdec (Dyer et al, 2010), a widely-used hierarchical MT system that includes implementations of standard CP and FCP algorithms. $$$$$ For SCFG models and sequential tagging models, a node also corresponds to a source span and non-terminal type, but for word-based and phrase-based models, the relationship to the source string (or lattice) may be more complicated.
We implement Linear CP (LCP) on top of Cdec (Dyer et al, 2010), a widely-used hierarchical MT system that includes implementations of standard CP and FCP algorithms. $$$$$ Note that because of our representation, built-in types like double, int, and bool (together with their default operators) are semirings.
We implement Linear CP (LCP) on top of Cdec (Dyer et al, 2010), a widely-used hierarchical MT system that includes implementations of standard CP and FCP algorithms. $$$$$ Once a forest has been constructed representing the possible translations, general inference algorithms can be applied.

In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG). $$$$$ Multiplication must distribute over addition, and v ® 0 must equal 0.
In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG). $$$$$ The design of cdec separates the creation of a translation forest from its rescoring with a language models or similar models.3 Since the structure of the unified search space is context free (§3), we use the logic for language model rescoring described by Chiang (2007), although any weighted intersection algorithm can be applied.
In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG). $$$$$ This work was partially supported by the GALE program of the Defense Advanced Research Projects Agency, Contract No.
In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG). $$$$$ Both the objective (conditional log likelihood) and its gradient have the form of a difference in two quantities: each has one term that is computed over the translation hypergraph which is subtracted from the result of the same computation over the alignment hypergraph (refer to Figures 1 and 2).

Finally, the cdec decoder (Dyer et al, 2010) includes a grammar extractor that performs well only when all rules can be held in memory. $$$$$ Edges are associated with exactly one synchronous production in the source and target language, and alternative translation possibilities are expressed as alternative edges.
Finally, the cdec decoder (Dyer et al, 2010) includes a grammar extractor that performs well only when all rules can be held in memory. $$$$$ Further support was provided the EuroMatrix project funded by the European Commission (7th Framework Programme).
Finally, the cdec decoder (Dyer et al, 2010) includes a grammar extractor that performs well only when all rules can be held in memory. $$$$$ Hence all model types benefit immediately from new algorithms (for rescoring, inference, etc.
Finally, the cdec decoder (Dyer et al, 2010) includes a grammar extractor that performs well only when all rules can be held in memory. $$$$$ ); new models can be more easily prototyped; and controlled comparison of models is made easier.

We use the cdec decoder (Dyer et al, 2010) with default settings for this purpose. $$$$$ Further support was provided the EuroMatrix project funded by the European Commission (7th Framework Programme).
We use the cdec decoder (Dyer et al, 2010) with default settings for this purpose. $$$$$ Two monolingual parses are better than one (synchronous parse).
We use the cdec decoder (Dyer et al, 2010) with default settings for this purpose. $$$$$ In this view, translation (or tagging) deductions have the structure of a context-free forest, or directed hypergraph, where edges have a single head and 0 or more tail nodes (Nederhof, 2003).

Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al, 2010). $$$$$ Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.
Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al, 2010). $$$$$ When used with sequential tagging models, this pipeline is identical to traditional sequential CRF training (Sha and Pereira, 2003).
Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al, 2010). $$$$$ Table 1 shows the C++ representation used for semirings.

 $$$$$ Furthermore, prior to language model integration (and distortion model integration, in the case of phrase based translation), pruning is unnecessary for most kinds of models, further simplifying the model-specific code.
 $$$$$ Hence all model types benefit immediately from new algorithms (for rescoring, inference, etc.
 $$$$$ Since gradient-based optimization techniques may require thousands of evaluations to converge, the batch training pipeline is split into map and reduce components, facilitating distribution over very large clusters.
 $$$$$ Multiplication must distribute over addition, and v ® 0 must equal 0.

We implemented UD on top of a widely-used HMT open-source system, cdec (Dyer et al, 2010). $$$$$ Further support was provided the EuroMatrix project funded by the European Commission (7th Framework Programme).
We implemented UD on top of a widely-used HMT open-source system, cdec (Dyer et al, 2010). $$$$$ Although intersection using the Chiang algorithm runs in polynomial time and space, the resulting rescored forest may still be too large to represent completely. cdec therefore supports three pruning strategies that can be used during intersection: full unpruned intersection (useful for tagging models to incorporate, e.g., Markov features, but not generally practical for translation), cube pruning, and cube growing (Huang and Chiang, 2007).
We implemented UD on top of a widely-used HMT open-source system, cdec (Dyer et al, 2010). $$$$$ Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.
We implemented UD on top of a widely-used HMT open-source system, cdec (Dyer et al, 2010). $$$$$ The conditional log likelihood is the difference in the log partition of the translation and alignment hypergraph, and is computed using the INSIDE algorithm.

We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. $$$$$ Thus, many of the semiring types define not only the elements shown in Table 1 but T::operator< as well.
We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. $$$$$ Multiplication and addition must be associative.
We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. $$$$$ Two monolingual parses are better than one (synchronous parse).
We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. $$$$$ Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006).

 $$$$$ Although intersection using the Chiang algorithm runs in polynomial time and space, the resulting rescored forest may still be too large to represent completely. cdec therefore supports three pruning strategies that can be used during intersection: full unpruned intersection (useful for tagging models to incorporate, e.g., Markov features, but not generally practical for translation), cube pruning, and cube growing (Huang and Chiang, 2007).
 $$$$$ An edge’s output label may contain mixtures of terminal symbol yields and positions indicating where a child node’s yield should be substituted.
 $$$$$ Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.
 $$$$$ Second, existing open source decoders were designed with the traditional phrase-based parameterization using a very small number of dense features (typically less than 10). cdec has been designed from the ground up to support any parameterization, from those with a handful of dense features up to models with millions of sparse features (Blunsom et al., 2008; Chiang et al., 2009).

In our work, we use hierarchical phrase-based translation (Chiang, 2007), as implemented in the cdec framework (Dyer et al, 2010). $$$$$ In these models, the translation model is trained to maximize conditional log likelihood of the training data under a specified grammar.
In our work, we use hierarchical phrase-based translation (Chiang, 2007), as implemented in the cdec framework (Dyer et al, 2010). $$$$$ Figure 3 illustrates two example hypergraphs, one generated using a SCFG model and other from a phrase-based model.
In our work, we use hierarchical phrase-based translation (Chiang, 2007), as implemented in the cdec framework (Dyer et al, 2010). $$$$$ Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the sponsors.
In our work, we use hierarchical phrase-based translation (Chiang, 2007), as implemented in the cdec framework (Dyer et al, 2010). $$$$$ The reduce function aggregates the results and performs the optimization using standard algorithms, including LBFGS (Liu et al., 1989), RPROP (Riedmiller and Braun, 1993), and stochastic gradient descent.

The work reported in this paper was carried out while the author was at the University of Cambridge.It has been noted that line optimisation over a lattice can be implemented as a semi-ring of sets of linear functions (Dyer et al, 2010). $$$$$ Multiplication and addition must be associative.
The work reported in this paper was carried out while the author was at the University of Cambridge.It has been noted that line optimisation over a lattice can be implemented as a semi-ring of sets of linear functions (Dyer et al, 2010). $$$$$ In cdec, the only model-specific logic is confined to the first step in the process where an input string (or lattice, etc.) is transduced into the unified hypergraph representation.
The work reported in this paper was carried out while the author was at the University of Cambridge.It has been noted that line optimisation over a lattice can be implemented as a semi-ring of sets of linear functions (Dyer et al, 2010). $$$$$ Discussions with Philipp Koehn, Chris Callison-Burch, Zhifei Li, Lane Schwarz, and Jimmy Lin were likewise crucial to the successful execution of this project.
The work reported in this paper was carried out while the author was at the University of Cambridge.It has been noted that line optimisation over a lattice can be implemented as a semi-ring of sets of linear functions (Dyer et al, 2010). $$$$$ Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al., 2009).

Grammars were extracted from the resulting parallel text and used in our hierarchical phrase-based system using cdec (Dyer et al, 2010) as the decoder. $$$$$ Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 2009).
Grammars were extracted from the resulting parallel text and used in our hierarchical phrase-based system using cdec (Dyer et al, 2010) as the decoder. $$$$$ This makes it difficult to explore alternative translation models without also re-implementing rescoring and pruning logic.
Grammars were extracted from the resulting parallel text and used in our hierarchical phrase-based system using cdec (Dyer et al, 2010) as the decoder. $$$$$ Once this unscored translation forest has been generated, any non-coaccessible states (i.e., states that are not reachable from the goal node) are removed and the resulting structure is rescored with language models using a user-specified intersection/pruning strategy (§4) resulting in a rescored translation forest and completing phase 1.
Grammars were extracted from the resulting parallel text and used in our hierarchical phrase-based system using cdec (Dyer et al, 2010) as the decoder. $$$$$ The gradient with respect to a particular feature is the difference in this feature’s expected value in the translation and alignment hypergraphs, and can be computed using either INSIDEOUTSIDE or the expectation semiring and INSIDE.

We have evaluated the one-translation-per-discourse feature using the cdecMT system (Dyer et al, 2010). $$$$$ Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.
We have evaluated the one-translation-per-discourse feature using the cdecMT system (Dyer et al, 2010). $$$$$ The design of cdec separates the creation of a translation forest from its rescoring with a language models or similar models.3 Since the structure of the unified search space is context free (§3), we use the logic for language model rescoring described by Chiang (2007), although any weighted intersection algorithm can be applied.
We have evaluated the one-translation-per-discourse feature using the cdecMT system (Dyer et al, 2010). $$$$$ present an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars.
We have evaluated the one-translation-per-discourse feature using the cdecMT system (Dyer et al, 2010). $$$$$ Semirings are a useful mathematical abstraction for dealing with translation forests since many useful quantities can be computed using a single linear-time algorithm but with different semirings.

An efficient implementation that integrates well with the open source cdec SMT system (Dyer et al., 2010). $$$$$ The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003).
An efficient implementation that integrates well with the open source cdec SMT system (Dyer et al., 2010). $$$$$ The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006).
An efficient implementation that integrates well with the open source cdec SMT system (Dyer et al., 2010). $$$$$ Joshua was run on the Sun HotSpot JVM, version 1.6.0 12.

We used cdec (Dyer et al, 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al, 2002) on the NIST MT06 corpus. $$$$$ Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009).
We used cdec (Dyer et al, 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al, 2002) on the NIST MT06 corpus. $$$$$ In these models, the translation model is trained to maximize conditional log likelihood of the training data under a specified grammar.
We used cdec (Dyer et al, 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al, 2002) on the NIST MT06 corpus. $$$$$ The design of cdec separates the creation of a translation forest from its rescoring with a language models or similar models.3 Since the structure of the unified search space is context free (§3), we use the logic for language model rescoring described by Chiang (2007), although any weighted intersection algorithm can be applied.
We used cdec (Dyer et al, 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al, 2002) on the NIST MT06 corpus. $$$$$ For tagging, word-based, and phrase-based models, these are strictly arranged in a monotone, leftbranching structure.

In all experiments, our MT system learned a synchronous context-free grammar (Chiang, 2007), using GIZA++ for word alignments, MIRA for parameter tuning (Crammer et al, 2006) ,cdec for decoding (Dyer et al, 2010), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation. $$$$$ ); new models can be more easily prototyped; and controlled comparison of models is made easier.
In all experiments, our MT system learned a synchronous context-free grammar (Chiang, 2007), using GIZA++ for word alignments, MIRA for parameter tuning (Crammer et al, 2006) ,cdec for decoding (Dyer et al, 2010), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation. $$$$$ Recent research has proposed a unified representation for the various translation and tagging formalisms that is based on weighted logic programming (Lopez, 2009).
In all experiments, our MT system learned a synchronous context-free grammar (Chiang, 2007), using GIZA++ for word alignments, MIRA for parameter tuning (Crammer et al, 2006) ,cdec for decoding (Dyer et al, 2010), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation. $$$$$ Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009).

To date, several open-source SMT systems (based on either phrase based models or syntax-based models) have been developed, such as Moses (Koehn et al, 2007), Joshua (Li et al, 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al, 2010), cdec (Dyer et al, 2010), Jane (Vilar et al, 2010) and SilkRoad, and offer good references for the development of the NiuTrans toolkit. $$$$$ General rescoring (with language models or other models), pruning, inference, and alignment algorithms then apply to the unified data structure (§4).
To date, several open-source SMT systems (based on either phrase based models or syntax-based models) have been developed, such as Moses (Koehn et al, 2007), Joshua (Li et al, 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al, 2010), cdec (Dyer et al, 2010), Jane (Vilar et al, 2010) and SilkRoad, and offer good references for the development of the NiuTrans toolkit. $$$$$ Since log likelihood is differentiable with respect to the feature weights in an exponential model, it is possible to use gradient-based optimization techniques to train the system, enabling the parameterization of the model using millions of sparse features.
To date, several open-source SMT systems (based on either phrase based models or syntax-based models) have been developed, such as Moses (Koehn et al, 2007), Joshua (Li et al, 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al, 2010), cdec (Dyer et al, 2010), Jane (Vilar et al, 2010) and SilkRoad, and offer good references for the development of the NiuTrans toolkit. $$$$$ Hence all model types benefit immediately from new algorithms (for rescoring, inference, etc.
To date, several open-source SMT systems (based on either phrase based models or syntax-based models) have been developed, such as Moses (Koehn et al, 2007), Joshua (Li et al, 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al, 2010), cdec (Dyer et al, 2010), Jane (Vilar et al, 2010) and SilkRoad, and offer good references for the development of the NiuTrans toolkit. $$$$$ This makes it difficult to explore alternative translation models without also re-implementing rescoring and pruning logic.
