For translation experiments, we used cdec (Dyer et al, 2010), a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system. $$$$$ A generic first-order expectation semiring is also provided (Li and Eisner, 2009).
For translation experiments, we used cdec (Dyer et al, 2010), a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system. $$$$$ The first have zero tail nodes (i.e., an arity of 0), and correspond to word or phrase translation pairs (with all translation options existing on edges deriving the same head node), or glue rules that glue phrases together.

Our implementation is mostly in Python on top of the cdec system (Dyer et al, 2010) via the pycdec interface (Chahuneau et al, 2012). $$$$$ We are taking advantage of its modular design to study alternative algorithms for language model integration.
Our implementation is mostly in Python on top of the cdec system (Dyer et al, 2010) via the pycdec interface (Chahuneau et al, 2012). $$$$$ The design of cdec separates the creation of a translation forest from its rescoring with a language models or similar models.3 Since the structure of the unified search space is context free (§3), we use the logic for language model rescoring described by Chiang (2007), although any weighted intersection algorithm can be applied.
Our implementation is mostly in Python on top of the cdec system (Dyer et al, 2010) via the pycdec interface (Chahuneau et al, 2012). $$$$$ The design of cdec separates the creation of a translation forest from its rescoring with a language models or similar models.3 Since the structure of the unified search space is context free (§3), we use the logic for language model rescoring described by Chiang (2007), although any weighted intersection algorithm can be applied.
Our implementation is mostly in Python on top of the cdec system (Dyer et al, 2010) via the pycdec interface (Chahuneau et al, 2012). $$$$$ We are also improving support for parallel training using Hadoop (an open-source implementation of MapReduce).

The weights of the log-linear translation models were tuned towards the BLEU metric on development data using cdec's (Dyer et al, 2010) implementation of MERT (Och, 2003). $$$$$ Semirings are a useful mathematical abstraction for dealing with translation forests since many useful quantities can be computed using a single linear-time algorithm but with different semirings.
The weights of the log-linear translation models were tuned towards the BLEU metric on development data using cdec's (Dyer et al, 2010) implementation of MERT (Och, 2003). $$$$$ A generic first-order expectation semiring is also provided (Li and Eisner, 2009).
The weights of the log-linear translation models were tuned towards the BLEU metric on development data using cdec's (Dyer et al, 2010) implementation of MERT (Och, 2003). $$$$$ The design of cdec separates the creation of a translation forest from its rescoring with a language models or similar models.3 Since the structure of the unified search space is context free (§3), we use the logic for language model rescoring described by Chiang (2007), although any weighted intersection algorithm can be applied.

We implement Linear CP (LCP) on top of Cdec (Dyer et al, 2010), a widely-used hierarchical MT system that includes implementations of standard CP and FCP algorithms. $$$$$ The last option generates an alignment forest, from which a word alignment or feature expectations can be extracted.
We implement Linear CP (LCP) on top of Cdec (Dyer et al, 2010), a widely-used hierarchical MT system that includes implementations of standard CP and FCP algorithms. $$$$$ present an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars.
We implement Linear CP (LCP) on top of Cdec (Dyer et al, 2010), a widely-used hierarchical MT system that includes implementations of standard CP and FCP algorithms. $$$$$ A semiring is a 5-tuple (K, ®, ®, 0,1) that indicates the set from which the values will be drawn, K, a generic addition and multiplication operation, ® and ®, and their identities 0 and 1.
We implement Linear CP (LCP) on top of Cdec (Dyer et al, 2010), a widely-used hierarchical MT system that includes implementations of standard CP and FCP algorithms. $$$$$ Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.

In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG). $$$$$ In the case of SCFG grammars, the edges correspond simply to rules in the synchronous grammar.
In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG). $$$$$ The decoding pipeline consists of two phases.
In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG). $$$$$ In cdec, the only model-specific logic is confined to the first step in the process where an input string (or lattice, etc.) is transduced into the unified hypergraph representation.
In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG). $$$$$ The gradient with respect to a particular feature is the difference in this feature’s expected value in the translation and alignment hypergraphs, and can be computed using either INSIDEOUTSIDE or the expectation semiring and INSIDE.

Finally, the cdec decoder (Dyer et al, 2010) includes a grammar extractor that performs well only when all rules can be held in memory. $$$$$ All decoders produced a BLEU score between 31.4 and 31.6 (small differences are accounted for by different tie-breaking behavior and OOV handling). formalism=scfg grammar=grammar.mt03.scfg.gz add pass through rules=true scfg max span limit=15 feature function=LanguageModel en.3gram.pruned.lm.gz -o 3 feature function=WordPenalty intersection strategy=cube pruning cubepruning pop limit=30 8 Future work C. Dyer.
Finally, the cdec decoder (Dyer et al, 2010) includes a grammar extractor that performs well only when all rules can be held in memory. $$$$$ In cdec’s translation hypergraph, a node represents a contiguous sequence of target language words.
Finally, the cdec decoder (Dyer et al, 2010) includes a grammar extractor that performs well only when all rules can be held in memory. $$$$$ From this unified representation, the decoder can not only the 1or translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques.

We use the cdec decoder (Dyer et al, 2010) with default settings for this purpose. $$$$$ This alignment algorithm is explored in depth by Dyer (2010).
We use the cdec decoder (Dyer et al, 2010) with default settings for this purpose. $$$$$ Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 2009).
We use the cdec decoder (Dyer et al, 2010) with default settings for this purpose. $$$$$ Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007).

Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al, 2010). $$$$$ Edges are associated with exactly one synchronous production in the source and target language, and alternative translation possibilities are expressed as alternative edges.
Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al, 2010). $$$$$ ); new models can be more easily prototyped; and controlled comparison of models is made easier.
Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al, 2010). $$$$$ Table 1 shows the C++ representation used for semirings.

 $$$$$ All of these will be made publicly available as the projects progress.
 $$$$$ Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.
 $$$$$ ); new models can be more easily prototyped; and controlled comparison of models is made easier.
 $$$$$ Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009).

We implemented UD on top of a widely-used HMT open-source system, cdec (Dyer et al, 2010). $$$$$ Edges are associated with exactly one synchronous production in the source and target language, and alternative translation possibilities are expressed as alternative edges.
We implemented UD on top of a widely-used HMT open-source system, cdec (Dyer et al, 2010). $$$$$ Discussions with Philipp Koehn, Chris Callison-Burch, Zhifei Li, Lane Schwarz, and Jimmy Lin were likewise crucial to the successful execution of this project.
We implemented UD on top of a widely-used HMT open-source system, cdec (Dyer et al, 2010). $$$$$ We are also improving support for parallel training using Hadoop (an open-source implementation of MapReduce).

We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. $$$$$ In this view, translation (or tagging) deductions have the structure of a context-free forest, or directed hypergraph, where edges have a single head and 0 or more tail nodes (Nederhof, 2003).
We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. $$$$$ Although Viterbi and k-best extraction algorithms are often expressed as INSIDE algorithms with the tropical semiring, cdec provides a separate derivation extraction framework that makes use of a < operator (Huang and Chiang, 2005).
We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. $$$$$ The design of cdec separates the creation of a translation forest from its rescoring with a language models or similar models.3 Since the structure of the unified search space is context free (§3), we use the logic for language model rescoring described by Chiang (2007), although any weighted intersection algorithm can be applied.
We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. $$$$$ In addition to the widely used MERT algorithm, cdec also provides a training pipeline for discriminatively trained probabilistic translation models (Blunsom et al., 2008; Blunsom and Osborne, 2008).

 $$$$$ Multiplication and addition must be associative.
 $$$$$ Table 1 shows the C++ representation used for semirings.
 $$$$$ The second phase of the decoding pipeline (depicted in Figure 2) computes a value from the rescored forest: 1- or k-best derivations, feature expectations, or intersection with a target language reference (sentence or lattice).

In our work, we use hierarchical phrase-based translation (Chiang, 2007), as implemented in the cdec framework (Dyer et al, 2010). $$$$$ Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms.
In our work, we use hierarchical phrase-based translation (Chiang, 2007), as implemented in the cdec framework (Dyer et al, 2010). $$$$$ All of these will be made publicly available as the projects progress.
In our work, we use hierarchical phrase-based translation (Chiang, 2007), as implemented in the cdec framework (Dyer et al, 2010). $$$$$ Joshua was run on the Sun HotSpot JVM, version 1.6.0 12.
In our work, we use hierarchical phrase-based translation (Chiang, 2007), as implemented in the cdec framework (Dyer et al, 2010). $$$$$ The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models.

The work reported in this paper was carried out while the author was at the University of Cambridge.It has been noted that line optimisation over a lattice can be implemented as a semi-ring of sets of linear functions (Dyer et al, 2010). $$$$$ Furthermore, prior to language model integration (and distortion model integration, in the case of phrase based translation), pruning is unnecessary for most kinds of models, further simplifying the model-specific code.
The work reported in this paper was carried out while the author was at the University of Cambridge.It has been noted that line optimisation over a lattice can be implemented as a semi-ring of sets of linear functions (Dyer et al, 2010). $$$$$ Joshua was run on the Sun HotSpot JVM, version 1.6.0 12.

Grammars were extracted from the resulting parallel text and used in our hierarchical phrase-based system using cdec (Dyer et al, 2010) as the decoder. $$$$$ In particular, by defining a semiring whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the INSIDE algorithm.
Grammars were extracted from the resulting parallel text and used in our hierarchical phrase-based system using cdec (Dyer et al, 2010) as the decoder. $$$$$ Second, existing open source decoders were designed with the traditional phrase-based parameterization using a very small number of dense features (typically less than 10). cdec has been designed from the ground up to support any parameterization, from those with a handful of dense features up to models with millions of sparse features (Blunsom et al., 2008; Chiang et al., 2009).
Grammars were extracted from the resulting parallel text and used in our hierarchical phrase-based system using cdec (Dyer et al, 2010) as the decoder. $$$$$ The conditional log likelihood is the difference in the log partition of the translation and alignment hypergraph, and is computed using the INSIDE algorithm.
Grammars were extracted from the resulting parallel text and used in our hierarchical phrase-based system using cdec (Dyer et al, 2010) as the decoder. $$$$$ While this training approach was originally proposed for SCFG-based translation models, it can be used to train any model type in cdec.

We have evaluated the one-translation-per-discourse feature using the cdecMT system (Dyer et al, 2010). $$$$$ Joshua was run on the Sun HotSpot JVM, version 1.6.0 12.
We have evaluated the one-translation-per-discourse feature using the cdecMT system (Dyer et al, 2010). $$$$$ In cdec, model-specific code is only required to construct a translation forest (§3).
We have evaluated the one-translation-per-discourse feature using the cdecMT system (Dyer et al, 2010). $$$$$ In cdec, the only model-specific logic is confined to the first step in the process where an input string (or lattice, etc.) is transduced into the unified hypergraph representation.
We have evaluated the one-translation-per-discourse feature using the cdecMT system (Dyer et al, 2010). $$$$$ All of these will be made publicly available as the projects progress.

An efficient implementation that integrates well with the open source cdec SMT system (Dyer et al., 2010). $$$$$ Multiplication and addition must be associative.
An efficient implementation that integrates well with the open source cdec SMT system (Dyer et al., 2010). $$$$$ All decoders produced a BLEU score between 31.4 and 31.6 (small differences are accounted for by different tie-breaking behavior and OOV handling). formalism=scfg grammar=grammar.mt03.scfg.gz add pass through rules=true scfg max span limit=15 feature function=LanguageModel en.3gram.pruned.lm.gz -o 3 feature function=WordPenalty intersection strategy=cube pruning cubepruning pop limit=30 8 Future work C. Dyer.
An efficient implementation that integrates well with the open source cdec SMT system (Dyer et al., 2010). $$$$$ The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006).
An efficient implementation that integrates well with the open source cdec SMT system (Dyer et al., 2010). $$$$$ Since the model-specific code need not worry about integration with rescoring models, it can be made quite simple and efficient.

We used cdec (Dyer et al, 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al, 2002) on the NIST MT06 corpus. $$$$$ Although intersection using the Chiang algorithm runs in polynomial time and space, the resulting rescored forest may still be too large to represent completely. cdec therefore supports three pruning strategies that can be used during intersection: full unpruned intersection (useful for tagging models to incorporate, e.g., Markov features, but not generally practical for translation), cube pruning, and cube growing (Huang and Chiang, 2007).
We used cdec (Dyer et al, 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al, 2002) on the NIST MT06 corpus. $$$$$ An edge’s output label may contain mixtures of terminal symbol yields and positions indicating where a child node’s yield should be substituted.
We used cdec (Dyer et al, 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al, 2002) on the NIST MT06 corpus. $$$$$ Edges are further annotated with feature values, and are annotated with the source span vector the edge corresponds to.

In all experiments, our MT system learned a synchronous context-free grammar (Chiang, 2007), using GIZA++ for word alignments, MIRA for parameter tuning (Crammer et al, 2006) ,cdec for decoding (Dyer et al, 2010), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation. $$$$$ In cdec, the only model-specific logic is confined to the first step in the process where an input string (or lattice, etc.) is transduced into the unified hypergraph representation.
In all experiments, our MT system learned a synchronous context-free grammar (Chiang, 2007), using GIZA++ for word alignments, MIRA for parameter tuning (Crammer et al, 2006) ,cdec for decoding (Dyer et al, 2010), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation. $$$$$ Discussions with Philipp Koehn, Chris Callison-Burch, Zhifei Li, Lane Schwarz, and Jimmy Lin were likewise crucial to the successful execution of this project.
In all experiments, our MT system learned a synchronous context-free grammar (Chiang, 2007), using GIZA++ for word alignments, MIRA for parameter tuning (Crammer et al, 2006) ,cdec for decoding (Dyer et al, 2010), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation. $$$$$ present an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars.
In all experiments, our MT system learned a synchronous context-free grammar (Chiang, 2007), using GIZA++ for word alignments, MIRA for parameter tuning (Crammer et al, 2006) ,cdec for decoding (Dyer et al, 2010), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation. $$$$$ The highlighted path is the workflow used in the test reported in §7.

To date, several open-source SMT systems (based on either phrase based models or syntax-based models) have been developed, such as Moses (Koehn et al, 2007), Joshua (Li et al, 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al, 2010), cdec (Dyer et al, 2010), Jane (Vilar et al, 2010) and SilkRoad, and offer good references for the development of the NiuTrans toolkit. $$$$$ Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms.
To date, several open-source SMT systems (based on either phrase based models or syntax-based models) have been developed, such as Moses (Koehn et al, 2007), Joshua (Li et al, 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al, 2010), cdec (Dyer et al, 2010), Jane (Vilar et al, 2010) and SilkRoad, and offer good references for the development of the NiuTrans toolkit. $$$$$ Multiplication and addition must be associative.
To date, several open-source SMT systems (based on either phrase based models or syntax-based models) have been developed, such as Moses (Koehn et al, 2007), Joshua (Li et al, 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al, 2010), cdec (Dyer et al, 2010), Jane (Vilar et al, 2010) and SilkRoad, and offer good references for the development of the NiuTrans toolkit. $$$$$ A semiring is a 5-tuple (K, ®, ®, 0,1) that indicates the set from which the values will be drawn, K, a generic addition and multiplication operation, ® and ®, and their identities 0 and 1.
