 $$$$$ While prior feature-based dynamic programming parsers have restricted training and evaluation to artificially short sentences, we present the first general, featurerich discriminative parser, based on a conditional random field model, which has been successfully scaled to the full WSJ parsing data.
 $$$$$ One early experiment on WSJ15 showed a seven time speed up.
 $$$$$ We found that an initial gain of η0 = 0.1 and batch size between 15 and 30 was optimal for this application.

Instead, we build a log-linear CRF-based dependency parser, which is similar to the CRF based constituent parser of Finkel et al (2008). $$$$$ Stochastic optimization methods have proven to be extremely efficient for the training of models involving computationally expensive objective functions like those encountered with our task (Vishwanathan et al., 2006) and, in fact, the on-line backpropagation learning used in the neural network parser of Henderson (2004) is a form of stochastic gradient descent.
Instead, we build a log-linear CRF-based dependency parser, which is similar to the CRF based constituent parser of Finkel et al (2008). $$$$$ Starting with this grammar we found it difficult to achieve gains as well.
Instead, we build a log-linear CRF-based dependency parser, which is similar to the CRF based constituent parser of Finkel et al (2008). $$$$$ The full results for WSJ15 are shown in Table 3 and for WSJ40 are shown in Table 4.

Finkel et al (2008) show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS. $$$$$ We viewed this as equivalent to the more elaborate, smoothed unknown word models that are common in many PCFG parsers, such as (Klein and Manning, 2003; Petrov et al., 2006).
Finkel et al (2008) show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS. $$$$$ Many thanks to Teg Grenager and Paul Heymann for their advice (and their general awesomeness), and to our anonymous reviewers for helpful comments.
Finkel et al (2008) show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS. $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06R-0034, and by a Scottish Enterprise EdinburghStanford Link grant (R37588), as part of the EASIE project.

Therefore, we propose a simple corpus-weighting strategy, as shown in Algorithm 1, where Db i, k is the subset of training data used in kth update and b is the batch size; k is the update step, which is adjusted following the simulated annealing procedure (Finkel et al, 2008). $$$$$ For both WSJ15 and WSJ40, we trained a generative model; a discriminative model, which used lexicon features, but no grammar features other than the rules themselves; and a feature-based model which had access to all features.
Therefore, we propose a simple corpus-weighting strategy, as shown in Algorithm 1, where Db i, k is the subset of training data used in kth update and b is the batch size; k is the update step, which is adjusted following the simulated annealing procedure (Finkel et al, 2008). $$$$$ We used development data to decide when the models had converged.
Therefore, we propose a simple corpus-weighting strategy, as shown in Algorithm 1, where Db i, k is the subset of training data used in kth update and b is the batch size; k is the update step, which is adjusted following the simulated annealing procedure (Finkel et al, 2008). $$$$$ The process is analogous to the computation of partial derivatives in linear chain CRFs.
Therefore, we propose a simple corpus-weighting strategy, as shown in Algorithm 1, where Db i, k is the subset of training data used in kth update and b is the batch size; k is the update step, which is adjusted following the simulated annealing procedure (Finkel et al, 2008). $$$$$ We used a relatively simple grammar with few additional annotations.

 $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06R-0034, and by a Scottish Enterprise EdinburghStanford Link grant (R37588), as part of the EASIE project.
 $$$$$ On sentences of length our system achieves an F-score of a 36% relative reduction in error over a generative baseline.
 $$$$$ They define a simple linear model, use boosted decision trees to select feature conjunctions, and a line search to optimize the parameters.
 $$$$$ Secondly, we compute a class for each word based on the unknown word model of Klein and Manning (2003); this model takes into account capitalization, digits, dashes, and other character-level features.

We found that the SGD parameters described by Finkel et al (2008) worked equally well for our models, and, on the baseline, produced similar results to L-BFGS. $$$$$ Both models had access to the lexicon features.
We found that the SGD parameters described by Finkel et al (2008) worked equally well for our models, and, on the baseline, produced similar results to L-BFGS. $$$$$ Both models had access to the lexicon features.

Typically, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al, 2008). $$$$$ SGD was implemented using the standard update: ek+1 = ek − gk0L (D(k) And employed a gain schedule in the form where parameter τ was adjusted such that the gain is halved after five passes through the data.
Typically, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al, 2008). $$$$$ For a rule application, we allow arbitrary features to be defined over the rule categories, span and split point indices, and the words of the sentence.
Typically, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al, 2008). $$$$$ Standard deterministic optimization routines such as L-BFGS (Liu and Nocedal, 1989) make little progress in the initial iterations, often requiring several passes through the data in order to satisfy sufficient descent conditions placed on line searches.
Typically, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al, 2008). $$$$$ On sentences of length our system achieves an F-score of a 36% relative reduction in error over a generative baseline.

We directly compare asynchronous algorithms with multiprocessor synchronous mini-batch algorithms (e.g., Finkel et al, 2008) and traditional batch algorithms. $$$$$ Binarization of rules (Earley, 1970) is necessary to obtain cubic parsing time, and closure of unary chains is required for finding total probability mass (rather than just best parses) (Stolcke, 1995).
We directly compare asynchronous algorithms with multiprocessor synchronous mini-batch algorithms (e.g., Finkel et al, 2008) and traditional batch algorithms. $$$$$ They define a simple linear model, use boosted decision trees to select feature conjunctions, and a line search to optimize the parameters.
We directly compare asynchronous algorithms with multiprocessor synchronous mini-batch algorithms (e.g., Finkel et al, 2008) and traditional batch algorithms. $$$$$ This speed-up does not come with a performance cost; we attain an F-score of 90.9%, a 14% relative reduction in errors over previous work on WSJ15.
We directly compare asynchronous algorithms with multiprocessor synchronous mini-batch algorithms (e.g., Finkel et al, 2008) and traditional batch algorithms. $$$$$ This function, Lˆ is designed to approximate the true function L based off a small subset of the training data represented by Db.

Finkel et al (2008) used this approach to speed up training of a log-linear model for parsing. $$$$$ As discussed in Section 5 we performed experiments on both sentences of length < 15 and length < 40.
Finkel et al (2008) used this approach to speed up training of a log-linear model for parsing. $$$$$ Other recent work on discriminative parsing has neglected the use of features, despite their being one of the main advantages of discriminative training methods.
Finkel et al (2008) used this approach to speed up training of a log-linear model for parsing. $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06R-0034, and by a Scottish Enterprise EdinburghStanford Link grant (R37588), as part of the EASIE project.

As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al, 2008). $$$$$ This has the unfortunate effect that our features are optimized for shorter sentences and less training data, but we found development on the longer sentences to be infeasible.
As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al, 2008). $$$$$ As discussed in Section 5 we performed experiments on both sentences of length < 15 and length < 40.
As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al, 2008). $$$$$ In our case however this can make a big difference.We are not just looking up a score for the rule, but must compute all the features, and dot product them with the feature weights, which is far more time consuming.
As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al, 2008). $$$$$ One early experiment on WSJ15 showed a seven time speed up.

Finkel et al (2008) incorporated rich local features into a tree CRF model and built a competitive parser. $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06R-0034, and by a Scottish Enterprise EdinburghStanford Link grant (R37588), as part of the EASIE project.
Finkel et al (2008) incorporated rich local features into a tree CRF model and built a competitive parser. $$$$$ The full results for WSJ15 are shown in Table 3 and for WSJ40 are shown in Table 4.
Finkel et al (2008) incorporated rich local features into a tree CRF model and built a competitive parser. $$$$$ This paper extends the third thread of work, where joint inference via dynamic programming algorithms is used to train models and to attempt to find the globally best parse.
Finkel et al (2008) incorporated rich local features into a tree CRF model and built a competitive parser. $$$$$ Here b, the batch size, means that Db is created by drawing b training examples, with replacement, from the training set D. With this notation we can express the stochastic evaluation of the function as Lˆ (Db;e).

Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ Looking at how other tasks, such as named entity recognition and part-of-speech tagging, have evolved over time, it is clear that greater gains are to be gotten from developing better features than from better models.
Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ Zs is subtracted from this value to get the normalized log probability of that rule in that position.
Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ This speed-up does not come with a performance cost; we attain an F-score of 90.9%, a 14% relative reduction in errors over previous work on WSJ15.
Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ We found no difference in parser performance between using stochastic gradient descent (SGD), and the more common, but significantly slower, L-BFGS.

 $$$$$ Secondly, we compute a class for each word based on the unknown word model of Klein and Manning (2003); this model takes into account capitalization, digits, dashes, and other character-level features.
 $$$$$ The complexity of the algorithm for a particular sentence is O(n3), where n is the length of the sentence.
 $$$$$ Another benefit of our feature based model is that it effortlessly allows smoothing over previously unseen rules.
 $$$$$ As discussed in Section 5 we performed experiments on both sentences of length < 15 and length < 40.

The model can be used for tasks like syntactic parsing (Finkel et al, 2008) and semantic role labeling (Cohn and Blunsom, 2005). $$$$$ While prior feature-based dynamic programming parsers have restricted training and evaluation to artificially short sentences, we present the first general, featurerich discriminative parser, based on a conditional random field model, which has been successfully scaled to the full WSJ parsing data.
The model can be used for tasks like syntactic parsing (Finkel et al, 2008) and semantic role labeling (Cohn and Blunsom, 2005). $$$$$ Discriminative feature-based methods are widely used in natural language processing, but sentence parsing is still dominated by generative methods.
The model can be used for tasks like syntactic parsing (Finkel et al, 2008) and semantic role labeling (Cohn and Blunsom, 2005). $$$$$ It has been shown on other NLP tasks that modeling improvements, such as the switch from generative training to discriminative training, usually provide much smaller performance gains than the gains possible from good feature engineering.
The model can be used for tasks like syntactic parsing (Finkel et al, 2008) and semantic role labeling (Cohn and Blunsom, 2005). $$$$$ Looking at how other tasks, such as named entity recognition and part-of-speech tagging, have evolved over time, it is clear that greater gains are to be gotten from developing better features than from better models.

In an important contrast, Koo et al (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using cluster based word-senses as intermediate abstractions in addition to POS tags (also see Finkel et al (2008)). $$$$$ On sentences of length our system achieves an F-score of a 36% relative reduction in error over a generative baseline.
In an important contrast, Koo et al (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using cluster based word-senses as intermediate abstractions in addition to POS tags (also see Finkel et al (2008)). $$$$$ This stochastic function must be designed to ensure that: Note that this property is satisfied, without scaling, for objective functions that sum over the training data, as it is in our case, but any priors must be scaled down by a factor of b/|D|.
In an important contrast, Koo et al (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using cluster based word-senses as intermediate abstractions in addition to POS tags (also see Finkel et al (2008)). $$$$$ While prior feature-based dynamic programming parsers have restricted training and evaluation to artificially short sentences, we present the first general, featurerich discriminative parser, based on a conditional random field model, which has been successfully scaled to the full WSJ parsing data.
In an important contrast, Koo et al (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using cluster based word-senses as intermediate abstractions in addition to POS tags (also see Finkel et al (2008)). $$$$$ They also do discriminative parsing of length 40 sentences, but with a substantially different setup.

Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008). $$$$$ This is further supported by Johnson (2001), who saw no parsing gains when switching from generative to discriminative training, and by Petrov et al. (2007) who saw only small gains of around 0.7% for their final model when switching training methods.
Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008). $$$$$ Other recent work on discriminative parsing has neglected the use of features, despite their being one of the main advantages of discriminative training methods.
Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008). $$$$$ The stochastic gradient, �L (D(i) b ;e), is then simply the derivative of the stochastic function value.
Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008). $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06R-0034, and by a Scottish Enterprise EdinburghStanford Link grant (R37588), as part of the EASIE project.

Recently, CRF using tree structures were used in (Finkel et al, 2008) in the case of parsing. $$$$$ Many thanks to Teg Grenager and Paul Heymann for their advice (and their general awesomeness), and to our anonymous reviewers for helpful comments.
Recently, CRF using tree structures were used in (Finkel et al, 2008) in the case of parsing. $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06R-0034, and by a Scottish Enterprise EdinburghStanford Link grant (R37588), as part of the EASIE project.
Recently, CRF using tree structures were used in (Finkel et al, 2008) in the case of parsing. $$$$$ Standard deterministic optimization routines such as L-BFGS (Liu and Nocedal, 1989) make little progress in the initial iterations, often requiring several passes through the data in order to satisfy sufficient descent conditions placed on line searches.
Recently, CRF using tree structures were used in (Finkel et al, 2008) in the case of parsing. $$$$$ Taskar et al. (2004) took a large margin approach to discriminative learning, but achieved only small gains.

Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al2011) in Penn Discourse Treebank (Prasad et al2008). $$$$$ It is difficult for a standard PCFG to learn this aspect of the English language, because the score it assigns to a rule does not take its span into account.
Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al2011) in Penn Discourse Treebank (Prasad et al2008). $$$$$ This stochastic function must be designed to ensure that: Note that this property is satisfied, without scaling, for objective functions that sum over the training data, as it is in our case, but any priors must be scaled down by a factor of b/|D|.
Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al2011) in Penn Discourse Treebank (Prasad et al2008). $$$$$ We have presented a new, feature-rich, dynamic programming based discriminative parser which is simpler, more effective, and faster to train and test than previous work, giving us new state-of-the-art performance when training and testing on sentences of length < 15 and the first results for such a parser trained and tested on sentences of length < 40.
Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al2011) in Penn Discourse Treebank (Prasad et al2008). $$$$$ As discussed in Section 5 we performed experiments on both sentences of length < 15 and length < 40.

One sees this clear trend in the supervised NLP literature examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al, 2005), exponentiated gradient algorithms (Collins et al, 2008), stochastic gradient for constituency parsing (Finkel et al, 2008). $$$$$ Utilization of stochastic optimization routines requires the implementation of a stochastic objective function.
One sees this clear trend in the supervised NLP literature examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al, 2005), exponentiated gradient algorithms (Collins et al, 2008), stochastic gradient for constituency parsing (Finkel et al, 2008). $$$$$ For all experiments, we trained and tested on the Penn treebank (PTB) (Marcus et al., 1993).
One sees this clear trend in the supervised NLP literature examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al, 2005), exponentiated gradient algorithms (Collins et al, 2008), stochastic gradient for constituency parsing (Finkel et al, 2008). $$$$$ In Figure 3 we show for an example from section 22 the parse trees produced by our generative model and our feature-based discriminative model, and the correct parse.
One sees this clear trend in the supervised NLP literature examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al, 2005), exponentiated gradient algorithms (Collins et al, 2008), stochastic gradient for constituency parsing (Finkel et al, 2008). $$$$$ Using the probabilities of each rule application, over each span/split, we can compute the expected feature values (the second term in Equation 4), by multiplying this probability by the value of the feature corresponding to the weight for which we are computing the partial derivative.

Once we have converted our sentences into parse trees, we train a discriminative constituency parser similar to that of (Finkelet al, 2008). $$$$$ We have provided just such a framework for improving parsing performance.
Once we have converted our sentences into parse trees, we train a discriminative constituency parser similar to that of (Finkelet al, 2008). $$$$$ This was done by collapsing all allowed unary chains to single unary rules, and disallowing multiple unary rule applications over the same span.1 We give the details of our binarization scheme in Section 5.
Once we have converted our sentences into parse trees, we train a discriminative constituency parser similar to that of (Finkelet al, 2008). $$$$$ We found no difference in parser performance between using stochastic gradient descent (SGD), and the more common, but significantly slower, L-BFGS.
Once we have converted our sentences into parse trees, we train a discriminative constituency parser similar to that of (Finkelet al, 2008). $$$$$ On WSJ15, we attain a state-of-the-artF-score a 14% relative reduction in error over previous models, while being two orders of magnitude faster.
