 $$$$$ We used stochastic gradient descent for if some child is a verb tag, then rule, with that child replaced by the word Unaries which span one word: (r,w) (r,ds(w)) (b(p(r)),w) (b(p(r)),ds(w)) these experiments; the length 15 models had a batch size of 15 and we allowed twenty passes through the data.3 The length 40 models had a batch size of 30 and we allowed ten passes through the data.
 $$$$$ Another benefit of our feature based model is that it effortlessly allows smoothing over previously unseen rules.

Instead, we build a log-linear CRF-based dependency parser, which is similar to the CRF based constituent parser of Finkel et al (2008). $$$$$ We found no difference in parser performance between using stochastic gradient descent (SGD), and the more common, but significantly slower, L-BFGS.
Instead, we build a log-linear CRF-based dependency parser, which is similar to the CRF based constituent parser of Finkel et al (2008). $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06R-0034, and by a Scottish Enterprise EdinburghStanford Link grant (R37588), as part of the EASIE project.
Instead, we build a log-linear CRF-based dependency parser, which is similar to the CRF based constituent parser of Finkel et al (2008). $$$$$ Many thanks to Teg Grenager and Paul Heymann for their advice (and their general awesomeness), and to our anonymous reviewers for helpful comments.
Instead, we build a log-linear CRF-based dependency parser, which is similar to the CRF based constituent parser of Finkel et al (2008). $$$$$ Binarization of rules (Earley, 1970) is necessary to obtain cubic parsing time, and closure of unary chains is required for finding total probability mass (rather than just best parses) (Stolcke, 1995).

Finkel et al (2008) show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS. $$$$$ Following up on their previous work (Petrov et al., 2006) on grammar splitting, they do discriminative parsing with latent variables, which requires them to optimize a non-convex function.
Finkel et al (2008) show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS. $$$$$ On sentences of length our system achieves an F-score of a 36% relative reduction in error over a generative baseline.
Finkel et al (2008) show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS. $$$$$ They define a simple linear model, use boosted decision trees to select feature conjunctions, and a line search to optimize the parameters.
Finkel et al (2008) show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS. $$$$$ Other recent work on discriminative parsing has neglected the use of features, despite their being one of the main advantages of discriminative training methods.

Therefore, we propose a simple corpus-weighting strategy, as shown in Algorithm 1, where Db i, k is the subset of training data used in kth update and b is the batch size; k is the update step, which is adjusted following the simulated annealing procedure (Finkel et al, 2008). $$$$$ To properly situate this work with respect to both sets of literature we trained models on both length < 15 (WSJ15) and length < 40 (WSJ40), and we also tested on all sentences using the WSJ40 models.
Therefore, we propose a simple corpus-weighting strategy, as shown in Algorithm 1, where Db i, k is the subset of training data used in kth update and b is the batch size; k is the update step, which is adjusted following the simulated annealing procedure (Finkel et al, 2008). $$$$$ Many thanks to Teg Grenager and Paul Heymann for their advice (and their general awesomeness), and to our anonymous reviewers for helpful comments.
Therefore, we propose a simple corpus-weighting strategy, as shown in Algorithm 1, where Db i, k is the subset of training data used in kth update and b is the batch size; k is the update step, which is adjusted following the simulated annealing procedure (Finkel et al, 2008). $$$$$ The training set is very small, and it is a known fact that generative models tend to work better for small datasets and discriminative models tend to work better for larger datasets (Ng and Jordan, 2002).
Therefore, we propose a simple corpus-weighting strategy, as shown in Algorithm 1, where Db i, k is the subset of training data used in kth update and b is the batch size; k is the update step, which is adjusted following the simulated annealing procedure (Finkel et al, 2008). $$$$$ It is difficult for a standard PCFG to learn this aspect of the English language, because the score it assigns to a rule does not take its span into account.

 $$$$$ Our efficiency is primarily due to the use of stochastic optimization techniques, as well as parallelization and chart prefiltering.
 $$$$$ Practicality comes from three sources.
 $$$$$ To compute the partial derivatives, we walk through each rule, and span/split, and add the outside log-score of the parent, the inside log-score(s) of the child(ren), and the log-score for that rule and span/split.

We found that the SGD parameters described by Finkel et al (2008) worked equally well for our models, and, on the baseline, produced similar results to L-BFGS. $$$$$ On WSJ15, we attain a state-of-the-artF-score a 14% relative reduction in error over previous models, while being two orders of magnitude faster.
We found that the SGD parameters described by Finkel et al (2008) worked equally well for our models, and, on the baseline, produced similar results to L-BFGS. $$$$$ However their model, like the discriminative parser of Johnson (2001), makes no use of features, and effectively ignores the largest advantage of discriminative training.
We found that the SGD parameters described by Finkel et al (2008) worked equally well for our models, and, on the baseline, produced similar results to L-BFGS. $$$$$ As discussed in Section 5 we performed experiments on both sentences of length < 15 and length < 40.
We found that the SGD parameters described by Finkel et al (2008) worked equally well for our models, and, on the baseline, produced similar results to L-BFGS. $$$$$ There are two probable reasons for this result.

Typically, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al, 2008). $$$$$ Our features are divided into two types: lexicon features, which are over words and tags, and grammar features which are over the local subtrees and corresponding span/split (both have access to the entire sentence).
Typically, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al, 2008). $$$$$ One early experiment on WSJ15 showed a seven time speed up.
Typically, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al, 2008). $$$$$ Other recent work on discriminative parsing has neglected the use of features, despite their being one of the main advantages of discriminative training methods.
Typically, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al, 2008). $$$$$ Work in this context has mainly been limited to use of artificially short sentences due to exorbitant training and inference times.

We directly compare asynchronous algorithms with multiprocessor synchronous mini-batch algorithms (e.g., Finkel et al, 2008) and traditional batch algorithms. $$$$$ We also show that the use of SGD for training CRFs performs as well as L-BFGS in a fraction of the time.
We directly compare asynchronous algorithms with multiprocessor synchronous mini-batch algorithms (e.g., Finkel et al, 2008) and traditional batch algorithms. $$$$$ We ran two kinds of experiments: a discriminatively trained model, which used only the rules and no other grammar features, and a featurebased model which did make use of grammar features.
We directly compare asynchronous algorithms with multiprocessor synchronous mini-batch algorithms (e.g., Finkel et al, 2008) and traditional batch algorithms. $$$$$ The discriminatively trained generative WSJ40 model (discriminative in Table 4) was trained using two of the same machines, with 16 gigabytes of RAM each for the clients.4 It took about one day per pass through the data.

Finkel et al (2008) used this approach to speed up training of a log-linear model for parsing. $$$$$ This is equivalent to children of a state being produced by a second order Markov process.
Finkel et al (2008) used this approach to speed up training of a log-linear model for parsing. $$$$$ Other recent work on discriminative parsing has neglected the use of features, despite their being one of the main advantages of discriminative training methods.
Finkel et al (2008) used this approach to speed up training of a log-linear model for parsing. $$$$$ On sentences of length our system achieves an F-score of a 36% relative reduction in error over a generative baseline.
Finkel et al (2008) used this approach to speed up training of a log-linear model for parsing. $$$$$ We also show that the use of SGD for training CRFs performs as well as L-BFGS in a fraction of the time.

As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al, 2008). $$$$$ It has been shown on other NLP tasks that modeling improvements, such as the switch from generative training to discriminative training, usually provide much smaller performance gains than the gains possible from good feature engineering.
As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al, 2008). $$$$$ On sentences of length our system achieves an F-score of a 36% relative reduction in error over a generative baseline.
As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al, 2008). $$$$$ The stochastic gradient, �L (D(i) b ;e), is then simply the derivative of the stochastic function value.
As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al, 2008). $$$$$ In our experiments SGD converged to a lower objective function value than L-BFGS, however it required far fewer iterations (see Figure 2) and achieved comparable test set performance to L-BFGS in a fraction of the time.

Finkel et al (2008) incorporated rich local features into a tree CRF model and built a competitive parser. $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06R-0034, and by a Scottish Enterprise EdinburghStanford Link grant (R37588), as part of the EASIE project.
Finkel et al (2008) incorporated rich local features into a tree CRF model and built a competitive parser. $$$$$ We found no difference in parser performance between using stochastic gradient descent (SGD), and the more common, but significantly slower, L-BFGS.
Finkel et al (2008) incorporated rich local features into a tree CRF model and built a competitive parser. $$$$$ We used development data to decide when the models had converged.
Finkel et al (2008) incorporated rich local features into a tree CRF model and built a competitive parser. $$$$$ Because we use a stochastic optimization method, as discussed in Section 3, we compute the objective for only a small portion of the training data at a time, typically between 15 and 30 sentences.

Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ Firstly, each word is annotated with a distributional similarity tag, from a distributional similarity model (Clark, 2000) trained on 100 million words from the British National Corpus and English Gigaword corpus.
Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ More recent is the work of (Turian and Melamed, 2006; Turian et al., 2007), which improved both the training time and accuracy of (Taskar et al., 2004).
Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ Many thanks to Teg Grenager and Paul Heymann for their advice (and their general awesomeness), and to our anonymous reviewers for helpful comments.
Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06R-0034, and by a Scottish Enterprise EdinburghStanford Link grant (R37588), as part of the EASIE project.

 $$$$$ We have a feature function, represented by f(r,s), which returns a vector with the value for each feature.
 $$$$$ It is difficult for a standard PCFG to learn this aspect of the English language, because the score it assigns to a rule does not take its span into account.
 $$$$$ We have presented a new, feature-rich, dynamic programming based discriminative parser which is simpler, more effective, and faster to train and test than previous work, giving us new state-of-the-art performance when training and testing on sentences of length < 15 and the first results for such a parser trained and tested on sentences of length < 40.
 $$$$$ The feature-based model with the relaxed grammar (relaxed in Table 3) took about four times as long as the regular feature-based model.

The model can be used for tasks like syntactic parsing (Finkel et al, 2008) and semantic role labeling (Cohn and Blunsom, 2005). $$$$$ This is further supported by Johnson (2001), who saw no parsing gains when switching from generative to discriminative training, and by Petrov et al. (2007) who saw only small gains of around 0.7% for their final model when switching training methods.
The model can be used for tasks like syntactic parsing (Finkel et al, 2008) and semantic role labeling (Cohn and Blunsom, 2005). $$$$$ This is similar to conventional probabilistic context-free grammar (PCFG) parsing, with two exceptions: (a) we maximize conditional likelihood of the parse tree, given the sentence, not joint likelihood of the tree and sentence; and (b) probabilities are normalized globally instead of locally – the graphical models depiction of our trees is undirected.
The model can be used for tasks like syntactic parsing (Finkel et al, 2008) and semantic role labeling (Cohn and Blunsom, 2005). $$$$$ The stochastic gradient, �L (D(i) b ;e), is then simply the derivative of the stochastic function value.
The model can be used for tasks like syntactic parsing (Finkel et al, 2008) and semantic role labeling (Cohn and Blunsom, 2005). $$$$$ One early experiment on WSJ15 showed a seven time speed up.

In an important contrast, Koo et al (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using cluster based word-senses as intermediate abstractions in addition to POS tags (also see Finkel et al (2008)). $$$$$ Utilization of stochastic optimization routines requires the implementation of a stochastic objective function.
In an important contrast, Koo et al (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using cluster based word-senses as intermediate abstractions in addition to POS tags (also see Finkel et al (2008)). $$$$$ They also do discriminative parsing of length 40 sentences, but with a substantially different setup.
In an important contrast, Koo et al (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using cluster based word-senses as intermediate abstractions in addition to POS tags (also see Finkel et al (2008)). $$$$$ The results clearly show that gains came from both the switch from generative to discriminative training, and from the extensive use of features.
In an important contrast, Koo et al (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using cluster based word-senses as intermediate abstractions in addition to POS tags (also see Finkel et al (2008)). $$$$$ This is further supported by Johnson (2001), who saw no parsing gains when switching from generative to discriminative training, and by Petrov et al. (2007) who saw only small gains of around 0.7% for their final model when switching training methods.

Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008). $$$$$ While they make extensive use of features, their setup is much more complex than ours and takes substantially longer to train – up to 5 days on WSJ15 – while achieving only small gains over (Taskar et al., 2004).
Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008). $$$$$ Because the computation of both the log likelihood and the partial derivatives involves summing over each tree individually, the computation can be parallelized by having many clients which each do the computation for one tree, and one central server which aggregates the information to compute the relevant information for a set of trees.
Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008). $$$$$ It has been shown on other NLP tasks that modeling improvements, such as the switch from generative training to discriminative training, usually provide much smaller performance gains than the gains possible from good feature engineering.
Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008). $$$$$ The stochastic gradient, �L (D(i) b ;e), is then simply the derivative of the stochastic function value.

Recently, CRF using tree structures were used in (Finkel et al, 2008) in the case of parsing. $$$$$ The discriminatively trained generative WSJ40 model (discriminative in Table 4) was trained using two of the same machines, with 16 gigabytes of RAM each for the clients.4 It took about one day per pass through the data.
Recently, CRF using tree structures were used in (Finkel et al, 2008) in the case of parsing. $$$$$ Other recent work on discriminative parsing has neglected the use of features, despite their being one of the main advantages of discriminative training methods.
Recently, CRF using tree structures were used in (Finkel et al, 2008) in the case of parsing. $$$$$ While prior feature-based dynamic programming parsers have restricted training and evaluation to artificially short sentences, we present the first general, featurerich discriminative parser, based on a conditional random field model, which has been successfully scaled to the full WSJ parsing data.
Recently, CRF using tree structures were used in (Finkel et al, 2008) in the case of parsing. $$$$$ Many thanks to Teg Grenager and Paul Heymann for their advice (and their general awesomeness), and to our anonymous reviewers for helpful comments.

Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al2011) in Penn Discourse Treebank (Prasad et al2008). $$$$$ Because they were focusing on grammar splitting they, like (Johnson, 2001), did not employ any features, and, like (Taskar et al., 2004), they saw only small gains from switching from generative to discriminative training.
Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al2011) in Penn Discourse Treebank (Prasad et al2008). $$$$$ Our features are divided into two types: lexicon features, which are over words and tags, and grammar features which are over the local subtrees and corresponding span/split (both have access to the entire sentence).
Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al2011) in Penn Discourse Treebank (Prasad et al2008). $$$$$ Additionally, they made no use of features, one of the primary benefits of discriminative learning.
Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al2011) in Penn Discourse Treebank (Prasad et al2008). $$$$$ The feature-based model with the relaxed grammar (relaxed in Table 3) took about four times as long as the regular feature-based model.

One sees this clear trend in the supervised NLP literature examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al, 2005), exponentiated gradient algorithms (Collins et al, 2008), stochastic gradient for constituency parsing (Finkel et al, 2008). $$$$$ We preprocessed the words in the sentences to obtain two extra pieces of information.
One sees this clear trend in the supervised NLP literature examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al, 2005), exponentiated gradient algorithms (Collins et al, 2008), stochastic gradient for constituency parsing (Finkel et al, 2008). $$$$$ This is further supported by Johnson (2001), who saw no parsing gains when switching from generative to discriminative training, and by Petrov et al. (2007) who saw only small gains of around 0.7% for their final model when switching training methods.
One sees this clear trend in the supervised NLP literature examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al, 2005), exponentiated gradient algorithms (Collins et al, 2008), stochastic gradient for constituency parsing (Finkel et al, 2008). $$$$$ Other recent work on discriminative parsing has neglected the use of features, despite their being one of the main advantages of discriminative training methods.

Once we have converted our sentences into parse trees, we train a discriminative constituency parser similar to that of (Finkelet al, 2008). $$$$$ We then define a conditional probability distribution over entire trees, using the standard CRF distribution, shown in (1).
Once we have converted our sentences into parse trees, we train a discriminative constituency parser similar to that of (Finkelet al, 2008). $$$$$ Standard deterministic optimization routines such as L-BFGS (Liu and Nocedal, 1989) make little progress in the initial iterations, often requiring several passes through the data in order to satisfy sufficient descent conditions placed on line searches.
Once we have converted our sentences into parse trees, we train a discriminative constituency parser similar to that of (Finkelet al, 2008). $$$$$ We also show that the use of SGD for training CRFs performs as well as L-BFGS in a fraction of the time.
Once we have converted our sentences into parse trees, we train a discriminative constituency parser similar to that of (Finkelet al, 2008). $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06R-0034, and by a Scottish Enterprise EdinburghStanford Link grant (R37588), as part of the EASIE project.
