In Moore and Lewis (2010), the authors compare several approaches to selecting data for LM and Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ For this work we follow the procedure of Moore and Lewis (2010), which applies the cosmetic change of using the cross-entropy rather than perplexity.
In Moore and Lewis (2010), the authors compare several approaches to selecting data for LM and Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ It is tempting to describe these as methods for finding in-domain data hidden in a general-domain corpus.
In Moore and Lewis (2010), the authors compare several approaches to selecting data for LM and Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ Our in-domain data consisted of the IWSLT corpus of approximately 30,000 sentences in Chinese and English.

Axelrod et al (2011) improved the perplexity based approach and proposed bilingual cross entropy difference as a ranking function with in- and general-domain language models. $$$$$ We used the three methods from Section 4 to identify the best-scoring sentences in the generaldomain corpus.
Axelrod et al (2011) improved the perplexity based approach and proposed bilingual cross entropy difference as a ranking function with in- and general-domain language models. $$$$$ Table 4 shows baseline results for the in-domain translation system and the general-domain system, evaluated on the in-domain data.

 $$$$$ As further evidence, consider the results of concatenating the in-domain corpus with the best extracted subcorpora (using the bilingual MooreLewis method), shown in Table 3.
 $$$$$ The resulting model had a phrase table with 515k entries.
 $$$$$ We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain.
 $$$$$ However, Section 6.3 shows that using two translation models over all the available data (one in-domain, one general-domain) outperforms any single individual translation model so far, albeit only slightly.

This would empirically provide more accurate lexical probabilities, and thus better match the translation task at hand (Axelrod et al., 2011). $$$$$ These sentences may be selected with simple cross-entropy based methods, of which we present three.
This would empirically provide more accurate lexical probabilities, and thus better match the translation task at hand (Axelrod et al., 2011). $$$$$ Both the in- and generaldomain corpora were identically segmented (in Chinese) and tokenized (in English), but otherwise unprocessed.
This would empirically provide more accurate lexical probabilities, and thus better match the translation task at hand (Axelrod et al., 2011). $$$$$ Statistical Machine Translation (SMT) system performance is dependent on the quantity and quality of available training data.
This would empirically provide more accurate lexical probabilities, and thus better match the translation task at hand (Axelrod et al., 2011). $$$$$ Nonetheless, we have shown that relatively tiny amounts of this pseudo in-domain data can prove more useful than the entire general-domain corpus for the purposes of domain-targeted translation tasks.

Axelrod et al (2011) proposed a bilingual cross-entropy difference to select data from parallel corpus for domain adaptation which captures the contextual information slightly, and outperformed monolingual cross-entropy difference (Moore and Lewis, 2010), which first shows the advantage of bilingual data selection. $$$$$ The other monolingual method, source-side cross-entropy difference, was able to perform nearly as well as the generaldomain model with only 35k sentences.
Axelrod et al (2011) proposed a bilingual cross-entropy difference to select data from parallel corpus for domain adaptation which captures the contextual information slightly, and outperformed monolingual cross-entropy difference (Moore and Lewis, 2010), which first shows the advantage of bilingual data selection. $$$$$ An underlying assumption in domain adaptation is that a general-domain corpus, if sufficiently broad, likely includes some sentences that could fall within the target domain and thus should be used for training.
Axelrod et al (2011) proposed a bilingual cross-entropy difference to select data from parallel corpus for domain adaptation which captures the contextual information slightly, and outperformed monolingual cross-entropy difference (Moore and Lewis, 2010), which first shows the advantage of bilingual data selection. $$$$$ However, the exact overlap between the phrase tables was tiny, minimizing this effect.
Axelrod et al (2011) proposed a bilingual cross-entropy difference to select data from parallel corpus for domain adaptation which captures the contextual information slightly, and outperformed monolingual cross-entropy difference (Moore and Lewis, 2010), which first shows the advantage of bilingual data selection. $$$$$ However, using multiple decoding paths, and no explicit model merging at all, produced even better results, by 2 BLEU points over the best individual model and 1.3 BLEU over the best interpolated model, which used A = 0.9.

We experimented with two different types of sub sampling techniques - Model 1, similar to that used by Schwenk et al (2011), and modified Moore Lewis (Axelrod et al, 2011) - for the language pairs es-en, en-es, fr-en and en-fr. $$$$$ Nonetheless, we have shown that relatively tiny amounts of this pseudo in-domain data can prove more useful than the entire general-domain corpus for the purposes of domain-targeted translation tasks.
We experimented with two different types of sub sampling techniques - Model 1, similar to that used by Schwenk et al (2011), and modified Moore Lewis (Axelrod et al, 2011) - for the language pairs es-en, en-es, fr-en and en-fr. $$$$$ This would empirically provide more accurate lexical probabilities, and thus better target the task at hand.
We experimented with two different types of sub sampling techniques - Model 1, similar to that used by Schwenk et al (2011), and modified Moore Lewis (Axelrod et al, 2011) - for the language pairs es-en, en-es, fr-en and en-fr. $$$$$ It is well and good to use the in-domain data to select pseudo in-domain data from the generaldomain corpus, but given that this requires access to an in-domain corpus, one might as well use it.
We experimented with two different types of sub sampling techniques - Model 1, similar to that used by Schwenk et al (2011), and modified Moore Lewis (Axelrod et al, 2011) - for the language pairs es-en, en-es, fr-en and en-fr. $$$$$ The task of domain adaptation is to translate a text in a particular (target) domain for which only a small amount of training data is available, using an MT system trained on a larger set of data that is not restricted to the target domain.

 $$$$$ We furthermore extend this idea for MT-specific purposes.

 $$$$$ We both keep the models separate and reduce their size.
 $$$$$ We apply the method to machine translation, even though perplexity reduction has been shown to not correlate with translation performance (Axelrod, 2006).
