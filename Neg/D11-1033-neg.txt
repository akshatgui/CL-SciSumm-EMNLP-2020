In Moore and Lewis (2010), the authors compare several approaches to selecting data for LM and Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ These sentences may be selected with simple cross-entropy based methods, of which we present three.
In Moore and Lewis (2010), the authors compare several approaches to selecting data for LM and Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ The general-domain baseline was substantially larger, having been trained on 12 million sentence pairs, and had a phrase table containing 1.5 billion entries.
In Moore and Lewis (2010), the authors compare several approaches to selecting data for LM and Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ The simplest method, using only the source-side cross-entropy, was able to outperform the general-domain model when selecting 150k out of 12 million sentences.
In Moore and Lewis (2010), the authors compare several approaches to selecting data for LM and Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ They then rank the general-domain corpus sentences using: and again taking the lowest-scoring sentences.

Axelrod et al (2011) improved the perplexity based approach and proposed bilingual cross entropy difference as a ranking function with in- and general-domain language models. $$$$$ Our general-domain corpus was 12 million parallel sentences comprising a variety of publicly available datasets, web data, and private translation texts.
Axelrod et al (2011) improved the perplexity based approach and proposed bilingual cross entropy difference as a ranking function with in- and general-domain language models. $$$$$ The in-domain baseline consisted of a translation system trained using Moses, as described above, on the IWSLT corpus.
Axelrod et al (2011) improved the perplexity based approach and proposed bilingual cross entropy difference as a ranking function with in- and general-domain language models. $$$$$ The resulting model had a phrase table with 515k entries.

 $$$$$ We used the Moses decoder to produce all the system outputs, and scored them with the NIST mt-eval31a 4 tool used in the IWSLT evalutation.
 $$$$$ The results in Table 2 show that all three methods (Cross-Entropy, Moore-Lewis, bilingual MooreLewis) can extract subsets of the general-domain corpus that are useful for the purposes of statistical machine translation.
 $$$$$ The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining inand general-domain systems during decoding.
 $$$$$ A common approach to managing multiple translation models is to interpolate them, as in (Foster and Kuhn, 2007) and (L¨u et al., 2007).

This would empirically provide more accurate lexical probabilities, and thus better match the translation task at hand (Axelrod et al., 2011). $$$$$ For this reason, one would prefer to use more in-domain data for training.
This would empirically provide more accurate lexical probabilities, and thus better match the translation task at hand (Axelrod et al., 2011). $$$$$ The first two data selection methods are applications of language-modeling techniques to MT (one for the first time).
This would empirically provide more accurate lexical probabilities, and thus better match the translation task at hand (Axelrod et al., 2011). $$$$$ We used the three methods from Section 4 to identify the best-scoring sentences in the generaldomain corpus.

Axelrod et al (2011) proposed a bilingual cross-entropy difference to select data from parallel corpus for domain adaptation which captures the contextual information slightly, and outperformed monolingual cross-entropy difference (Moore and Lewis, 2010), which first shows the advantage of bilingual data selection. $$$$$ In practice, most practical systems also perform target-side language model adaptation (Eck et al., 2004); we eschew this in order to isolate the effects of translation model adaptation alone.
Axelrod et al (2011) proposed a bilingual cross-entropy difference to select data from parallel corpus for domain adaptation which captures the contextual information slightly, and outperformed monolingual cross-entropy difference (Moore and Lewis, 2010), which first shows the advantage of bilingual data selection. $$$$$ Statistical Machine Translation (SMT) system performance is dependent on the quantity and quality of available training data.
Axelrod et al (2011) proposed a bilingual cross-entropy difference to select data from parallel corpus for domain adaptation which captures the contextual information slightly, and outperformed monolingual cross-entropy difference (Moore and Lewis, 2010), which first shows the advantage of bilingual data selection. $$$$$ A translation system trained on a pseudo indomain subset of the general corpus, selected with the bilingual Moore-Lewis method, can be further improved by combining with an in-domain model.
Axelrod et al (2011) proposed a bilingual cross-entropy difference to select data from parallel corpus for domain adaptation which captures the contextual information slightly, and outperformed monolingual cross-entropy difference (Moore and Lewis, 2010), which first shows the advantage of bilingual data selection. $$$$$ This tiny combined system was also 3+ points better than the general-domain system by itself, and 6+ points better than the in-domain system alone.

We experimented with two different types of sub sampling techniques - Model 1, similar to that used by Schwenk et al (2011), and modified Moore Lewis (Axelrod et al, 2011) - for the language pairs es-en, en-es, fr-en and en-fr. $$$$$ The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining inand general-domain systems during decoding.
We experimented with two different types of sub sampling techniques - Model 1, similar to that used by Schwenk et al (2011), and modified Moore Lewis (Axelrod et al, 2011) - for the language pairs es-en, en-es, fr-en and en-fr. $$$$$ In addition to improving the performance of a single general model with respect to a target domain, there is significant interest in using two translation models, one trained on a larger general-domain corpus and the other on a smaller in-domain corpus, to translate in-domain text.
We experimented with two different types of sub sampling techniques - Model 1, similar to that used by Schwenk et al (2011), and modified Moore Lewis (Axelrod et al, 2011) - for the language pairs es-en, en-es, fr-en and en-fr. $$$$$ This criterion biases towards sentences that are both like the in-domain corpus and unlike the average of the general-domain corpus.

 $$$$$ This tiny combined system was also 3+ points better than the general-domain system by itself, and 6+ points better than the in-domain system alone.
 $$$$$ We presented in Section 5 several methods to improve the performance of a single general-domain translation system by restricting its training corpus on an information-theoretic basis to a very small number of sentences.
 $$$$$ And yet, none of these scores are anywhere near the perplexity of 36.96 according to the LM trained only on in-domain data.
 $$$$$ In practice, most practical systems also perform target-side language model adaptation (Eck et al., 2004); we eschew this in order to isolate the effects of translation model adaptation alone.

 $$$$$ We present three techniques for ranking and selecting subsets of a general-domain corpus, with an eye towards improving overall translation performance.
 $$$$$ We constructed 4gram language models with interpolated modified Kneser-Ney discounting (Chen and Goodman, 1998), and set the GoodTuring threshold to 1 for trigrams.
 $$$$$ This tiny combined system was also 3+ points better than the general-domain system by itself, and 6+ points better than the in-domain system alone.
 $$$$$ As these sentences are not themselves identical the in-domain data, we call them These subcorpora – 1% the size of the original – can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus.
