Briscoe and Carroll (1993) observed that half of the parse failures were caused by inaccurate sub categorization information in the lexicon. $$$$$ All errors and mistakes remain our responsibility.
Briscoe and Carroll (1993) observed that half of the parse failures were caused by inaccurate sub categorization information in the lexicon. $$$$$ However, the time a parse takes can often be lengthened by incorrect choices (and the consequent need to back up manually) and by the process of adding lexical entries and occasional rules.
Briscoe and Carroll (1993) observed that half of the parse failures were caused by inaccurate sub categorization information in the lexicon. $$$$$ Section 4 presents the method and results for constructing a LALR(1) parse table for the ANLT grammar and discusses these in the light of both computational complexity and other empirical results concerning parse table size and construction time.

If, instead, this procedure returns a list of several possible actions with corresponding probabilities, we can then parse with a model similar to the probabilistic LR models described by Briscoe and Carroll (1993), where the probability of a parse tree is the product of the probabilities of each of the actions taken in its derivation. $$$$$ Consequently, the parser finds, and ranks highest, an analysis in which supports and helps are treated as transitive verbs forming verb phrases with object NP gaps, and that supports or helps as a zero relative clause with that analyzed as a prenominal subjectâ€”compare a person or thing that that supports or helps.
If, instead, this procedure returns a list of several possible actions with corresponding probabilities, we can then parse with a model similar to the probabilistic LR models described by Briscoe and Carroll (1993), where the probability of a parse tree is the product of the probabilities of each of the actions taken in its derivation. $$$$$ Richard Sharman kindly calculated the perplexity measures for this corpus.
If, instead, this procedure returns a list of several possible actions with corresponding probabilities, we can then parse with a model similar to the probabilistic LR models described by Briscoe and Carroll (1993), where the probability of a parse tree is the product of the probabilities of each of the actions taken in its derivation. $$$$$ These requirements immediately suggest that approaches that recover only lexical tags (e.g. de Rose 1988) or a syntactic analysis that is the 'closest fit' to some previously defined set of possible analyses (e.g.

 $$$$$ These requirements immediately suggest that approaches that recover only lexical tags (e.g. de Rose 1988) or a syntactic analysis that is the 'closest fit' to some previously defined set of possible analyses (e.g.
 $$$$$ This technique is superior to parsers based on probabilistic lexical tagging or probabilistic context-free grammar because it allows for a more context-dependent probabilistic language model, as well as use of a more linguistically adequate grammar formalism.
 $$$$$ This research is supported by SERC/DTI-IED project 4/1/1261 'Extensions to the Alvey Natural Language Tools' and by ESPRIT BRA 3030 'Acquisition of Lexical Information from Machine-Readable Dictionaries.'
 $$$$$ It is not surprising that probabilities based solely on the frequency of syntactic rules are not capable of resolving this type of ambiguity; in an example such as John saw the man on Monday again it is the temporal interpretation of Monday that favors the adverbial interpretation (and thus nonlocal attachment).

work by Briscoe and Carroll (1993) on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags. $$$$$ The two main algorithms utilized are the Viterbi (1967) algorithm and the Baum-Welch algorithm (Baum 1972).
work by Briscoe and Carroll (1993) on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags. $$$$$ However, the technique is limited in several ways; firstly, such grammars are restricted to small (maximum about 15 nonterminal) CNF CFGs because of the computational cost of iterative re-estimation with an algorithm polynomial in sentence length and nonterminal category size; and secondly, because some form of supervised training will be essential if the analyses assigned by the grammar are to be linguistically motivated.
work by Briscoe and Carroll (1993) on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags. $$$$$ However, (unseen) sentences whose correct analysis is outside the coverage of the grammar reri:ain a problem.
work by Briscoe and Carroll (1993) on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags. $$$$$ Magerman and Marcus 1991), but only one global probability can be associated with the relevant CF production.

Finally, we observe that there are also trainable stochastic shift-reduce parser models (Briscoe and Carroll, 1993), which are theoretically related to shift-reduce parsing, but operate in a highly non deterministic fashion during parsing. $$$$$ The most straightforward technique for associating probabilities with the parse table is to assign a probability to each action in the action part of the table (e.g.
Finally, we observe that there are also trainable stochastic shift-reduce parser models (Briscoe and Carroll, 1993), which are theoretically related to shift-reduce parsing, but operate in a highly non deterministic fashion during parsing. $$$$$ This research is supported by SERC/DTI-IED project 4/1/1261 'Extensions to the Alvey Natural Language Tools' and by ESPRIT BRA 3030 'Acquisition of Lexical Information from Machine-Readable Dictionaries.'
Finally, we observe that there are also trainable stochastic shift-reduce parser models (Briscoe and Carroll, 1993), which are theoretically related to shift-reduce parsing, but operate in a highly non deterministic fashion during parsing. $$$$$ Section 8 describes and presents the results of our first experiment parsing LDOCE noun definitions, and Section 9 draws some preliminary conclusions and outlines ways in which the work described should be modified and extended.
Finally, we observe that there are also trainable stochastic shift-reduce parser models (Briscoe and Carroll, 1993), which are theoretically related to shift-reduce parsing, but operate in a highly non deterministic fashion during parsing. $$$$$ We report promising results of a pilot study training on 150 noun definitions from the Longman Dictionary of Contemporary English (LDOCE) and retesting on these plus a further 55 definitions.

Following (Briscoe and Carroll, 1993), conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration. $$$$$ In our current system this type of lookahead is limited to up to four indeterminacies ahead.
Following (Briscoe and Carroll, 1993), conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration. $$$$$ The slightly worse results (relative to mean definition length) obtained for the unseen data appear to be caused more by the nonexistence of a correct analysis in a number of cases, rather than by a marked decline in the usefulness of the rule probabilities.
Following (Briscoe and Carroll, 1993), conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration. $$$$$ Our use of local ambiguity packing does not in practice seem to result in exponentially bad performance with respect to sentence length (cf.
Following (Briscoe and Carroll, 1993), conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration. $$$$$ In addition, computation of all possible analyses is likely to be expensive and, in the limit, intractable.

The probability function can be obtained on the basis of a treebank, as proposed by (Briscoe and Carroll, 1993). $$$$$ This component pairs logical forms with each distinct syntactic analysis that represent, among other things, the predicate-argument structure of the input.
The probability function can be obtained on the basis of a treebank, as proposed by (Briscoe and Carroll, 1993). $$$$$ Thus the probabilistic CFG model predicts (incorrectly) that a) and f) will have the same probability of occurrence.

The model by (Briscoe and Carroll, 1993) however incorporated a mistake involving lookahead, which was corrected by (Inui et al, 2000). $$$$$ Another respect in which the model is approximate is that we are associating probabilities with the context-free backbone of the unification grammar.
The model by (Briscoe and Carroll, 1993) however incorporated a mistake involving lookahead, which was corrected by (Inui et al, 2000). $$$$$ The major task of the backbone grammar is to encode sufficient information (in the atomic categoried CF rules) from the unification grammar to constrain the application of the latter's rules at parse time.
The model by (Briscoe and Carroll, 1993) however incorporated a mistake involving lookahead, which was corrected by (Inui et al, 2000). $$$$$ A total of 246 definitions, selected without regard for their syntactic form, were parsed semi-automatically using the parser described in Section 5.

One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al, 2000) is that trained probabilistic LR parsers should be proper. $$$$$ Alex Lascarides and four anonymous reviewers' comments on earlier drafts were very helpful to us in preparing the final version.
One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al, 2000) is that trained probabilistic LR parsers should be proper. $$$$$ Richard Sharman kindly calculated the perplexity measures for this corpus.
One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al, 2000) is that trained probabilistic LR parsers should be proper. $$$$$ The latter is constructed by associating probabilities with the LR parse table directly.
One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al, 2000) is that trained probabilistic LR parsers should be proper. $$$$$ Section 7 explains the technique we employ for deriving a probabilistic version of the LR parse table from the training corpus, and demonstrates that this leads to a more refined and parse-contextâ€”dependent probabilistic model capable of distinguishing derivations that in a probabilistic context-free model would be equally probable.

There have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., Chodorow et al 1985), specially constructed definition parsers (e.g., Wilks et al 1996, Vossen 1995), and even general coverage syntactic parsers (e.g., Briscoe and Carroll 1993). $$$$$ We would like to thank Longman Group Ltd. for allowing us access to the LDOCE MRD and Ann Copestake and Antonio Sanfilippo for considerable help in the analysis of the LDOCE noun definition corpus.
There have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., Chodorow et al 1985), specially constructed definition parsers (e.g., Wilks et al 1996, Vossen 1995), and even general coverage syntactic parsers (e.g., Briscoe and Carroll 1993). $$$$$ The most straightforward technique for associating probabilities with the parse table is to assign a probability to each action in the action part of the table (e.g.
There have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., Chodorow et al 1985), specially constructed definition parsers (e.g., Wilks et al 1996, Vossen 1995), and even general coverage syntactic parsers (e.g., Briscoe and Carroll 1993). $$$$$ Briscoe (1987) demonstrates that the structure of the search space in parse derivations makes a left-to-right, incremental mode of parse selection most efficient.
There have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., Chodorow et al 1985), specially constructed definition parsers (e.g., Wilks et al 1996, Vossen 1995), and even general coverage syntactic parsers (e.g., Briscoe and Carroll 1993). $$$$$ Finally, we discuss limitations of the current system and possible extensions to deal with lexical (syntactic and semantic) frequency of occurrence.

Our second method of acquiring verb grammatical relations uses the statistical parser developed by Briscoe and Carroll (1993, 1997) which is an extension of the ANLT grammar development system which we used for our deep grammatical analysis as reported in Section 3 above. $$$$$ One obvious technique would be to generate all possible parses with a conventional parser and to have the analyst select the correct parse from the set returned (or reject them all).
Our second method of acquiring verb grammatical relations uses the statistical parser developed by Briscoe and Carroll (1993, 1997) which is an extension of the ANLT grammar development system which we used for our deep grammatical analysis as reported in Section 3 above. $$$$$ We report promising results of a pilot study training on 150 noun definitions from the Longman Dictionary of Contemporary English (LDOCE) and retesting on these plus a further 55 definitions.
Our second method of acquiring verb grammatical relations uses the statistical parser developed by Briscoe and Carroll (1993, 1997) which is an extension of the ANLT grammar development system which we used for our deep grammatical analysis as reported in Section 3 above. $$$$$ In this version of the table a probability is associated with each shift action in the standard way, but separate probabilities are associated with reduce Parse derivations for the winter holiday camp closed. actions, depending on the state reached after the action; for example, in state 4 with lookahead N@ the probability of reducing with rule 10 is 0.17 if the state reached is 3 and 0.22 if the state reached is 5.
Our second method of acquiring verb grammatical relations uses the statistical parser developed by Briscoe and Carroll (1993, 1997) which is an extension of the ANLT grammar development system which we used for our deep grammatical analysis as reported in Section 3 above. $$$$$ A naive implementation of an interactive LR parser would ask the user the correct category for each ambiguous word as it was shifted; many open-class words are assigned upwards of twenty lexical categories by the ANLT lexicon with comparatively fine distinctions between them, so this strategy would be completely impracticable.
