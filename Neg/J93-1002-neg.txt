Briscoe and Carroll (1993) observed that half of the parse failures were caused by inaccurate sub categorization information in the lexicon. $$$$$ Section 3 discusses work on LR parsing of natural language and presents our technique for automatic construction of LR parsers for unification-based grammars.
Briscoe and Carroll (1993) observed that half of the parse failures were caused by inaccurate sub categorization information in the lexicon. $$$$$ For example, the ANLT word and sentence grammar (Grover et al. 1989; Carroll and Grover 1989) consists of an English lexicon of approximately 40,000 lexemes and a 'compiled' fixed-arity term unification grammar containing around 700 phrase structure rules.
Briscoe and Carroll (1993) observed that half of the parse failures were caused by inaccurate sub categorization information in the lexicon. $$$$$ For example, the ANLT word and sentence grammar (Grover et al. 1989; Carroll and Grover 1989) consists of an English lexicon of approximately 40,000 lexemes and a 'compiled' fixed-arity term unification grammar containing around 700 phrase structure rules.
Briscoe and Carroll (1993) observed that half of the parse failures were caused by inaccurate sub categorization information in the lexicon. $$$$$ We would like to thank Longman Group Ltd. for allowing us access to the LDOCE MRD and Ann Copestake and Antonio Sanfilippo for considerable help in the analysis of the LDOCE noun definition corpus.

If, instead, this procedure returns a list of several possible actions with corresponding probabilities, we can then parse with a model similar to the probabilistic LR models described by Briscoe and Carroll (1993), where the probability of a parse tree is the product of the probabilities of each of the actions taken in its derivation. $$$$$ Richard Sharman kindly calculated the perplexity measures for this corpus.
If, instead, this procedure returns a list of several possible actions with corresponding probabilities, we can then parse with a model similar to the probabilistic LR models described by Briscoe and Carroll (1993), where the probability of a parse tree is the product of the probabilities of each of the actions taken in its derivation. $$$$$ Extensions to the LR technique, for example those using LR-regular grammars (Culic and Cohen 1973; Bermudez 1991), might be used to further cut down on interactions; however, computation of the parse tables to drive such extended LR parsers may prove intractable for large NL grammars (Hektoen 1991).
If, instead, this procedure returns a list of several possible actions with corresponding probabilities, we can then parse with a model similar to the probabilistic LR models described by Briscoe and Carroll (1993), where the probability of a parse tree is the product of the probabilities of each of the actions taken in its derivation. $$$$$ Such checking is cheap in terms of machine resources and very effective in cutting down both the number of choice points the user is forced to consider and also the average number of options in each one.
If, instead, this procedure returns a list of several possible actions with corresponding probabilities, we can then parse with a model similar to the probabilistic LR models described by Briscoe and Carroll (1993), where the probability of a parse tree is the product of the probabilities of each of the actions taken in its derivation. $$$$$ This technique is superior to parsers based on probabilistic lexical tagging or probabilistic context-free grammar because it allows for a more context-dependent probabilistic language model, as well as use of a more linguistically adequate grammar formalism.

 $$$$$ 7.
 $$$$$ These requirements immediately suggest that approaches that recover only lexical tags (e.g. de Rose 1988) or a syntactic analysis that is the 'closest fit' to some previously defined set of possible analyses (e.g.
 $$$$$ The construction method ensures that for any given grammar the CF backbone captures at least as much information as the optimal CFG that contains the same number of rules as the unification grammar.

work by Briscoe and Carroll (1993) on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags. $$$$$ The most straightforward technique for associating probabilities with the parse table is to assign a probability to each action in the action part of the table (e.g.
work by Briscoe and Carroll (1993) on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags. $$$$$ Alex Lascarides and four anonymous reviewers' comments on earlier drafts were very helpful to us in preparing the final version.
work by Briscoe and Carroll (1993) on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags. $$$$$ Section 6 describes our implementation of a breadth-first LR parser and compares its performance empirically to a highly optimized chart parser for the same grammar, suggesting that (optimized) LR parsing is more efficient in practice for the ANLT grammar despite exponential worst case complexity results.
work by Briscoe and Carroll (1993) on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags. $$$$$ By combining such techniques and relaxing the CNF constraint, for example, by adopting the trellis algorithm version of Baum-Welch re-estimation (Kupiec 1991), it might be possible to create a computationally tractable system operating with a realistic NL grammar that would only infer a new rule from a finite space of linguistically motivated possibilities in the face of parse failure or improbability.

Finally, we observe that there are also trainable stochastic shift-reduce parser models (Briscoe and Carroll, 1993), which are theoretically related to shift-reduce parsing, but operate in a highly non deterministic fashion during parsing. $$$$$ Briscoe (1987) demonstrates that the structure of the search space in parse derivations makes a left-to-right, incremental mode of parse selection most efficient.
Finally, we observe that there are also trainable stochastic shift-reduce parser models (Briscoe and Carroll, 1993), which are theoretically related to shift-reduce parsing, but operate in a highly non deterministic fashion during parsing. $$$$$ We describe work toward the construction of a very wide-coverage probabilistic parsing system for natural language (NL), based on LR parsing techniques.
Finally, we observe that there are also trainable stochastic shift-reduce parser models (Briscoe and Carroll, 1993), which are theoretically related to shift-reduce parsing, but operate in a highly non deterministic fashion during parsing. $$$$$ In general, these words will not be adjacent in the text, so it will not be possible to use existing approaches unmodified (e.g.

Following (Briscoe and Carroll, 1993), conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration. $$$$$ Lexical entries are more problematic, since there is little sign that the number of new entries required will tail off.
Following (Briscoe and Carroll, 1993), conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration. $$$$$ The goal of the work reported here is to develop a practical parser capable of returning probabilistically highly ranked analyses (from the usually large number of syntactically legitimate possibilities) for material drawn from a specific corpus on the basis of minimal (supervised) training and manual modification.
Following (Briscoe and Carroll, 1993), conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration. $$$$$ In addition, Hiyan Alshawi, David Weir, and Steve Young have helped clarify our thinking and made several suggestions that have influenced the way this research has developed.
Following (Briscoe and Carroll, 1993), conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration. $$$$$ For example, b) would be an appropriate analysis for toy coffee grinder, while c) would be appropriate for cat food tin, and each of d) and e) yields one of the two possible interpretations of the man in the park with the telescope.

The probability function can be obtained on the basis of a treebank, as proposed by (Briscoe and Carroll, 1993). $$$$$ . esp the period, and coordinations without explicit conjunctions ending with etc., and so forth.
The probability function can be obtained on the basis of a treebank, as proposed by (Briscoe and Carroll, 1993). $$$$$ This modified version of the system presented here is able to return analyses for sentences over 31 words in length, yields slightly better results on a replication of the experiment reported in Section 8, and the resultant parser is approximately three times faster at returning the three highest-ranked parsers than that presented here.
The probability function can be obtained on the basis of a treebank, as proposed by (Briscoe and Carroll, 1993). $$$$$ However, the disadvantage of this approach is that it relies on a prior (and perhaps CPU-intensive) on-line computation of the full set of analyses.
The probability function can be obtained on the basis of a treebank, as proposed by (Briscoe and Carroll, 1993). $$$$$ In Tomita's LR parsing framework, each such rule must be manually converted into a rule of the following form in which some subpart of each category has been replaced by an atomic symbol.

The model by (Briscoe and Carroll, 1993) however incorporated a mistake involving lookahead, which was corrected by (Inui et al, 2000). $$$$$ Alex Lascarides and four anonymous reviewers' comments on earlier drafts were very helpful to us in preparing the final version.
The model by (Briscoe and Carroll, 1993) however incorporated a mistake involving lookahead, which was corrected by (Inui et al, 2000). $$$$$ Instead the parser pursues each option in a limited breadth-first fashion and only requests help with analysis paths that remain active.
The model by (Briscoe and Carroll, 1993) however incorporated a mistake involving lookahead, which was corrected by (Inui et al, 2000). $$$$$ Our parser is driven by the LALR(1) state table computed from the backbone grammar, but in addition on each reduction the parser performs the unifications appropriate to the unification grammar version of the backbone rule involved.
The model by (Briscoe and Carroll, 1993) however incorporated a mistake involving lookahead, which was corrected by (Inui et al, 2000). $$$$$ Immediate prospects for applying such techniques to realistic NL grammars do not seem promising—the ANLT backbone grammar discussed in Section 4 contains almost 500 categories.

One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al, 2000) is that trained probabilistic LR parsers should be proper. $$$$$ In the ANLT grammar and lexicon, lexical ambiguity is at least as pervasive as structural ambiguity.
One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al, 2000) is that trained probabilistic LR parsers should be proper. $$$$$ Probabilistic Approaches to Parsing In the field of speech recognition, statistical techniques based on hidden Markov mod
One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al, 2000) is that trained probabilistic LR parsers should be proper. $$$$$ As long as the method employed ensured that any analysis assigned was a member of the set defined by the grammar, these problems during training should not arise.
One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al, 2000) is that trained probabilistic LR parsers should be proper. $$$$$ Section 7 explains the technique we employ for deriving a probabilistic version of the LR parse table from the training corpus, and demonstrates that this leads to a more refined and parse-context—dependent probabilistic model capable of distinguishing derivations that in a probabilistic context-free model would be equally probable.

There have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., Chodorow et al 1985), specially constructed definition parsers (e.g., Wilks et al 1996, Vossen 1995), and even general coverage syntactic parsers (e.g., Briscoe and Carroll 1993). $$$$$ For example, the ANLT word and sentence grammar (Grover et al. 1989; Carroll and Grover 1989) consists of an English lexicon of approximately 40,000 lexemes and a 'compiled' fixed-arity term unification grammar containing around 700 phrase structure rules.
There have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., Chodorow et al 1985), specially constructed definition parsers (e.g., Wilks et al 1996, Vossen 1995), and even general coverage syntactic parsers (e.g., Briscoe and Carroll 1993). $$$$$ Reduction to NP of a pronoun in object position always results in the parser returning to state 11.
There have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., Chodorow et al 1985), specially constructed definition parsers (e.g., Wilks et al 1996, Vossen 1995), and even general coverage syntactic parsers (e.g., Briscoe and Carroll 1993). $$$$$ (The reasons for choosing this corpus are discussed in the introduction.)
There have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., Chodorow et al 1985), specially constructed definition parsers (e.g., Wilks et al 1996, Vossen 1995), and even general coverage syntactic parsers (e.g., Briscoe and Carroll 1993). $$$$$ Our eventual goal is to recover a semantically and pragmatically appropriate syntactic analysis capable of supporting semantic interpretation.

Our second method of acquiring verb grammatical relations uses the statistical parser developed by Briscoe and Carroll (1993, 1997) which is an extension of the ANLT grammar development system which we used for our deep grammatical analysis as reported in Section 3 above. $$$$$ This research is supported by SERC/DTI-IED project 4/1/1261 'Extensions to the Alvey Natural Language Tools' and by ESPRIT BRA 3030 'Acquisition of Lexical Information from Machine-Readable Dictionaries.'
Our second method of acquiring verb grammatical relations uses the statistical parser developed by Briscoe and Carroll (1993, 1997) which is an extension of the ANLT grammar development system which we used for our deep grammatical analysis as reported in Section 3 above. $$$$$ In Section 2, we briefly review extant work on probabilistic approaches to corpus analysis and parsing and argue the need for a more refined probabilistic model to distinguish distinct derivations.
Our second method of acquiring verb grammatical relations uses the statistical parser developed by Briscoe and Carroll (1993, 1997) which is an extension of the ANLT grammar development system which we used for our deep grammatical analysis as reported in Section 3 above. $$$$$ Our apto grammar tuning is based on a semi-automatic parsing phase which additions to the grammar are made manually and statistical information concerning the frequency of use of grammar rules is acquired.
