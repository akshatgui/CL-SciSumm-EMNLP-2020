 $$$$$ Different parameter settings within this framework approximate different existing similarity measures as well as many more which have, until now, been unexplored.
 $$$$$ Third, from Figure 3, it appears that this WordNet prediction task favours measures which select high recall neighbours.
 $$$$$ Further, different parameter settings will approximate different existing similarity measures as well as many more which have, until now, been unexplored.
 $$$$$ We study similarity between high and low frequency nouns since we want to investigate any associations between word frequency and quality of neighbours found by the measures but it is impractical to evaluate a large number of similarity measures over all nouns.

(Weeds and Weir, 2003)) measure of Lin (1998) as a representative case, and utilized it for our analysis and as a starting point for improvement. $$$$$ Here, the degree of association between a noun n and a verb v is their MI.
(Weeds and Weir, 2003)) measure of Lin (1998) as a representative case, and utilized it for our analysis and as a starting point for improvement. $$$$$ As is common in this field (e.g.
(Weeds and Weir, 2003)) measure of Lin (1998) as a representative case, and utilized it for our analysis and as a starting point for improvement. $$$$$ In the semantic domain, the hypothesis that words which mean similar things behave in similar ways (Levin, 1993), has led researchers (e.g.
(Weeds and Weir, 2003)) measure of Lin (1998) as a representative case, and utilized it for our analysis and as a starting point for improvement. $$$$$ However, in the future we wish to extend our evaluation to other tasks such as malapropism correction (Budanitsky and Hirst, 2001) and PP-attachment ambiguity resolution (Resnik, 1993) and also to the probabilistic model.

 $$$$$ Although optimum similarity for the combinatorial model occurs at ,8=0.5, similarity is always higher for lower values of than for higher values of )3. ing the a-skew divergence measure and those found using the MI-Based model.
 $$$$$ Here, the degree of association between a noun n and a verb v is their MI.
 $$$$$ We will now consider some different possibilities for measuring the degree of association between a noun n and a verb v. In the combinatorial model, we simply consider whether a verb has ever been seen to co-occur with the noun.
 $$$$$ Although pseudo-disambiguation itself is an artificial task, it has relevance in at least two real application areas.

However, it is not at all obvious that one universally best measure exists for all applications (Weeds and Weir, 2003). $$$$$ Each set of test triples was split into five disjoint subsets, containing two triples for each noun, so that average performance and standard error could be computed.
However, it is not at all obvious that one universally best measure exists for all applications (Weeds and Weir, 2003). $$$$$ We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns.
However, it is not at all obvious that one universally best measure exists for all applications (Weeds and Weir, 2003). $$$$$ In this section, we evaluate the performance of the framework, using the combinatorial and MI-based models of precision and recall, at two application based tasks against Lin's MIbased Measure (simun) and the a-skew Divergence Measure (simasd).
However, it is not at all obvious that one universally best measure exists for all applications (Weeds and Weir, 2003). $$$$$ Further, there is no clear way of deciding which is the best measure.

 $$$$$ Standard errors in the optimal mean similarities are not given but were of the order of 0.1.
 $$$$$ Second, we evaluate the framework at its optimal parameter settings for two different applications (Section 3), showing that it outperforms existing state-ofthe-art similarity measures for both high and low frequency nouns.
 $$$$$ In this section, we evaluate the performance of the framework, using the combinatorial and MI-based models of precision and recall, at two application based tasks against Lin's MIbased Measure (simun) and the a-skew Divergence Measure (simasd).

Weeds and Weir (2003) proposed a general framework for distributional similarity that mainly consists of the notions of what they call Precision and Recall. $$$$$ We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns.
Weeds and Weir (2003) proposed a general framework for distributional similarity that mainly consists of the notions of what they call Precision and Recall. $$$$$ The two evaluation tasks used pseudo-disambiguation and WordNet (Fellbaum, 1998) prediction are fairly standard for distributional similarity measures.
Weeds and Weir (2003) proposed a general framework for distributional similarity that mainly consists of the notions of what they call Precision and Recall. $$$$$ However, we believe that a distributional similarity measure which more closely predicts WordNet, is more likely to be a good predictor of semantic similarity.

title=Textual_Entailment_Resource_Pool 69 To date, most distributional similarity research concentrated on symmetric measures, such as the widely cited and competitive (as shown in (Weeds and Weir, 2003)) LIN measure (Lin, 1998): LIN (u, v)=? f? FV u? FV v [w u (f)+ w v (f)]? f? FV u w u (f)+? f? FV v w v (f) where FV x is the feature vector of a word x and w x (f) is the weight of the feature f in that word? s vector, set to their point wise mutual information. $$$$$ However, in the future we wish to extend our evaluation to other tasks such as malapropism correction (Budanitsky and Hirst, 2001) and PP-attachment ambiguity resolution (Resnik, 1993) and also to the probabilistic model.
title=Textual_Entailment_Resource_Pool 69 To date, most distributional similarity research concentrated on symmetric measures, such as the widely cited and competitive (as shown in (Weeds and Weir, 2003)) LIN measure (Lin, 1998): LIN (u, v)=? f? FV u? FV v [w u (f)+ w v (f)]? f? FV u w u (f)+? f? FV v w v (f) where FV x is the feature vector of a word x and w x (f) is the weight of the feature f in that word? s vector, set to their point wise mutual information. $$$$$ This formula can be used in combination with any of the models for precision and recall outlined above.
title=Textual_Entailment_Resource_Pool 69 To date, most distributional similarity research concentrated on symmetric measures, such as the widely cited and competitive (as shown in (Weeds and Weir, 2003)) LIN measure (Lin, 1998): LIN (u, v)=? f? FV u? FV v [w u (f)+ w v (f)]? f? FV u w u (f)+? f? FV v w v (f) where FV x is the feature vector of a word x and w x (f) is the weight of the feature f in that word? s vector, set to their point wise mutual information. $$$$$ We have explored two such models here - a combinatorial model and a MIbased model - and have shown that the MIbased model achieves significantly improved results over the combinatorial model.

 $$$$$ For the askew divergence measure we set a = 0.99 since this most closely approximates the KullbackLeibler divergence measure.
 $$$$$ Additionally, three of the five subsets were used as a development set to optimise parameters (k, ,i3 and -y) and the remaining two used as a test set to find error rates at these optimal settings.
 $$$$$ Accordingly, we might automatically build thesauruses which could be used in tasks such as malapropism correction (Budanitsky and Hirst, 2001) and text summarization (Silber and McCoy, 2002).
 $$$$$ So, if syn(n) is the set of senses of the noun n in WordNet, sup(c) is the set of (possibly indirect) superclasses of concept c in WordNet and P(c) is the probability that a randomly selected noun refers to an instance of c, then the similarity between ni and n2 can be calculated using the formula for simwn in Figure 1.

For this reason, a new approach could be envisaged for this task, in the direction of the work by (Weeds and Weir, 2003), by building rankings of similarity for each verb. $$$$$ However, we have shown that using a much lower value of -y so that the combination of precision and recall is closer to a weighted arithmetic mean than a harmonic mean yields better results in the two application tasks considered here.
For this reason, a new approach could be envisaged for this task, in the direction of the work by (Weeds and Weir, 2003), by building rankings of similarity for each verb. $$$$$ Third, we begin to investigate to what extent existing similarity measures might be characterised in terms of parameter settings within the framework (Section 4).
For this reason, a new approach could be envisaged for this task, in the direction of the work by (Weeds and Weir, 2003), by building rankings of similarity for each verb. $$$$$ We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns.

As a case study, we used our evaluation methodology to compare four methods for learning entailment rules between predicates: DIRT (Lin and Pantel,2001), Cover (Weeds and Weir, 2003), BInc (Szpek tor and Dagan, 2008) and Berant et al (2010). $$$$$ We present a general framework for distributional similarity based on the concepts of precision and recall.
As a case study, we used our evaluation methodology to compare four methods for learning entailment rules between predicates: DIRT (Lin and Pantel,2001), Cover (Weeds and Weir, 2003), BInc (Szpek tor and Dagan, 2008) and Berant et al (2010). $$$$$ We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns.
