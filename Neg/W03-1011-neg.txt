 $$$$$ We take a generic approach that does not directly reduce distributional similarity to a single dimension.
 $$$$$ Fourth, we provide an understanding of why a single existing measure cannot achieve optimal results in every application of distributional similarity measures.
 $$$$$ Here, the degree of association between a noun n and a verb v is their MI.
 $$$$$ Using the MI-based model for precision and recall and with a parameter setting of -y = 1.0, the general framework for distributional similarity proposed herein closely approximates Lin's (1998) Measure.

(Weeds and Weir, 2003)) measure of Lin (1998) as a representative case, and utilized it for our analysis and as a starting point for improvement. $$$$$ First, we propose a general framework for distributional similarity based on the concepts of precision and recall (Section 2).
(Weeds and Weir, 2003)) measure of Lin (1998) as a representative case, and utilized it for our analysis and as a starting point for improvement. $$$$$ Different parameter settings within this framework approximate different existing similarity measures as well as many more which have, until now, been unexplored.

 $$$$$ Standard errors in the optimal mean similarities are not given but were of the order of 0.1.
 $$$$$ However, the loose definition of distributional similarity that two words are distributionally similar if they appear in similar contexts has led to many distributional similarity measures being proposed; for example, the L1 Norm, the Euclidean Distance, the Cosine Metric (Salton and McGill, 1983), Jaccard's Coefficient (Frakes and Baeza-Yates, 1992), the Dice Coefficient (Frakes and Baeza-Yates, 1992), the KullbackLeibler Divergence (Cover and Thomas, 1991), the Jenson-Shannon Divergence (Rao, 1983), the a-skew Divergence (Lee, 1999), the Confusion Probability (Essen and Steinbiss, 1992), Hindle's Mutual Information(MI)-Based Measure (Hindle, 1990) and Lin's MI-Based Measure (Lin, 1998).
 $$$$$ Lee, 1999) have become a standard evaluation technique and, in the current context, we may use a word's neighbours to decide which of two cooccurrences is the most likely.

However, it is not at all obvious that one universally best measure exists for all applications (Weeds and Weir, 2003). $$$$$ Accordingly, we might automatically build thesauruses which could be used in tasks such as malapropism correction (Budanitsky and Hirst, 2001) and text summarization (Silber and McCoy, 2002).
However, it is not at all obvious that one universally best measure exists for all applications (Weeds and Weir, 2003). $$$$$ Standard errors in the optimal mean similarities are not given but were of the order of 0.1.
However, it is not at all obvious that one universally best measure exists for all applications (Weeds and Weir, 2003). $$$$$ In the future, we intend to extend the work to the characterisation of other tasks and other existing similarity measures.
However, it is not at all obvious that one universally best measure exists for all applications (Weeds and Weir, 2003). $$$$$ This formula can be used in combination with any of the models for precision and recall outlined above.

 $$$$$ Table 3 summarizes the optimal mean similarities and parameter settings for the general framework using both the combinatorial (sim) and the MI-based (simmt) models.
 $$$$$ We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns.

Weeds and Weir (2003) proposed a general framework for distributional similarity that mainly consists of the notions of what they call Precision and Recall. $$$$$ For the askew divergence measure we set a = 0.99 since this most closely approximates the KullbackLeibler divergence measure.
Weeds and Weir (2003) proposed a general framework for distributional similarity that mainly consists of the notions of what they call Precision and Recall. $$$$$ Using the MI-based model for precision and recall and with a parameter setting of -y = 1.0, the general framework for distributional similarity proposed herein closely approximates Lin's (1998) Measure.
Weeds and Weir (2003) proposed a general framework for distributional similarity that mainly consists of the notions of what they call Precision and Recall. $$$$$ This is because the relative importance of precision and recall can be tuned to the task at hand.

title=Textual_Entailment_Resource_Pool 69 To date, most distributional similarity research concentrated on symmetric measures, such as the widely cited and competitive (as shown in (Weeds and Weir, 2003)) LIN measure (Lin, 1998) $$$$$ We present a general framework for distributional similarity based on the concepts of precision and recall.
title=Textual_Entailment_Resource_Pool 69 To date, most distributional similarity research concentrated on symmetric measures, such as the widely cited and competitive (as shown in (Weeds and Weir, 2003)) LIN measure (Lin, 1998) $$$$$ Different parameter settings within this framework approximate different existing similarity measures as well as many more which have, until now, been unexplored.
title=Textual_Entailment_Resource_Pool 69 To date, most distributional similarity research concentrated on symmetric measures, such as the widely cited and competitive (as shown in (Weeds and Weir, 2003)) LIN measure (Lin, 1998) $$$$$ Having performed this transformation, the neighbour sets for the same word w may be represented by two ordered sets of words [wk, w1] and [w, wl].
title=Textual_Entailment_Resource_Pool 69 To date, most distributional similarity research concentrated on symmetric measures, such as the widely cited and competitive (as shown in (Weeds and Weir, 2003)) LIN measure (Lin, 1998) $$$$$ In the semantic domain, the hypothesis that words which mean similar things behave in similar ways (Levin, 1993), has led researchers (e.g.

 $$$$$ Since we use the same data and methodology as in earlier work, some detail is omitted in the subsequent discussion but full details and rationale can be found in Weeds and Weir (2003).
 $$$$$ We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns.
 $$$$$ P(root of tree6) = 1).
 $$$$$ Results for Lin's MI-based measure (simun) and the a-skew divergence measure (simasd) are also given and results are divided into those for high frequency nouns and those for low frequency nouns.

For this reason, a new approach could be envisaged for this task, in the direction of the work by (Weeds and Weir, 2003), by building rankings of similarity for each verb. $$$$$ Fourth, we provide an understanding of why a single existing measure cannot achieve optimal results in every application of distributional similarity measures.
For this reason, a new approach could be envisaged for this task, in the direction of the work by (Weeds and Weir, 2003), by building rankings of similarity for each verb. $$$$$ As is common in this field (e.g.
For this reason, a new approach could be envisaged for this task, in the direction of the work by (Weeds and Weir, 2003), by building rankings of similarity for each verb. $$$$$ There are two obvious ways to optimise a pair of numbers such as precision and recall.

As a case study, we used our evaluation methodology to compare four methods for learning entailment rules between predicates $$$$$ Although optimum similarity for the combinatorial model occurs at ,8=0.5, similarity is always higher for lower values of than for higher values of )3. ing the a-skew divergence measure and those found using the MI-Based model.
As a case study, we used our evaluation methodology to compare four methods for learning entailment rules between predicates $$$$$ There are two obvious ways to optimise a pair of numbers such as precision and recall.
As a case study, we used our evaluation methodology to compare four methods for learning entailment rules between predicates $$$$$ We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns.
As a case study, we used our evaluation methodology to compare four methods for learning entailment rules between predicates $$$$$ In this section, we evaluate the performance of the framework, using the combinatorial and MI-based models of precision and recall, at two application based tasks against Lin's MIbased Measure (simun) and the a-skew Divergence Measure (simasd).
