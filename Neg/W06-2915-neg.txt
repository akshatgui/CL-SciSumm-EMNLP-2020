Given a pair of document collections A and B, our goal is not to construct classifiers that can predict if a document was written from the perspective of A or B (Lin et al, 2006), but to determine if the document collection pair (A, B) convey opposing perspectives. $$$$$ Given a new document W with a unknown document perspective, the perspective D� is calculated based on the following conditional probability.
Given a pair of document collections A and B, our goal is not to construct classifiers that can predict if a document was written from the perspective of A or B (Lin et al, 2006), but to determine if the document collection pair (A, B) convey opposing perspectives. $$$$$ Can computers learn to identify the perspective of a document?
Given a pair of document collections A and B, our goal is not to construct classifiers that can predict if a document was written from the perspective of A or B (Lin et al, 2006), but to determine if the document collection pair (A, B) convey opposing perspectives. $$$$$ We show that much of a document’s perspective is expressed in word usage, and statistical learning algorithms such as SVM and naive Bayes models can successfully uncover the word patterns that reflect author perspective with high accuracy.
Given a pair of document collections A and B, our goal is not to construct classifiers that can predict if a document was written from the perspective of A or B (Lin et al, 2006), but to determine if the document collection pair (A, B) convey opposing perspectives. $$$$$ The results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy.

In some cases, the author may also introduce their own perspective (Lin et al, 2006) through the use of framing (Greene and Resnik, 2009). $$$$$ Second, the bitterlemons corpus enables us to test the generalizability of the proposed models in a very realistic setting: training on articles written by a small number of writers (two editors) and testing on articles from a much larger group of writers (more than 200 different guests).
In some cases, the author may also introduce their own perspective (Lin et al, 2006) through the use of framing (Greene and Resnik, 2009). $$$$$ The distribution of documents and sentences are listed in cluding edition numbers, publication dates, topics, titles, author names and biographic information.
In some cases, the author may also introduce their own perspective (Lin et al, 2006) through the use of framing (Greene and Resnik, 2009). $$$$$ We evaluate three different models for the task of identifying perspective at the document level: two naive Bayes models (NB) with different inference methods and Support Vector Machines (SVM) (Cristianini and Shawe-Taylor, 2000).

Hierarchical Bayesian modelling has recently gained notable popularity in many core areas of natural language processing, from morphological segmentation (Goldwater et al, 2009) to opinion modelling (Lin et al, 2006). $$$$$ However, can computers learn to identify the perspective of a document given a training corpus?
Hierarchical Bayesian modelling has recently gained notable popularity in many core areas of natural language processing, from morphological segmentation (Goldwater et al, 2009) to opinion modelling (Lin et al, 2006). $$$$$ However, can computers learn to identify the perspective of a document given a training corpus?
Hierarchical Bayesian modelling has recently gained notable popularity in many core areas of natural language processing, from morphological segmentation (Goldwater et al, 2009) to opinion modelling (Lin et al, 2006). $$$$$ The results suggest that the choice of words made by the authors, either consciously or subconsciously, reflects much of their political perspectives.

We show that using adaptive naive Bayes improves on state of the art classification using the Bitter Lemons corpus (Lin et al, 2006), a document collection that has been used by a variety of authors to evaluate perspective classification. $$$$$ By perspective we mean a point of view, for example, from the perspective of Democrats or Republicans.
We show that using adaptive naive Bayes improves on state of the art classification using the Bitter Lemons corpus (Lin et al, 2006), a document collection that has been used by a variety of authors to evaluate perspective classification. $$$$$ The comparable performance between the naive Bayes models and LSPM is in fact surprising.
We show that using adaptive naive Bayes improves on state of the art classification using the Bitter Lemons corpus (Lin et al, 2006), a document collection that has been used by a variety of authors to evaluate perspective classification. $$$$$ We run three chains and collect 5000 samples.

The support vector machine (SVM), NB B and LSPM results are taken directly from Lin et al (2006). $$$$$ As with review classification, we treat perspective identification as a document-level classification task, discriminating, in a sense, between different types of opinions.
The support vector machine (SVM), NB B and LSPM results are taken directly from Lin et al (2006). $$$$$ By introducing latent variables and sharing parameters, the Latent Sentence Perspective Model is shown to capture well how perspectives are reflected at the document and sentence levels.

We report 4-fold cross-validation (DP-4) using the folds in Greene and Resnik (2009), where training and testing data come from different websites for each of the sides, as well as 10-fold cross-validation performance on the entire corpus, irrespective of the site. Bitter Lemons (BL) $$$$$ In this paper we investigate a new problem identifying the which a document is written.
We report 4-fold cross-validation (DP-4) using the folds in Greene and Resnik (2009), where training and testing data come from different websites for each of the sides, as well as 10-fold cross-validation performance on the entire corpus, irrespective of the site. Bitter Lemons (BL) $$$$$ In addition, we develop a novel statistical model to estimate how strongly a sentence conveys perspective, in the absence of sentence-level annotations.
We report 4-fold cross-validation (DP-4) using the folds in Greene and Resnik (2009), where training and testing data come from different websites for each of the sides, as well as 10-fold cross-validation performance on the entire corpus, irrespective of the site. Bitter Lemons (BL) $$$$$ By introducing latent variables and sharing parameters, the Latent Sentence Perspective Model is shown to capture well how perspectives are reflected at the document and sentence levels.
We report 4-fold cross-validation (DP-4) using the folds in Greene and Resnik (2009), where training and testing data come from different websites for each of the sides, as well as 10-fold cross-validation performance on the entire corpus, irrespective of the site. Bitter Lemons (BL) $$$$$ A Gibbs Samplers Based the model specification described in Section 4.2 we derive the Gibbs samplers (Chen et al., 2000) for the Latent Sentence Perspective Model as follows, where dbinom and dmultinom are the density functions of binomial and multinomial distributions, respectively.

 $$$$$ We set the hyperparameters απ, βπ, and αθ to one, resulting in non-informative priors.
 $$$$$ In this paper we investigate a new problem identifying the which a document is written.
 $$$$$ A Gibbs Samplers Based the model specification described in Section 4.2 we derive the Gibbs samplers (Chen et al., 2000) for the Latent Sentence Perspective Model as follows, where dbinom and dmultinom are the density functions of binomial and multinomial distributions, respectively.

For these features we replace the opinion words with their positive or negative polarity equivalents (Lin et al, 2006). $$$$$ This material is based on work supported by the Advanced Research and Development Activity (ARDA) under contract number NBCHC040037.
For these features we replace the opinion words with their positive or negative polarity equivalents (Lin et al, 2006). $$$$$ Can computers learn to identify which sentences strongly convey a particular perspective?
For these features we replace the opinion words with their positive or negative polarity equivalents (Lin et al, 2006). $$$$$ The experimental results are shown in Table 5.
For these features we replace the opinion words with their positive or negative polarity equivalents (Lin et al, 2006). $$$$$ For example, in documents that are written from the Palestinian perspective, the word “palestinian” is mentioned more frequently than the word “israel.” It is, however, the reverse for documents that are written from the Israeli perspective.

They attempt to identify a position of a debate, such as ideological (Somasundaran et al, 2010, Lin et al, 2006) or product comparison debate (Somasundaran et al, 2009). $$$$$ The experimental results show that the proposed statistical models can successfully identify the perspective from which a document is written with high accuracy.
They attempt to identify a position of a debate, such as ideological (Somasundaran et al, 2010, Lin et al, 2006) or product comparison debate (Somasundaran et al, 2009). $$$$$ Second, the bitterlemons corpus enables us to test the generalizability of the proposed models in a very realistic setting: training on articles written by a small number of writers (two editors) and testing on articles from a much larger group of writers (more than 200 different guests).
They attempt to identify a position of a debate, such as ideological (Somasundaran et al, 2010, Lin et al, 2006) or product comparison debate (Somasundaran et al, 2009). $$$$$ Can computers learn to identify which sentences strongly convey a particular perspective?
They attempt to identify a position of a debate, such as ideological (Somasundaran et al, 2010, Lin et al, 2006) or product comparison debate (Somasundaran et al, 2009). $$$$$ Can computers learn to identify the perspective of a document?

These experiments were conducted in political debate corpus (Lin et al 2006). $$$$$ We evaluated the subjectivity of each sentence using the automatic subjective sentence classifier from (Riloff and Wiebe, 2003), and find that 65.6% of Palestinian sentences and 66.2% of Israeli sentences are classified as subjective.
These experiments were conducted in political debate corpus (Lin et al 2006). $$$$$ In other words, sentences that are presenting common background information or introducing an issue and that do not strongly convey any perspective should look similar whether they are in Palestinian or Israeli documents.
These experiments were conducted in political debate corpus (Lin et al 2006). $$$$$ The high but almost equivalent percentages of subjective sentences in the two perspectives support our observation in Section 2 that a perspective is largely expressed using subjective language, but that the amount of subjectivity in a document is not necessarily indicative of its perspective.
These experiments were conducted in political debate corpus (Lin et al 2006). $$$$$ The whole generative process is modeled as follows: The parameters π and θ have the same semantics as in the naive Bayes model.

(Lin et al, 2006) explores relationships between sentence-level and document-level classification for a stance-like prediction task. Among the literature on ideological subjectivity, perhaps most similar to our work is (Somasundaran and Wiebe, 2010). $$$$$ Moreover, the ideology and beliefs authors possess are often expressed in ways other than positive or negative language toward specific targets.
(Lin et al, 2006) explores relationships between sentence-level and document-level classification for a stance-like prediction task. Among the literature on ideological subjectivity, perhaps most similar to our work is (Somasundaran and Wiebe, 2010). $$$$$ A positive or negative opinion toward a particular movie or product is fundamentally different from an overall perspective.
(Lin et al, 2006) explores relationships between sentence-level and document-level classification for a stance-like prediction task. Among the literature on ideological subjectivity, perhaps most similar to our work is (Somasundaran and Wiebe, 2010). $$$$$ The high accuracy on the mismatched experiments suggests that statistical models are not learning writing styles or editing artifacts.
(Lin et al, 2006) explores relationships between sentence-level and document-level classification for a stance-like prediction task. Among the literature on ideological subjectivity, perhaps most similar to our work is (Somasundaran and Wiebe, 2010). $$$$$ The results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy.

Some sentences are written from a certain perspective (Lin et al, 2006) or point of view. $$$$$ We collected a total of 594 articles published on the website from late 2001 to early 2005.
Some sentences are written from a certain perspective (Lin et al, 2006) or point of view. $$$$$ For training SVM, we represent each document as a V-dimensional feature vector, where V is the vocabulary size and each coordinate is the normalized term frequency within the document.
Some sentences are written from a certain perspective (Lin et al, 2006) or point of view. $$$$$ We develop statistical models to capture how perspectives are expressed at the document and sentence levels, and evaluate the proposed models on articles about the Israeli-Palestinian conflict.
Some sentences are written from a certain perspective (Lin et al, 2006) or point of view. $$$$$ We develop statistical models to learn how perspectives are reflected in word usage, and we treat the problem of identifying perspectives as a classification task.

As it is easy for a human to identify the perspective of an author (Lin et al, 2006), this measure facilitated the annotation task. $$$$$ As with review classification, we treat perspective identification as a document-level classification task, discriminating, in a sense, between different types of opinions.
As it is easy for a human to identify the perspective of an author (Lin et al, 2006), this measure facilitated the annotation task. $$$$$ We run three chains and collect 5000 samples.
As it is easy for a human to identify the perspective of an author (Lin et al, 2006), this measure facilitated the annotation task. $$$$$ If what the SVM and naive Bayes models learn are writing styles or editing artifacts, the classification performance under the mismatched conditions will be considerably degraded.

In our second study, we make a direct comparison with prior state-of-the-art classification using the Bitter Lemons corpus of Lin et al (2006). $$$$$ First, each article is already labeled as either Palestinian or Israeli by the editors, allowing us to exploit existing annotations.
In our second study, we make a direct comparison with prior state-of-the-art classification using the Bitter Lemons corpus of Lin et al (2006). $$$$$ Authors from different perspectives often choose words from a similar vocabulary but emphasize them differently.
In our second study, we make a direct comparison with prior state-of-the-art classification using the Bitter Lemons corpus of Lin et al (2006). $$$$$ A Gibbs Samplers Based the model specification described in Section 4.2 we derive the Gibbs samplers (Chen et al., 2000) for the Latent Sentence Perspective Model as follows, where dbinom and dmultinom are the density functions of binomial and multinomial distributions, respectively.
In our second study, we make a direct comparison with prior state-of-the-art classification using the Bitter Lemons corpus of Lin et al (2006). $$$$$ The first half of burn-in samples are discarded.

Israeli-Palestinian Conflict In order to make a direct comparison here with prior state-of-the-art work on sentiment analysis, we re port on sentiment classification using OPUS features in experiments using a publicly available corpus involving opposing perspectives, the Bitter Lemons corpus introduced by Lin et al (2006) .Corpus. $$$$$ One’s opinion will change from movie to movie, whereas one’s perspective can be seen as more static, often underpinned by one’s ideology or beliefs about the world.
Israeli-Palestinian Conflict In order to make a direct comparison here with prior state-of-the-art work on sentiment analysis, we re port on sentiment classification using OPUS features in experiments using a publicly available corpus involving opposing perspectives, the Bitter Lemons corpus introduced by Lin et al (2006) .Corpus. $$$$$ In addition to identifying the perspective of a document, we are interested in knowing which sentences of the document strongly conveys perspective information.
Israeli-Palestinian Conflict In order to make a direct comparison here with prior state-of-the-art work on sentiment analysis, we re port on sentiment classification using OPUS features in experiments using a publicly available corpus involving opposing perspectives, the Bitter Lemons corpus introduced by Lin et al (2006) .Corpus. $$$$$ The website is set up to “contribute to mutual understanding [between Palestinians and Israelis] through the open exchange of ideas.”3 Every week an issue about the IsraeliPalestinian conflict is selected for discussion (e.g., “Disengagement: unilateral or coordinated?”), and a Palestinian editor and an Israeli editor each contribute one article addressing the issue.
Israeli-Palestinian Conflict In order to make a direct comparison here with prior state-of-the-art work on sentiment analysis, we re port on sentiment classification using OPUS features in experiments using a publicly available corpus involving opposing perspectives, the Bitter Lemons corpus introduced by Lin et al (2006) .Corpus. $$$$$ For training SVM, we represent each document as a V-dimensional feature vector, where V is the vocabulary size and each coordinate is the normalized term frequency within the document.

Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al (2006) under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science. $$$$$ Can computers learn to identify the perspective of a document?
Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al (2006) under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science. $$$$$ 5 may capture aspects of the document collection that we never intended to model.
Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al (2006) under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science. $$$$$ Political analysts regularly monitor the positions that countries take on international and domestic issues.

They often belong to controversial subjects (e.g., religion, terrorism, etc.) where the same event can beseen from two or more opposing perspectives, like the IsraeliPalestinian conflict (Lin et al, 2006). $$$$$ There has been research in discourse analysis that examines how different perspectives are expressed in political discourse (van Dijk, 1988; Pan et al., 1999; Geis, 1987).
They often belong to controversial subjects (e.g., religion, terrorism, etc.) where the same event can beseen from two or more opposing perspectives, like the IsraeliPalestinian conflict (Lin et al, 2006). $$$$$ The accuracy of LSPM is comparable or even slightly better than that of the naive Bayes models.
