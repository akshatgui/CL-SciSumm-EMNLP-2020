These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. $$$$$ The performance of the baseline model is shown in the top row of table 3.
These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. $$$$$ Figure1 shows a derivation taken from CCGbank.
These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. $$$$$ The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex traction and coordination.We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations.
These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. $$$$$ In order to estimate the conditional probabilitiesof our model, we recursively smooth empirical es timates e?i of specific conditional distributions with(possible smoothed) estimates of less specific distri butions e?i

Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). $$$$$ This results in a smaller and less overgenerating grammar than standard PCFGs (ca.3,000 rules when instantiated with the above cate gories in sections 02-21, instead of >12,400 in the original Treebank representation (Collins, 1999)).
Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). $$$$$ There is a separate level for nouns and noun phrases, but, like the nonterminal NP in the Penn Treebank, noun phrases do not carry any number agreement.
Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). $$$$$ In order to estimate the conditional probabilitiesof our model, we recursively smooth empirical es timates e?i of specific conditional distributions with(possible smoothed) estimates of less specific distri butions e?i
Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). $$$$$ Almost alltypes of construction?with the exception of gap ping and UCP (?Unlike Coordinate Phrases?)

Hockenmaier and Steedman (2002) describe a generative model for CCG, which only requires a non-iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. $$$$$ Expansion HeadCat NonHeadCat P(exp j : : : ) P(H j : : : ) P(S j : : : ) Baseline P P;exp P;exp;H + Conj P;con jP P;exp;con jP P;exp;H ;con jP + Grandparent P;GP P;GP;exp P;GP;exp;H + ? P#?L;RP P;exp#?L;RP P;exp;H#?L;RP Table 1: The unlexicalized models The models described here are all extensions of a very simple model which models derivations by a top-down tree-generating process.
Hockenmaier and Steedman (2002) describe a generative model for CCG, which only requires a non-iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. $$$$$ For all experiments reported here and in section 5, the frequency threshold was set to 5.

The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory rules are encoded as rule instances, together with a number of additional rules which deal with punctuation and type-changing. $$$$$ The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex traction and coordination.We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations.
The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory rules are encoded as rule instances, together with a number of additional rules which deal with punctuation and type-changing. $$$$$ The performance of the baseline model is shown in the top row of table 3.
The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory rules are encoded as rule instances, together with a number of additional rules which deal with punctuation and type-changing. $$$$$ However, the grammar underlying the Penn Treebank is very permissive, and a parser can do well on the standard Parsevalmeasures without committing itself on certain se mantically significant decisions, such as predicting null elements arising from deletion or movement.

Few hand crafted, deep linguistic grammars achieve the coverage and robustness needed to parse large corpora (see (Riezler et al, 2002), (Burke et al, 2004) and (Hockenmaier and Steedman, 2002) for exceptions), and speed remains a serious challenge. $$$$$ In order to estimate the conditional probabilitiesof our model, we recursively smooth empirical es timates e?i of specific conditional distributions with(possible smoothed) estimates of less specific distri butions e?i
Few hand crafted, deep linguistic grammars achieve the coverage and robustness needed to parse large corpora (see (Riezler et al, 2002), (Burke et al, 2004) and (Hockenmaier and Steedman, 2002) for exceptions), and speed remains a serious challenge. $$$$$ CCG grammars are characterized by much larger categorysets than standard Penn Treebank grammars, distin guishing for example between many classes of verbswith different subcategorization frames.
Few hand crafted, deep linguistic grammars achieve the coverage and robustness needed to parse large corpora (see (Riezler et al, 2002), (Burke et al, 2004) and (Hockenmaier and Steedman, 2002) for exceptions), and speed remains a serious challenge. $$$$$ The grammar contains a set of type-changing rules similar to the lexical rules described in Carpenter (1992).
Few hand crafted, deep linguistic grammars achieve the coverage and robustness needed to parse large corpora (see (Riezler et al, 2002), (Burke et al, 2004) and (Hockenmaier and Steedman, 2002) for exceptions), and speed remains a serious challenge. $$$$$ The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex traction and coordination.We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations.

We used Clark & Curran's wide coverage statistical parser (Clark and Curran, 2004) trained on CCG-bank, which in turn is derived from the Penn-Treebank (Hockenmaier and Steedman, 2002). $$$$$ Categories, such as ((S[b]nNP)=PP)=NP, encode unsaturated subcat frames.
We used Clark & Curran's wide coverage statistical parser (Clark and Curran, 2004) trained on CCG-bank, which in turn is derived from the Penn-Treebank (Hockenmaier and Steedman, 2002). $$$$$ This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Gram mar (CCG) parser.
We used Clark & Curran's wide coverage statistical parser (Clark and Curran, 2004) trained on CCG-bank, which in turn is derived from the Penn-Treebank (Hockenmaier and Steedman, 2002). $$$$$ Therefore, we alsoevaluate performance using a dependency evaluation reported by Collins (1999), which counts word word dependencies as determined by local trees and their labels.

 $$$$$ The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex traction and coordination.We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations.
 $$$$$ We too can extend the baseline model described in the previous section by including more features.
 $$$$$ CCG grammars are characterized by much larger categorysets than standard Penn Treebank grammars, distin guishing for example between many classes of verbswith different subcategorization frames.
 $$$$$ This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Gram mar (CCG) parser.

 $$$$$ These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations.
 $$$$$ The complement-adjunct distinction is made explicit; for instance as a nonexecutive director is marked up as PP-CLR in the Tree bank, and hence treated as a PP-complement of join, whereas Nov. 29 is marked up as an NP-TMP and therefore analyzed as VP modifier.
 $$$$$ The performance of the baseline model is shown in the top row of table 3.
 $$$$$ State-of-the-art statistical parsers use many other features, or conditioning variables, such as head words, subcategorization frames, distance measures and grandparent nodes.

 $$$$$ This measureis neutral with respect to the branching factor.
 $$$$$ Incontrast to Gildea (2001), we find a significant improvement from modeling word word dependencies.

In order to obtain CCG derivations for all sentences in the ACE corpus, we used the CCG parser introduced in (Hockenmaier and Steedman,2002). $$$$$ This measureis neutral with respect to the branching factor.
In order to obtain CCG derivations for all sentences in the ACE corpus, we used the CCG parser introduced in (Hockenmaier and Steedman,2002). $$$$$ These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations.
In order to obtain CCG derivations for all sentences in the ACE corpus, we used the CCG parser introduced in (Hockenmaier and Steedman,2002). $$$$$ We too can extend the baseline model described in the previous section by including more features.
In order to obtain CCG derivations for all sentences in the ACE corpus, we used the CCG parser introduced in (Hockenmaier and Steedman,2002). $$$$$ According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to thefigures given by Collins (1999) for a lin guistically less expressive grammar.

Hockenmaier and Steedman (2002) describe a generative model of normal-form derivations. $$$$$ As a re sult, the categorial lexicon extracted for this purposefrom the training corpus has 1207 categories, com pared with the 48 POS-tags of the Penn Treebank.On the other hand, grammar rules in CCG are lim ited to a small number of simple unary and binary combinatory schemata such as function application and composition.
Hockenmaier and Steedman (2002) describe a generative model of normal-form derivations. $$$$$ If P is a leaf node, generate its head word w. Otherwise, generate the category ofits head daughter H . If P is binary branching, gen erate the category of its non-head daughter S (a complement or modifier of H).The model itself includes no prior knowledge spe cific to CCG other than that it only allows unary andbinary branching trees, and that the sets of nontermi nals and terminals are not disjoint (hence the need to include leaf as a possible expansion, which acts as a stop probability).
Hockenmaier and Steedman (2002) describe a generative model of normal-form derivations. $$$$$ In order to estimate the conditional probabilitiesof our model, we recursively smooth empirical es timates e?i of specific conditional distributions with(possible smoothed) estimates of less specific distri butions e?i
Hockenmaier and Steedman (2002) describe a generative model of normal-form derivations. $$$$$ There is a separate level for nouns and noun phrases, but, like the nonterminal NP in the Penn Treebank, noun phrases do not carry any number agreement.

The CCG parser has been trained and tested on CCGbank (Hockenmaier and Steedman, 2002a), a tree bank of CCG derivations obtained from the Penn Treebank, from which we also obtain our training data. $$$$$ However, the grammar underlying the Penn Treebank is very permissive, and a parser can do well on the standard Parsevalmeasures without committing itself on certain se mantically significant decisions, such as predicting null elements arising from deletion or movement.
The CCG parser has been trained and tested on CCGbank (Hockenmaier and Steedman, 2002a), a tree bank of CCG derivations obtained from the Penn Treebank, from which we also obtain our training data. $$$$$ In order to compare our performance with the parser of Clark et al (2002), we also evaluate our best model according to the dependency evaluation introduced for that parser.
The CCG parser has been trained and tested on CCGbank (Hockenmaier and Steedman, 2002a), a tree bank of CCG derivations obtained from the Penn Treebank, from which we also obtain our training data. $$$$$ Fur thermore, as noted by Hockenmaier (2001), it doesnot penalize equivalent analyses of multiple modi Computational Linguistics (ACL), Philadelphia, July 2002, pp.

The feature set for the normal-form model is the same except that, following Hockenmaier and Steedman (2002), the dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features. $$$$$ Figure1 shows a derivation taken from CCGbank.
The feature set for the normal-form model is the same except that, following Hockenmaier and Steedman (2002), the dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features. $$$$$ CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Tree bank trees using an algorithm described by Hockenmaier and Steedman (2002).
The feature set for the normal-form model is the same except that, following Hockenmaier and Steedman (2002), the dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features. $$$$$ As a re sult, the categorial lexicon extracted for this purposefrom the training corpus has 1207 categories, com pared with the 48 POS-tags of the Penn Treebank.On the other hand, grammar rules in CCG are lim ited to a small number of simple unary and binary combinatory schemata such as function application and composition.
The feature set for the normal-form model is the same except that, following Hockenmaier and Steedman (2002), the dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features. $$$$$ State-of-the-art statistical parsers use many other features, or conditioning variables, such as head words, subcategorization frames, distance measures and grandparent nodes.

The best-performing model encodes word-word dependencies in terms of the local rule instantiations, as in Hockenmaier and Steedman (2002). $$$$$ The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex traction and coordination.We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations.
The best-performing model encodes word-word dependencies in terms of the local rule instantiations, as in Hockenmaier and Steedman (2002). $$$$$ arecovered by the translation procedure, which pro cesses 98.3% of the sentences in the training corpus (WSJ sections 02-21) and 98.5% of the sentences in the test corpus (WSJ section 23).
The best-performing model encodes word-word dependencies in terms of the local rule instantiations, as in Hockenmaier and Steedman (2002). $$$$$ All the experiments reported in this section were conducted using sections 02-21 of CCGbank as training corpus, and section 23 as test corpus.

For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). $$$$$ The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex traction and coordination.We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations.
For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). $$$$$ Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly.
For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). $$$$$ CCG grammars are characterized by much larger categorysets than standard Penn Treebank grammars, distin guishing for example between many classes of verbswith different subcategorization frames.

We focus on two parsing models $$$$$ The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex traction and coordination.We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations.
We focus on two parsing models $$$$$ This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Gram mar (CCG) parser.
We focus on two parsing models $$$$$ We discuss a simple, but imperfect, solution to this problem in section 7.
We focus on two parsing models $$$$$ Proceedings of the 40th Annual Meeting of the Association for Pierre Vinken ; 61 years old ; will join the board as a nonexecutive director Nov 29 N=N N ; N=N N (S[adj]nNP)nNP ; (S[dcl]nNP)=(S[b]nNP) ((S[b]nNP)=PP)=NP NP=N N PP=NP NP=N N=N N ((SnNP)n(SnNP))=N N > > > > > N N N N (SnNP)n(SnNP) > NP NP NP NP < > > NP S[adj]nNP (S[b]nNP)=PP PP > NPnNP S[b]nNP < < NP S[b]nNP > NP S[dcl]nNP < S[dcl] Figure 1: A CCG derivation in our corpus fiers.

Hockenmaier and Steedman (2002) saw a similar effect. $$$$$ CCG grammars are characterized by much larger categorysets than standard Penn Treebank grammars, distin guishing for example between many classes of verbswith different subcategorization frames.
Hockenmaier and Steedman (2002) saw a similar effect. $$$$$ As a re sult, the categorial lexicon extracted for this purposefrom the training corpus has 1207 categories, com pared with the 48 POS-tags of the Penn Treebank.On the other hand, grammar rules in CCG are lim ited to a small number of simple unary and binary combinatory schemata such as function application and composition.
Hockenmaier and Steedman (2002) saw a similar effect. $$$$$ For six out of the 2379 sentences in our test corpus we do not get a parse.1The reason is that a lexicon consisting of the word category pairs observed in the training corpus does not contain all the entries required to parse the test corpus.
Hockenmaier and Steedman (2002) saw a similar effect. $$$$$ State-of-the-art statistical parsers use many other features, or conditioning variables, such as head words, subcategorization frames, distance measures and grandparent nodes.

The model used by the CCG parser of Hockenmaier and Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. $$$$$ Incontrast to Gildea (2001), we find a significant improvement from modeling word word dependencies.
The model used by the CCG parser of Hockenmaier and Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. $$$$$ We too can extend the baseline model described in the previous section by including more features.
The model used by the CCG parser of Hockenmaier and Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. $$$$$ The performance of the baseline model is shown in the top row of table 3.
The model used by the CCG parser of Hockenmaier and Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. $$$$$ This model was originally described in Hockenmaier (2001), where it was applied to a preliminary version of CCGbank, and its definition is repeated here in the top row of Table 1.

State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. $$$$$ Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly.
State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. $$$$$ in the sense that analyses involving the combinatory rules of type-raising and composition are only used when syntactically necessary.
State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. $$$$$ As a re sult, the categorial lexicon extracted for this purposefrom the training corpus has 1207 categories, com pared with the 48 POS-tags of the Penn Treebank.On the other hand, grammar rules in CCG are lim ited to a small number of simple unary and binary combinatory schemata such as function application and composition.

First, we review the dependency model proposed by Hockenmaier and Steedman (2002b). $$$$$ State-of-the-art statistical parsers use many other features, or conditioning variables, such as head words, subcategorization frames, distance measures and grandparent nodes.
First, we review the dependency model proposed by Hockenmaier and Steedman (2002b). $$$$$ In order to compare our performance with the parser of Clark et al (2002), we also evaluate our best model according to the dependency evaluation introduced for that parser.
First, we review the dependency model proposed by Hockenmaier and Steedman (2002b). $$$$$ Therefore, we alsoevaluate performance using a dependency evaluation reported by Collins (1999), which counts word word dependencies as determined by local trees and their labels.
First, we review the dependency model proposed by Hockenmaier and Steedman (2002b). $$$$$ Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly.
