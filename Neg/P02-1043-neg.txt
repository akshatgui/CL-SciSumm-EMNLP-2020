These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. $$$$$ If P is a leaf node, generate its head word w. Otherwise, generate the category ofits head daughter H . If P is binary branching, gen erate the category of its non-head daughter S (a complement or modifier of H).The model itself includes no prior knowledge spe cific to CCG other than that it only allows unary andbinary branching trees, and that the sets of nontermi nals and terminals are not disjoint (hence the need to include leaf as a possible expansion, which acts as a stop probability).
These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. $$$$$ State-of-the-art statistical parsers use many other features, or conditioning variables, such as head words, subcategorization frames, distance measures and grandparent nodes.
These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. $$$$$ In order to compare our performance with the parser of Clark et al (2002), we also evaluate our best model according to the dependency evaluation introduced for that parser.
These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. $$$$$ Incontrast to Gildea (2001), we find a significant improvement from modeling word word dependencies.

Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). $$$$$ We too can extend the baseline model described in the previous section by including more features.
Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). $$$$$ For all experiments reported here and in section 5, the frequency threshold was set to 5.
Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). $$$$$ The grammar contains a set of type-changing rules similar to the lexical rules described in Carpenter (1992).
Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). $$$$$ This results in a smaller and less overgenerating grammar than standard PCFGs (ca.3,000 rules when instantiated with the above cate gories in sections 02-21, instead of >12,400 in the original Treebank representation (Collins, 1999)).

Hockenmaier and Steedman (2002) describe a generative model for CCG, which only requires a non-iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. $$$$$ CCG grammars are characterized by much larger categorysets than standard Penn Treebank grammars, distin guishing for example between many classes of verbswith different subcategorization frames.
Hockenmaier and Steedman (2002) describe a generative model for CCG, which only requires a non-iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. $$$$$ We too can extend the baseline model described in the previous section by including more features.
Hockenmaier and Steedman (2002) describe a generative model for CCG, which only requires a non-iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. $$$$$ These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations.
Hockenmaier and Steedman (2002) describe a generative model for CCG, which only requires a non-iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. $$$$$ The -CLR tag is not in fact a very reliable indicator of whether a constituent should be treated as a complement, but the translation to CCG is automatic and must do the best it can with the information in the Treebank.The verbal categories in CCGbank carry features distinguishing declarative verbs (and auxiliaries) from past participles in past tense, past par ticiples for passive, bare infinitives and ing-forms.

The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory rules are encoded as rule instances, together with a number of additional rules which deal with punctuation and type-changing. $$$$$ In order to estimate the conditional probabilitiesof our model, we recursively smooth empirical es timates e?i of specific conditional distributions with(possible smoothed) estimates of less specific distri butions e?i
The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory rules are encoded as rule instances, together with a number of additional rules which deal with punctuation and type-changing. $$$$$ The currently best single-model statistical parser (Charniak, 1999) achieves Parseval scores of over 89% on the Penn Treebank.
The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory rules are encoded as rule instances, together with a number of additional rules which deal with punctuation and type-changing. $$$$$ The currently best single-model statistical parser (Charniak, 1999) achieves Parseval scores of over 89% on the Penn Treebank.

Few hand crafted, deep linguistic grammars achieve the coverage and robustness needed to parse large corpora (see (Riezler et al, 2002), (Burke et al, 2004) and (Hockenmaier and Steedman, 2002) for exceptions), and speed remains a serious challenge. $$$$$ This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Gram mar (CCG) parser.

We used Clark & Curran's wide coverage statistical parser (Clark and Curran, 2004) trained on CCG-bank, which in turn is derived from the Penn-Treebank (Hockenmaier and Steedman, 2002). $$$$$ Given a (parent) node with category P, choose the expansion exp of P, where exp can beleaf (for lexical categories), unary (for unary ex pansions such as type-raising), left (for binary trees where the head daughter is left) or right (binary trees, head right).
We used Clark & Curran's wide coverage statistical parser (Clark and Curran, 2004) trained on CCG-bank, which in turn is derived from the Penn-Treebank (Hockenmaier and Steedman, 2002). $$$$$ The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex traction and coordination.We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations.

 $$$$$ We discuss a simple, but imperfect, solution to this problem in section 7.
 $$$$$ The grammar contains a set of type-changing rules similar to the lexical rules described in Carpenter (1992).
 $$$$$ Almost alltypes of construction?with the exception of gap ping and UCP (?Unlike Coordinate Phrases?)
 $$$$$ leftor right, which is implicit in CCG categories) de fines a hP;H;Si dependency between the head word of S, wS, and the head word of H , wH.

 $$$$$ We replace all rare words in the training data with their POS-tag.
 $$$$$ However, the grammar underlying the Penn Treebank is very permissive, and a parser can do well on the standard Parsevalmeasures without committing itself on certain se mantically significant decisions, such as predicting null elements arising from deletion or movement.
 $$$$$ State-of-the-art statistical parsers use many other features, or conditioning variables, such as head words, subcategorization frames, distance measures and grandparent nodes.
 $$$$$ This results in a smaller and less overgenerating grammar than standard PCFGs (ca.3,000 rules when instantiated with the above cate gories in sections 02-21, instead of >12,400 in the original Treebank representation (Collins, 1999)).

 $$$$$ The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex traction and coordination.We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations.
 $$$$$ The currently best single-model statistical parser (Charniak, 1999) achieves Parseval scores of over 89% on the Penn Treebank.
 $$$$$ Since CCG produces unary and binary branching trees with a very fine-grained category set, CCG Parseval scores cannot be compared with scores of standard Treebank parsers.
 $$$$$ As a re sult, the categorial lexicon extracted for this purposefrom the training corpus has 1207 categories, com pared with the 48 POS-tags of the Penn Treebank.On the other hand, grammar rules in CCG are lim ited to a small number of simple unary and binary combinatory schemata such as function application and composition.

In order to obtain CCG derivations for all sentences in the ACE corpus, we used the CCG parser introduced in (Hockenmaier and Steedman,2002). $$$$$ Incontrast to Gildea (2001), we find a significant improvement from modeling word word dependencies.
In order to obtain CCG derivations for all sentences in the ACE corpus, we used the CCG parser introduced in (Hockenmaier and Steedman,2002). $$$$$ This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Gram mar (CCG) parser.
In order to obtain CCG derivations for all sentences in the ACE corpus, we used the CCG parser introduced in (Hockenmaier and Steedman,2002). $$$$$ 335-342.
In order to obtain CCG derivations for all sentences in the ACE corpus, we used the CCG parser introduced in (Hockenmaier and Steedman,2002). $$$$$ We too can extend the baseline model described in the previous section by including more features.

Hockenmaier and Steedman (2002) describe a generative model of normal-form derivations. $$$$$ State-of-the-art statistical parsers use many other features, or conditioning variables, such as head words, subcategorization frames, distance measures and grandparent nodes.
Hockenmaier and Steedman (2002) describe a generative model of normal-form derivations. $$$$$ According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to thefigures given by Collins (1999) for a lin guistically less expressive grammar.
Hockenmaier and Steedman (2002) describe a generative model of normal-form derivations. $$$$$ The performance of the baseline model is shown in the top row of table 3.
Hockenmaier and Steedman (2002) describe a generative model of normal-form derivations. $$$$$ This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Gram mar (CCG) parser.

The CCG parser has been trained and tested on CCGbank (Hockenmaier and Steedman, 2002a), a tree bank of CCG derivations obtained from the Penn Treebank, from which we also obtain our training data. $$$$$ If P is a leaf node, generate its head word w. Otherwise, generate the category ofits head daughter H . If P is binary branching, gen erate the category of its non-head daughter S (a complement or modifier of H).The model itself includes no prior knowledge spe cific to CCG other than that it only allows unary andbinary branching trees, and that the sets of nontermi nals and terminals are not disjoint (hence the need to include leaf as a possible expansion, which acts as a stop probability).
The CCG parser has been trained and tested on CCGbank (Hockenmaier and Steedman, 2002a), a tree bank of CCG derivations obtained from the Penn Treebank, from which we also obtain our training data. $$$$$ CCG grammars are characterized by much larger categorysets than standard Penn Treebank grammars, distin guishing for example between many classes of verbswith different subcategorization frames.

The feature set for the normal-form model is the same except that, following Hockenmaier and Steedman (2002), the dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features. $$$$$ The -CLR tag is not in fact a very reliable indicator of whether a constituent should be treated as a complement, but the translation to CCG is automatic and must do the best it can with the information in the Treebank.The verbal categories in CCGbank carry features distinguishing declarative verbs (and auxiliaries) from past participles in past tense, past par ticiples for passive, bare infinitives and ing-forms.
The feature set for the normal-form model is the same except that, following Hockenmaier and Steedman (2002), the dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features. $$$$$ According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to thefigures given by Collins (1999) for a lin guistically less expressive grammar.
The feature set for the normal-form model is the same except that, following Hockenmaier and Steedman (2002), the dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features. $$$$$ CCG grammars are characterized by much larger categorysets than standard Penn Treebank grammars, distin guishing for example between many classes of verbswith different subcategorization frames.
The feature set for the normal-form model is the same except that, following Hockenmaier and Steedman (2002), the dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features. $$$$$ Since CCG produces unary and binary branching trees with a very fine-grained category set, CCG Parseval scores cannot be compared with scores of standard Treebank parsers.

The best-performing model encodes word-word dependencies in terms of the local rule instantiations, as in Hockenmaier and Steedman (2002). $$$$$ This results in a smaller and less overgenerating grammar than standard PCFGs (ca.3,000 rules when instantiated with the above cate gories in sections 02-21, instead of >12,400 in the original Treebank representation (Collins, 1999)).
The best-performing model encodes word-word dependencies in terms of the local rule instantiations, as in Hockenmaier and Steedman (2002). $$$$$ Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly.
The best-performing model encodes word-word dependencies in terms of the local rule instantiations, as in Hockenmaier and Steedman (2002). $$$$$ We discuss a simple, but imperfect, solution to this problem in section 7.
The best-performing model encodes word-word dependencies in terms of the local rule instantiations, as in Hockenmaier and Steedman (2002). $$$$$ The currently best single-model statistical parser (Charniak, 1999) achieves Parseval scores of over 89% on the Penn Treebank.

For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). $$$$$ There is a separate level for nouns and noun phrases, but, like the nonterminal NP in the Penn Treebank, noun phrases do not carry any number agreement.
For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). $$$$$ Incontrast to Gildea (2001), we find a significant improvement from modeling word word dependencies.
For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). $$$$$ According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to thefigures given by Collins (1999) for a lin guistically less expressive grammar.
For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). $$$$$ This measureis neutral with respect to the branching factor.

We focus on two parsing models: PCFG, the baseline of Hockenmaier and Steedman (2002) which treats the grammar as a PCFG (Table 1); and HWDep, a headword dependency model which is the best performing model of the parser. $$$$$ LikeCollins (1999), we assume that the test data is POS tagged, and can therefore replace unknown words inthe test data with their POS-tag, which is more ap propriate for a formalism like CCG with a large set of lexical categories than one generic token for all unknown words.
We focus on two parsing models: PCFG, the baseline of Hockenmaier and Steedman (2002) which treats the grammar as a PCFG (Table 1); and HWDep, a headword dependency model which is the best performing model of the parser. $$$$$ In the unlabeled case hi (where it only matters whether word a is a dependent of word b, not whatthe label of the local tree is which defines this depen dency), scores can be compared across grammars with different sets of labels and different kinds of trees.
We focus on two parsing models: PCFG, the baseline of Hockenmaier and Steedman (2002) which treats the grammar as a PCFG (Table 1); and HWDep, a headword dependency model which is the best performing model of the parser. $$$$$ This results in a smaller and less overgenerating grammar than standard PCFGs (ca.3,000 rules when instantiated with the above cate gories in sections 02-21, instead of >12,400 in the original Treebank representation (Collins, 1999)).
We focus on two parsing models: PCFG, the baseline of Hockenmaier and Steedman (2002) which treats the grammar as a PCFG (Table 1); and HWDep, a headword dependency model which is the best performing model of the parser. $$$$$ If P is a leaf node, generate its head word w. Otherwise, generate the category ofits head daughter H . If P is binary branching, gen erate the category of its non-head daughter S (a complement or modifier of H).The model itself includes no prior knowledge spe cific to CCG other than that it only allows unary andbinary branching trees, and that the sets of nontermi nals and terminals are not disjoint (hence the need to include leaf as a possible expansion, which acts as a stop probability).

Hockenmaier and Steedman (2002) saw a similar effect. $$$$$ Almost alltypes of construction?with the exception of gap ping and UCP (?Unlike Coordinate Phrases?)
Hockenmaier and Steedman (2002) saw a similar effect. $$$$$ The currently best single-model statistical parser (Charniak, 1999) achieves Parseval scores of over 89% on the Penn Treebank.
Hockenmaier and Steedman (2002) saw a similar effect. $$$$$ Therefore, we alsoevaluate performance using a dependency evaluation reported by Collins (1999), which counts word word dependencies as determined by local trees and their labels.
Hockenmaier and Steedman (2002) saw a similar effect. $$$$$ The derivations in CCGbank are ?normal-form?

The model used by the CCG parser of Hockenmaier and Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. $$$$$ CCG grammars are characterized by much larger categorysets than standard Penn Treebank grammars, distin guishing for example between many classes of verbswith different subcategorization frames.
The model used by the CCG parser of Hockenmaier and Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. $$$$$ This model was originally described in Hockenmaier (2001), where it was applied to a preliminary version of CCGbank, and its definition is repeated here in the top row of Table 1.
The model used by the CCG parser of Hockenmaier and Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. $$$$$ The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex traction and coordination.We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations.

State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. $$$$$ According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to thefigures given by Collins (1999) for a lin guistically less expressive grammar.
State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. $$$$$ The grammar contains a set of type-changing rules similar to the lexical rules described in Carpenter (1992).
State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. $$$$$ 335-342.

First, we review the dependency model proposed by Hockenmaier and Steedman (2002b). $$$$$ This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Gram mar (CCG) parser.
First, we review the dependency model proposed by Hockenmaier and Steedman (2002b). $$$$$ Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly.
First, we review the dependency model proposed by Hockenmaier and Steedman (2002b). $$$$$ If P is a leaf node, generate its head word w. Otherwise, generate the category ofits head daughter H . If P is binary branching, gen erate the category of its non-head daughter S (a complement or modifier of H).The model itself includes no prior knowledge spe cific to CCG other than that it only allows unary andbinary branching trees, and that the sets of nontermi nals and terminals are not disjoint (hence the need to include leaf as a possible expansion, which acts as a stop probability).
First, we review the dependency model proposed by Hockenmaier and Steedman (2002b). $$$$$ There is a separate level for nouns and noun phrases, but, like the nonterminal NP in the Penn Treebank, noun phrases do not carry any number agreement.
