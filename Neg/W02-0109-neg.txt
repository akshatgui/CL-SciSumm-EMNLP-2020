If evaluated against the requirements for teaching environments discussed in (Loper and Bird, 2002), GATE covers them all quite well. $$$$$ Fourth, it should be easy to write structured programs, ideally object-oriented but without the burden associated with languages like C++.
If evaluated against the requirements for teaching environments discussed in (Loper and Bird, 2002), GATE covers them all quite well. $$$$$ The tutorials include a high-level discussion that explains and motivates the domain, followed by a detailed walk-through that uses examples to show how NLTK can be used to perform specific tasks.
If evaluated against the requirements for teaching environments discussed in (Loper and Bird, 2002), GATE covers them all quite well. $$$$$ Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset.
If evaluated against the requirements for teaching environments discussed in (Loper and Bird, 2002), GATE covers them all quite well. $$$$$ The toolkit is implemented as a collection of independent modules, each of which defines a specific data structure or task.

However, other such modules, e.g., those from NLTK (Loper and Bird, 2002), can be used for such assignments. $$$$$ The wide variety of existing modules provide many opportunities for creating these simple assignments.
However, other such modules, e.g., those from NLTK (Loper and Bird, 2002), can be used for such assignments. $$$$$ Finally, the language must have an easy-to-use graphics library to support the development of graphical user interfaces.
However, other such modules, e.g., those from NLTK (Loper and Bird, 2002), can be used for such assignments. $$$$$ The project extended these, adding a new probabilistic parsing interface, and using subclasses to create a probabilistic version of the context free grammar data structure.
However, other such modules, e.g., those from NLTK (Loper and Bird, 2002), can be used for such assignments. $$$$$ NLTK is an open source project, and we welcome any contributions.

We use the Punkt sentence splitter from NLTK (Loper and Bird, 2002) to perform both sentence and word segmentation on each text chunk. $$$$$ The remaining modules define data structures and interfaces for performing specific NLP tasks.
We use the Punkt sentence splitter from NLTK (Loper and Bird, 2002) to perform both sentence and word segmentation on each text chunk. $$$$$ These are elementary on the computational side, providing a gentle introduction to students having no prior experience in computer science.
We use the Punkt sentence splitter from NLTK (Loper and Bird, 2002) to perform both sentence and word segmentation on each text chunk. $$$$$ We are grateful to Mitch Marcus and the Department of Computer and Information Science at the University of Pennsylvania for sponsoring the work reported here.
We use the Punkt sentence splitter from NLTK (Loper and Bird, 2002) to perform both sentence and word segmentation on each text chunk. $$$$$ Reference Documentation provides precise definitions for every module, interface, class, method, function, and variable in the toolkit.

NLTK, the Natural Language Toolkit, is a suite of Python modules providing many NLP data types, processing tasks, corpus samples and readers, together with animated algorithms, tutorials, and problem sets (Loper and Bird, 2002). $$$$$ We are indebted to our students for feedback on the toolkit, and to anonymous reviewers, Jee Bang, and the workshop organizers for comments on an earlier version of this paper.
NLTK, the Natural Language Toolkit, is a suite of Python modules providing many NLP data types, processing tasks, corpus samples and readers, together with animated algorithms, tutorials, and problem sets (Loper and Bird, 2002). $$$$$ The wide variety of existing modules provide many opportunities for creating these simple assignments.
NLTK, the Natural Language Toolkit, is a suite of Python modules providing many NLP data types, processing tasks, corpus samples and readers, together with animated algorithms, tutorials, and problem sets (Loper and Bird, 2002). $$$$$ Indeed, there should be a wide variety of ways in which students can extend the toolkit.
NLTK, the Natural Language Toolkit, is a suite of Python modules providing many NLP data types, processing tasks, corpus samples and readers, together with animated algorithms, tutorials, and problem sets (Loper and Bird, 2002). $$$$$ Here, NLTK provides some useful starting points: predefined interfaces and data structures, and existing modules that implement the same interface.

For English and German documents in all experiments, we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970), and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size). $$$$$ First, the language must have a shallow learning curve, so that novice programmers get immediate rewards for their efforts.
For English and German documents in all experiments, we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970), and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size). $$$$$ Recent work includes (Copestake, 2000; Baldridge et al., 2002a).
For English and German documents in all experiments, we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970), and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size). $$$$$ These new components were used in conjunction with several existing components, such as the chart data structure, to define two implementations of the probabilistic parsing interface.
For English and German documents in all experiments, we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970), and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size). $$$$$ It is clear that these considerable overheads and shortcomings warrant a fresh approach.

We have not yet used the Natural Language Toolkit (Loper and Bird, 2002) (see Section 3.1) in this course. $$$$$ Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset.
We have not yet used the Natural Language Toolkit (Loper and Bird, 2002) (see Section 3.1) in this course. $$$$$ By relying on the built-in features of various languages, the teacher avoids having to develop a lot of software infrastructure.
We have not yet used the Natural Language Toolkit (Loper and Bird, 2002) (see Section 3.1) in this course. $$$$$ A more challenging task is to develop a new module.
We have not yet used the Natural Language Toolkit (Loper and Bird, 2002) (see Section 3.1) in this course. $$$$$ As a result, students who worked at home needed to download new versions of the toolkit several times throughout the semester.

Finally, all texts were lemmatized using Porter's stemmer (1980) for English and Snowballstemmers for other languages using an implementation provided by the NLTK (Loper and Bird, 2002). $$$$$ Students can also consult these reports if they would like further information about how the toolkit is designed, and why it is designed that way.
Finally, all texts were lemmatized using Porter's stemmer (1980) for English and Snowballstemmers for other languages using an implementation provided by the NLTK (Loper and Bird, 2002). $$$$$ Apart from the practical component, computational linguistics courses may also depend on software for in-class demonstrations.
Finally, all texts were lemmatized using Porter's stemmer (1980) for English and Snowballstemmers for other languages using an implementation provided by the NLTK (Loper and Bird, 2002). $$$$$ The tool can then be reset to any one of these charts at any time.
Finally, all texts were lemmatized using Porter's stemmer (1980) for English and Snowballstemmers for other languages using an implementation provided by the NLTK (Loper and Bird, 2002). $$$$$ Second, the language must support rapid prototyping and a short develop/test cycle; an obligatory compilation step is a serious detraction.

We strip unnecessary HTML tags and Wiki templates with mwlib5 and split sentences with NLTK (Loper and Bird, 2002). $$$$$ Technical Reports explain and justify the toolkitâ€™s design and implementation.
We strip unnecessary HTML tags and Wiki templates with mwlib5 and split sentences with NLTK (Loper and Bird, 2002). $$$$$ Fourth, it should be easy to write structured programs, ideally object-oriented but without the burden associated with languages like C++.
We strip unnecessary HTML tags and Wiki templates with mwlib5 and split sentences with NLTK (Loper and Bird, 2002). $$$$$ Another issue was the fact that we were actively developing NLTK during the semester; some modules were only completed one or two weeks before the students used them.
We strip unnecessary HTML tags and Wiki templates with mwlib5 and split sentences with NLTK (Loper and Bird, 2002). $$$$$ These design criteria are listed in the order of their importance.

Some popular options include the NLTK (Loper and Bird, 2002), CSLU (Cole, 1999), Trindi (Larsson and Traum, 2000) and Regulus (Rayner et al, 2003) toolkits. $$$$$ Most students had a background in either computer science or linguistics (and occasionally both).
Some popular options include the NLTK (Loper and Bird, 2002), CSLU (Cole, 1999), Trindi (Larsson and Traum, 2000) and Regulus (Rayner et al, 2003) toolkits. $$$$$ Examples include the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), the EMU Speech Database System (Harrington and Cassidy, 1999), the General Architecture for Text Engineering (Bontcheva et al., 2002), the Maxent Package for Maximum Entropy Models (Baldridge et al., 2002b), and the Annotation Graph Toolkit (Maeda et al., 2002).
Some popular options include the NLTK (Loper and Bird, 2002), CSLU (Cole, 1999), Trindi (Larsson and Traum, 2000) and Regulus (Rayner et al, 2003) toolkits. $$$$$ By relying on the built-in features of various languages, the teacher avoids having to develop a lot of software infrastructure.
Some popular options include the NLTK (Loper and Bird, 2002), CSLU (Cole, 1999), Trindi (Larsson and Traum, 2000) and Regulus (Rayner et al, 2003) toolkits. $$$$$ NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora.

We use a simple path distance similarity measure, as implemented in NLTK (Loper and Bird, 2002). $$$$$ NLTK is unique in its combination of three factors.
We use a simple path distance similarity measure, as implemented in NLTK (Loper and Bird, 2002). $$$$$ Third, the code should be self-documenting, with a transparent syntax and semantics.
We use a simple path distance similarity measure, as implemented in NLTK (Loper and Bird, 2002). $$$$$ In surveying the available languages, we believe that Python offers an especially good fit to the above requirements.
We use a simple path distance similarity measure, as implemented in NLTK (Loper and Bird, 2002). $$$$$ Other Researchers and Developers.

Our word pairs are lemmatized using the Wordnet based lemmatizer of NLTK (Loper and Bird, 2002). $$$$$ Students were required to complete five assignments, two exams, and a final project.
Our word pairs are lemmatized using the Wordnet based lemmatizer of NLTK (Loper and Bird, 2002). $$$$$ NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware.
Our word pairs are lemmatized using the Wordnet based lemmatizer of NLTK (Loper and Bird, 2002). $$$$$ Various books introduce programming or computing to linguists.
Our word pairs are lemmatized using the Wordnet based lemmatizer of NLTK (Loper and Bird, 2002). $$$$$ The type checking module is used by all of the basic data types and processing classes.

The Natural Language Toolkit, or NLTK, was developed to give a broad range of students access to the core knowledge and skills of NLP (Loper and Bird, 2002). $$$$$ The user can exercise fine-grained control over the algorithm by selecting which edge they wish to apply a rule to.
The Natural Language Toolkit, or NLTK, was developed to give a broad range of students access to the core knowledge and skills of NLP (Loper and Bird, 2002). $$$$$ Clear designs and implementations are far preferable to ingenious yet indecipherable ones.
The Natural Language Toolkit, or NLTK, was developed to give a broad range of students access to the core knowledge and skills of NLP (Loper and Bird, 2002). $$$$$ Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset.
The Natural Language Toolkit, or NLTK, was developed to give a broad range of students access to the core knowledge and skills of NLP (Loper and Bird, 2002). $$$$$ Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset.

Tokenization ,lemmatization, and stop word removal was performed using the Natural Language Toolkit (Loper and Bird, 2002). $$$$$ Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset.
Tokenization ,lemmatization, and stop word removal was performed using the Natural Language Toolkit (Loper and Bird, 2002). $$$$$ We are indebted to our students for feedback on the toolkit, and to anonymous reviewers, Jee Bang, and the workshop organizers for comments on an earlier version of this paper.
Tokenization ,lemmatization, and stop word removal was performed using the Natural Language Toolkit (Loper and Bird, 2002). $$$$$ Python is an object-oriented scripting language developed by Guido van Rossum and available on all platforms (www.python.org).
Tokenization ,lemmatization, and stop word removal was performed using the Natural Language Toolkit (Loper and Bird, 2002). $$$$$ NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware.

Systems like NLTK (Loper and Bird, 2002) and Gate (Cunningham, 2002) do not offer functionality for Lexical Resource Management. $$$$$ Examples of such books are: Using Computers in Linguistics (Lawler and Dry, 1998), and Programming for Linguistics: Java Technology for Language Researchers (Hammond, 2002).
Systems like NLTK (Loper and Bird, 2002) and Gate (Cunningham, 2002) do not offer functionality for Lexical Resource Management. $$$$$ Python offers a shallow learning curve; it was designed to be easily learnt by children (van Rossum, 1999).
Systems like NLTK (Loper and Bird, 2002) and Gate (Cunningham, 2002) do not offer functionality for Lexical Resource Management. $$$$$ We are currently working on NLTK modules for Hidden Markov Models, language modeling, and tree adjoining grammars.

To identify content words, we used the NLTK-Lite tagger to assign a part of speech to each word (Loper and Bird, 2002). $$$$$ This work has found widespread pedagogical application.
To identify content words, we used the NLTK-Lite tagger to assign a part of speech to each word (Loper and Bird, 2002). $$$$$ NLTK is an open source project, and we welcome any contributions.
To identify content words, we used the NLTK-Lite tagger to assign a part of speech to each word (Loper and Bird, 2002). $$$$$ Apart from the practical component, computational linguistics courses may also depend on software for in-class demonstrations.
To identify content words, we used the NLTK-Lite tagger to assign a part of speech to each word (Loper and Bird, 2002). $$$$$ In the simplest assignments, students experiment with an existing module.

For the NL processing, the Natural Language Toolkit (NL Toolkit or NLTK), developed at the University of Pennsylvania by Loper and Bird (2002), and available for download from Source Forge at http: //nltk.sourceforge.net/ was used. $$$$$ The remaining modules define data structures and interfaces for performing specific NLP tasks.
For the NL processing, the Natural Language Toolkit (NL Toolkit or NLTK), developed at the University of Pennsylvania by Loper and Bird (2002), and available for download from Source Forge at http: //nltk.sourceforge.net/ was used. $$$$$ NLTK can be used to create student assignments of varying difficulty and scope.
For the NL processing, the Natural Language Toolkit (NL Toolkit or NLTK), developed at the University of Pennsylvania by Loper and Bird (2002), and available for download from Source Forge at http: //nltk.sourceforge.net/ was used. $$$$$ The type checking module is used by all of the basic data types and processing classes.

 $$$$$ Students can also consult these reports if they would like further information about how the toolkit is designed, and why it is designed that way.
 $$$$$ We are grateful to Mitch Marcus and the Department of Computer and Information Science at the University of Pennsylvania for sponsoring the work reported here.

In the end I decided to require the students to learn python because I wanted to use NLTK, the Natural Language Toolkit (Loper and Bird, 2002). $$$$$ Comprehensiveness.
In the end I decided to require the students to learn python because I wanted to use NLTK, the Natural Language Toolkit (Loper and Bird, 2002). $$$$$ An unfortunate consequence is that a significant part of such courses must be devoted to teaching programming languages.
In the end I decided to require the students to learn python because I wanted to use NLTK, the Natural Language Toolkit (Loper and Bird, 2002). $$$$$ It is clear that these considerable overheads and shortcomings warrant a fresh approach.
In the end I decided to require the students to learn python because I wanted to use NLTK, the Natural Language Toolkit (Loper and Bird, 2002). $$$$$ NLTK is an open source project, and we welcome any contributions.

 $$$$$ The tagger module defines a standard interface for augmenting each token of a text with supplementary information, such as its part of speech or its WordNet synset tag; and provides several different implementations for this interface.
 $$$$$ For example, a course might use Prolog for parsing, Perl for corpus processing, and a finite-state toolkit for morphological analysis.
 $$$$$ The toolkit should use consistent data structures and interfaces.
 $$$$$ Recent work includes (Copestake, 2000; Baldridge et al., 2002a).

The Natural Language Toolkit (NLTK) was developed in conjunction with a computational linguistics course at the University of Pennsylvania in 2001 (Loper and Bird, 2002). $$$$$ This list of modules will grow over time, as we add new tasks and algorithms to the toolkit.
The Natural Language Toolkit (NLTK) was developed in conjunction with a computational linguistics course at the University of Pennsylvania in 2001 (Loper and Bird, 2002). $$$$$ We did encounter a few difficulties during the semester.
The Natural Language Toolkit (NLTK) was developed in conjunction with a computational linguistics course at the University of Pennsylvania in 2001 (Loper and Bird, 2002). $$$$$ Python offers a shallow learning curve; it was designed to be easily learnt by children (van Rossum, 1999).
The Natural Language Toolkit (NLTK) was developed in conjunction with a computational linguistics course at the University of Pennsylvania in 2001 (Loper and Bird, 2002). $$$$$ Finally, many of the modules included in the toolkit provide students with good examples of what projects should look like, with well thought-out interfaces, clean code structure, and thorough documentation.
