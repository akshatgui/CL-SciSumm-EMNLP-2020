Haghighi et al (2008) mention one disadvantage of using edit distance, that is, precision quickly degrades with higher recall. $$$$$ The feature vector of each word type is computed from the appropriate monolingual corpus and summarizes the word’s monolingual characteristics; see section 5 for details and figure 2 for an illustration.
Haghighi et al (2008) mention one disadvantage of using edit distance, that is, precision quickly degrades with higher recall. $$$$$ We have presented a generative model for bilingual lexicon induction based on probabilistic CCA.
Haghighi et al (2008) mention one disadvantage of using edit distance, that is, precision quickly degrades with higher recall. $$$$$ In particular, our method is robust to permutations of the sentences in the corpora.
Haghighi et al (2008) mention one disadvantage of using edit distance, that is, precision quickly degrades with higher recall. $$$$$ D and EN-FR-D, presumably due in part to the lack of orthographic features.

Haghighi et al (2008), amongst a few others, propose using canonical correlation analysis to reduce the dimension. $$$$$ Moreover, parallel text could be scarce for a language pair even if monolingual data is readily available for both languages.
Haghighi et al (2008), amongst a few others, propose using canonical correlation analysis to reduce the dimension. $$$$$ For each of the remaining unmatched source word types si which have not yet been generated, we draw the word type features from a baseline normal distribution with variance σ2IdS, with hyperparameter σ2 » 0; unmatched target words are similarly generated.
Haghighi et al (2008), amongst a few others, propose using canonical correlation analysis to reduce the dimension. $$$$$ Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings.

Haghighi et al (2008) only use a small-sized bilingual lexicon containing 100 word pairs as seed lexicon. $$$$$ We projected each source and target feature into the shared canonical space, and for each projected source feature we examined the closest projected target features.
Haghighi et al (2008) only use a small-sized bilingual lexicon containing 100 word pairs as seed lexicon. $$$$$ Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings.
Haghighi et al (2008) only use a small-sized bilingual lexicon containing 100 word pairs as seed lexicon. $$$$$ On the other hand, hard EM only requires us to compute the best matching under the current model:5 We cast this optimization as a maximum weighted bipartite matching problem as follows.

Examples can be found in Fung and Cheung (2004), followed by Haghighi et al (2008). $$$$$ In our method, we represent each language as a monolingual lexicon (see figure 2): a list of word types characterized by monolingual feature vectors, such as context counts, orthographic substrings, and so on (section 5).
Examples can be found in Fung and Cheung (2004), followed by Haghighi et al (2008). $$$$$ Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings.
Examples can be found in Fung and Cheung (2004), followed by Haghighi et al (2008). $$$$$ It remains to be seen how such lexicons can be best utilized, but they invite new approaches to the statistical translation of resource-poor languages.
Examples can be found in Fung and Cheung (2004), followed by Haghighi et al (2008). $$$$$ We ran the intersected HMM wordalignment model (Liang et al., 2008) and added (s, t) to the lexicon if s was aligned to t at least three times and more than any other word.

Haghighi et al (2008) have reported that the most common errors detected in their analysis on top 100 errors were from semantically related words, which had strong context feature correlations. $$$$$ We present a method for learning bilingual translation lexicons from monolingual corpora.
Haghighi et al (2008) have reported that the most common errors detected in their analysis on top 100 errors were from semantically related words, which had strong context feature correlations. $$$$$ However, MCCA still achieved surprising precision at lower recall levels.
Haghighi et al (2008) have reported that the most common errors detected in their analysis on top 100 errors were from semantically related words, which had strong context feature correlations. $$$$$ For each of the remaining unmatched source word types si which have not yet been generated, we draw the word type features from a baseline normal distribution with variance σ2IdS, with hyperparameter σ2 » 0; unmatched target words are similarly generated.
Haghighi et al (2008) have reported that the most common errors detected in their analysis on top 100 errors were from semantically related words, which had strong context feature correlations. $$$$$ We ran the intersected HMM wordalignment model (Liang et al., 2008) and added (s, t) to the lexicon if s was aligned to t at least three times and more than any other word.

Earlier work, Haghighi et al (2008), proposed a method for inducing bilingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with. $$$$$ Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings.
Earlier work, Haghighi et al (2008), proposed a method for inducing bilingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with. $$$$$ Our model is based on canonical correlation analysis (CCA)1 and explains matched word pairs via vectors in a common latent space.
Earlier work, Haghighi et al (2008), proposed a method for inducing bilingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with. $$$$$ However, if a source word type is not a translation of any of the target word types, we can just generate it independently without requiring it to participate in the matching.
Earlier work, Haghighi et al (2008), proposed a method for inducing bilingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with. $$$$$ Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings.

CCA has been used in bilingual lexicon extraction from comparable corpora (Gaussier et al, 2004) and monolingual corpora (Haghighi et al., 2008). $$$$$ In particular, we compute the optimal full matching, but only retain the highest weighted edges.
CCA has been used in bilingual lexicon extraction from comparable corpora (Gaussier et al, 2004) and monolingual corpora (Haghighi et al., 2008). $$$$$ We also explored the relationships our model learns between features of different languages.
CCA has been used in bilingual lexicon extraction from comparable corpora (Gaussier et al, 2004) and monolingual corpora (Haghighi et al., 2008). $$$$$ Figure 3 shows the highest-confidence outputs in several languages.

Haghighi et al (2008) use a probabilistic model over word feature vectors containing co occurrence and orthographic features. $$$$$ Based on S and T, our goal is to output a matching m between s and t. We represent m as a set of integer pairs so that (i, j) E m if and only if si is matched with tj.
Haghighi et al (2008) use a probabilistic model over word feature vectors containing co occurrence and orthographic features. $$$$$ If two word types are truly translations, it will be better to relate their feature vectors through the latent space than to explain them independently via the baseline distribution.
Haghighi et al (2008) use a probabilistic model over word feature vectors containing co occurrence and orthographic features. $$$$$ Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings.
Haghighi et al (2008) use a probabilistic model over word feature vectors containing co occurrence and orthographic features. $$$$$ First, we generate a matching m E M, where M is the set of matchings in which each word type is matched to at most one other word type.2 We take MATCHING-PRIOR to be uniform over M.3 Then, for each matched pair of word types (i, j) E m, we need to generate the observed feature vectors of the source and target word types, fS(si) E RdS and fT (tj) E RdT .

 $$$$$ The features used in each representation are defined identically and derived only from the appropriate monolingual corpora.
 $$$$$ Since si and tj are translations of each other, we expect fS(si) and fT(tj) to be connected somehow by the generative process.
 $$$$$ In table 5(a), we present some of the orthographic feature relationships learned by our system.
 $$$$$ As input, we are given a monolingual corpus S (a sequence of word tokens) in a source language and a monolingual corpus T in a target language.

The availability of parsers is a more stringent constraint, but our results suggest that more basic NLP methods may be sufficient for bilingual lexicon extraction. In this work, we have used a set of seed translations (unlike e.g., Haghighi et al (2008)). $$$$$ Specifically, to generate the feature vectors, we first generate a random concept zi,j — N(0, Id), where Id is the d x d identity matrix.
The availability of parsers is a more stringent constraint, but our results suggest that more basic NLP methods may be sufficient for bilingual lexicon extraction. In this work, we have used a set of seed translations (unlike e.g., Haghighi et al (2008)). $$$$$ Our experiments show that high-precision translations can be mined without any access to parallel corpora.
The availability of parsers is a more stringent constraint, but our results suggest that more basic NLP methods may be sufficient for bilingual lexicon extraction. In this work, we have used a set of seed translations (unlike e.g., Haghighi et al (2008)). $$$$$ As we discuss in section 7, our extracted lexicons have low coverage, particularly for proper nouns, and thus all performance measures are (sometimes substantially) pessimistic.

For example, the paper by Haghighi et al (2008) (which demonstrates how orthography and contextual information can be successfully used) reports 61.7% accuracy on the 186 most confident predictions of nouns. $$$$$ Based on S and T, our goal is to output a matching m between s and t. We represent m as a set of integer pairs so that (i, j) E m if and only if si is matched with tj.
For example, the paper by Haghighi et al (2008) (which demonstrates how orthography and contextual information can be successfully used) reports 61.7% accuracy on the 186 most confident predictions of nouns. $$$$$ In this section, we broadly characterize and analyze the behavior of our system.
For example, the paper by Haghighi et al (2008) (which demonstrates how orthography and contextual information can be successfully used) reports 61.7% accuracy on the 186 most confident predictions of nouns. $$$$$ As might be expected, the task is harder when no seed lexicon is provided, when the languages are strongly divergent, or when the monolingual corpora are from different domains.
For example, the paper by Haghighi et al (2008) (which demonstrates how orthography and contextual information can be successfully used) reports 61.7% accuracy on the 186 most confident predictions of nouns. $$$$$ As input, we are given a monolingual corpus S (a sequence of word tokens) in a source language and a monolingual corpus T in a target language.

Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al, 2008), we are able to find translations for otherwise OOV terms. $$$$$ Indeed, running our MCCA model with only orthographic features on EN-ESW, labeled ORTHO in table 1, yielded 80.1 p0.33, a 31% error-reduction over EDITDIST in p0.33.
Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al, 2008), we are able to find translations for otherwise OOV terms. $$$$$ Our experiments show that high-precision translations can be mined without any access to parallel corpora.
Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al, 2008), we are able to find translations for otherwise OOV terms. $$$$$ Let s = (s1, ... , snS) denote nS word types appearing in the source language, and t = (t1, ... , tnT) denote word types in the target language.
Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al, 2008), we are able to find translations for otherwise OOV terms. $$$$$ Recall that f�(·) and fT (·) map source and target word types to vectors in RdS and RdT, respectively (see section 2).

Our dictionary mining approach is based on Canonical Correlation Analysis, as used previously by (Haghighi et al, 2008). $$$$$ Our experiments show that high-precision translations can be mined without any access to parallel corpora.
Our dictionary mining approach is based on Canonical Correlation Analysis, as used previously by (Haghighi et al, 2008). $$$$$ Based on S and T, our goal is to output a matching m between s and t. We represent m as a set of integer pairs so that (i, j) E m if and only if si is matched with tj.
Our dictionary mining approach is based on Canonical Correlation Analysis, as used previously by (Haghighi et al, 2008). $$$$$ It remains to be seen how such lexicons can be best utilized, but they invite new approaches to the statistical translation of resource-poor languages.
Our dictionary mining approach is based on Canonical Correlation Analysis, as used previously by (Haghighi et al, 2008). $$$$$ In table 5(b), we present context feature correspondences.

Here we describe the use of CCA to find the translations for the OOV German words (Haghighi et al, 2008). $$$$$ This task, though clearly more difficult than the standard parallel text approach, can operate on language pairs and in domains where standard approaches cannot.
Here we describe the use of CCA to find the translations for the OOV German words (Haghighi et al, 2008). $$$$$ Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings.
Here we describe the use of CCA to find the translations for the OOV German words (Haghighi et al, 2008). $$$$$ In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995).
Here we describe the use of CCA to find the translations for the OOV German words (Haghighi et al, 2008). $$$$$ The first is to use the evaluation lexicon Le and select the hundred most common noun word types in the source corpus which have translations in Le.

We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al, 2008). $$$$$ Here, the broad trend is for words which are either translations or semantically related across languages to be close in canonical space.
We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al, 2008). $$$$$ Based on S and T, our goal is to output a matching m between s and t. We represent m as a set of integer pairs so that (i, j) E m if and only if si is matched with tj.
We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al, 2008). $$$$$ Section 6.2 compares the performance of these two methods.
We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al, 2008). $$$$$ Each experiment requires a lexicon for evaluation.

We evaluate sets of high-probability words in each topic and multilingual "synsets" by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al (2008). $$$$$ We have presented a generative model for bilingual lexicon induction based on probabilistic CCA.
We evaluate sets of high-probability words in each topic and multilingual "synsets" by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al (2008). $$$$$ Let s = (s1, ... , snS) denote nS word types appearing in the source language, and t = (t1, ... , tnT) denote word types in the target language.
We evaluate sets of high-probability words in each topic and multilingual "synsets" by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al (2008). $$$$$ All reported numbers exclude evaluation on the seed lexicon entries, regardless of how those seeds are derived or whether they are correct.
We evaluate sets of high-probability words in each topic and multilingual "synsets" by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al (2008). $$$$$ We present a method for learning bilingual translation lexicons from monolingual corpora.

Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al (2008)). $$$$$ We ran the intersected HMM wordalignment model (Liang et al., 2008) and added (s, t) to the lexicon if s was aligned to t at least three times and more than any other word.
Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al (2008)). $$$$$ However, if a source word type is not a translation of any of the target word types, we can just generate it independently without requiring it to participate in the matching.
Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al (2008)). $$$$$ In this section, we broadly characterize and analyze the behavior of our system.
Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al (2008)). $$$$$ Here, the broad trend is for words which are either translations or semantically related across languages to be close in canonical space.

Haghighi et al (2008) also used this method to show how well translations could be learned from monolingual corpora under ideal conditions, where the contextual and temporal distribution of words in the two monolingual corpora are nearly identical. $$$$$ The source feature vector fS(si) is drawn from a multivariate Gaussian with mean WSzi,j and covariance FS, where WS is a dS x d matrix which transforms the language-independent concept zi,j into a languagedependent vector in the source space.
Haghighi et al (2008) also used this method to show how well translations could be learned from monolingual corpora under ideal conditions, where the contextual and temporal distribution of words in the two monolingual corpora are nearly identical. $$$$$ Indeed, running our MCCA model with only orthographic features on EN-ESW, labeled ORTHO in table 1, yielded 80.1 p0.33, a 31% error-reduction over EDITDIST in p0.33.
Haghighi et al (2008) also used this method to show how well translations could be learned from monolingual corpora under ideal conditions, where the contextual and temporal distribution of words in the two monolingual corpora are nearly identical. $$$$$ However, MCCA still achieved surprising precision at lower recall levels.
Haghighi et al (2008) also used this method to show how well translations could be learned from monolingual corpora under ideal conditions, where the contextual and temporal distribution of words in the two monolingual corpora are nearly identical. $$$$$ We have presented a generative model for bilingual lexicon induction based on probabilistic CCA.

Haghighi et al, (2008) made use of contextual and orthographic clues for learning a generative model from monolingual corpora and a seed lexicon. $$$$$ We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.
Haghighi et al, (2008) made use of contextual and orthographic clues for learning a generative model from monolingual corpora and a seed lexicon. $$$$$ Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings.
Haghighi et al, (2008) made use of contextual and orthographic clues for learning a generative model from monolingual corpora and a seed lexicon. $$$$$ In table 5(a), we present some of the orthographic feature relationships learned by our system.
Haghighi et al, (2008) made use of contextual and orthographic clues for learning a generative model from monolingual corpora and a seed lexicon. $$$$$ Let s = (s1, ... , snS) denote nS word types appearing in the source language, and t = (t1, ... , tnT) denote word types in the target language.

Haghighi et al (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. $$$$$ We have presented a generative model for bilingual lexicon induction based on probabilistic CCA.
Haghighi et al (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. $$$$$ Our experiments show that high-precision translations can be mined without any access to parallel corpora.
Haghighi et al (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. $$$$$ Let s = (s1, ... , snS) denote nS word types appearing in the source language, and t = (t1, ... , tnT) denote word types in the target language.
Haghighi et al (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. $$$$$ We projected each source and target feature into the shared canonical space, and for each projected source feature we examined the closest projected target features.
