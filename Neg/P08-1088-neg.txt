Haghighi et al (2008) mention one disadvantage of using edit distance, that is, precision quickly degrades with higher recall. $$$$$ As input, we are given a monolingual corpus S (a sequence of word tokens) in a source language and a monolingual corpus T in a target language.
Haghighi et al (2008) mention one disadvantage of using edit distance, that is, precision quickly degrades with higher recall. $$$$$ All reported numbers exclude evaluation on the seed lexicon entries, regardless of how those seeds are derived or whether they are correct.
Haghighi et al (2008) mention one disadvantage of using edit distance, that is, precision quickly degrades with higher recall. $$$$$ We found that it was helpful to explicitly control the number of edges.
Haghighi et al (2008) mention one disadvantage of using edit distance, that is, precision quickly degrades with higher recall. $$$$$ For EnglishArabic, we extract a lexicon from 100k parallel sentences of UN parallel corpora by running the HMM intersected alignment model (Liang et al., 2008), adding (s, t) to the lexicon if s was aligned to t at least three times and more than any other word.

Haghighi et al (2008), amongst a few others, propose using canonical correlation analysis to reduce the dimension. $$$$$ For the remainder of this work, we will use MCCA to refer to our model using both orthographic and context features.
Haghighi et al (2008), amongst a few others, propose using canonical correlation analysis to reduce the dimension. $$$$$ We have presented a generative model for bilingual lexicon induction based on probabilistic CCA.
Haghighi et al (2008), amongst a few others, propose using canonical correlation analysis to reduce the dimension. $$$$$ We projected each source and target feature into the shared canonical space, and for each projected source feature we examined the closest projected target features.
Haghighi et al (2008), amongst a few others, propose using canonical correlation analysis to reduce the dimension. $$$$$ Evaluating against the union of these lexicons yielded 98.0 p0.33, a significant improvement over the 92.3 using only the Wiktionary lexicon.

Haghighi et al (2008) only use a small-sized bilingual lexicon containing 100 word pairs as seed lexicon. $$$$$ We also explored how system performance varies for language pairs other than English-Spanish.
Haghighi et al (2008) only use a small-sized bilingual lexicon containing 100 word pairs as seed lexicon. $$$$$ As input, we are given a monolingual corpus S (a sequence of word tokens) in a source language and a monolingual corpus T in a target language.
Haghighi et al (2008) only use a small-sized bilingual lexicon containing 100 word pairs as seed lexicon. $$$$$ We also explored how system performance varies for language pairs other than English-Spanish.
Haghighi et al (2008) only use a small-sized bilingual lexicon containing 100 word pairs as seed lexicon. $$$$$ We present a method for learning bilingual translation lexicons from monolingual corpora.

Examples can be found in Fung and Cheung (2004), followed by Haghighi et al (2008). $$$$$ If too few word types are matched, learning will not progress quickly; if too many are matched, the model will be swamped with noise.
Examples can be found in Fung and Cheung (2004), followed by Haghighi et al (2008). $$$$$ The first is to use the evaluation lexicon Le and select the hundred most common noun word types in the source corpus which have translations in Le.
Examples can be found in Fung and Cheung (2004), followed by Haghighi et al (2008). $$$$$ Intuitively, CCA finds d-dimensional subspaces US E RdS×d of the source and UT E RdT ×d of the target such that the components of the projections U>S fS(si) and U>T fT(tj) are maximally correlated.4 US and UT can be found by solving an eigenvalue problem (see Hardoon et al. (2003) for details).
Examples can be found in Fung and Cheung (2004), followed by Haghighi et al (2008). $$$$$ Indeed, running our MCCA model with only orthographic features on EN-ESW, labeled ORTHO in table 1, yielded 80.1 p0.33, a 31% error-reduction over EDITDIST in p0.33.

Haghighi et al (2008) have reported that the most common errors detected in their analysis on top 100 errors were from semantically related words, which had strong context feature correlations. $$$$$ In all experiments, unless noted otherwise, we used a seed of size 100 obtained from Le and considered lexicons between the top n = 2,000 most frequent source and target noun word types which were not in the seed lexicon; each system proposed an already-ranked one-to-one translation lexicon amongst these n words.
Haghighi et al (2008) have reported that the most common errors detected in their analysis on top 100 errors were from semantically related words, which had strong context feature correlations. $$$$$ Running EDITDIST (see section 4.3) on ENES-W yielded 61.1 p0.33, but precision quickly degrades for higher recall levels (see EDITDIST in table 1).
Haghighi et al (2008) have reported that the most common errors detected in their analysis on top 100 errors were from semantically related words, which had strong context feature correlations. $$$$$ The source feature vector fS(si) is drawn from a multivariate Gaussian with mean WSzi,j and covariance FS, where WS is a dS x d matrix which transforms the language-independent concept zi,j into a languagedependent vector in the source space.
Haghighi et al (2008) have reported that the most common errors detected in their analysis on top 100 errors were from semantically related words, which had strong context feature correlations. $$$$$ All reported numbers exclude evaluation on the seed lexicon entries, regardless of how those seeds are derived or whether they are correct.

Earlier work, Haghighi et al (2008), proposed a method for inducing bilingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with. $$$$$ We have presented a novel generative model for bilingual lexicon induction and presented results under a variety of data conditions (section 6.1) and languages (section 6.3) showing that our system can produce accurate lexicons even in highly adverse conditions.
Earlier work, Haghighi et al (2008), proposed a method for inducing bilingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with. $$$$$ One concern is how our system performs on language pairs where orthographic features are less applicable.
Earlier work, Haghighi et al (2008), proposed a method for inducing bilingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with. $$$$$ In this paper, we consider the problem of learning translations from monolingual sources alone.

CCA has been used in bilingual lexicon extraction from comparable corpora (Gaussier et al, 2004) and monolingual corpora (Haghighi et al., 2008). $$$$$ As input, we are given a monolingual corpus S (a sequence of word tokens) in a source language and a monolingual corpus T in a target language.
CCA has been used in bilingual lexicon extraction from comparable corpora (Gaussier et al, 2004) and monolingual corpora (Haghighi et al., 2008). $$$$$ Evaluating against the union of these lexicons yielded 98.0 p0.33, a significant improvement over the 92.3 using only the Wiktionary lexicon.
CCA has been used in bilingual lexicon extraction from comparable corpora (Gaussier et al, 2004) and monolingual corpora (Haghighi et al., 2008). $$$$$ As input, we are given a monolingual corpus S (a sequence of word tokens) in a source language and a monolingual corpus T in a target language.
CCA has been used in bilingual lexicon extraction from comparable corpora (Gaussier et al, 2004) and monolingual corpora (Haghighi et al., 2008). $$$$$ Evaluating against the union of these lexicons yielded 98.0 p0.33, a significant improvement over the 92.3 using only the Wiktionary lexicon.

Haghighi et al (2008) use a probabilistic model over word feature vectors containing co occurrence and orthographic features. $$$$$ In this section, we broadly characterize and analyze the behavior of our system.
Haghighi et al (2008) use a probabilistic model over word feature vectors containing co occurrence and orthographic features. $$$$$ As input, we are given a monolingual corpus S (a sequence of word tokens) in a source language and a monolingual corpus T in a target language.
Haghighi et al (2008) use a probabilistic model over word feature vectors containing co occurrence and orthographic features. $$$$$ Our experiments show that high-precision translations can be mined without any access to parallel corpora.
Haghighi et al (2008) use a probabilistic model over word feature vectors containing co occurrence and orthographic features. $$$$$ For closely related languages, such as English and Spanish, translation pairs often share many orthographic features.

 $$$$$ One direct way to capture orthographic similarity between word pairs is edit distance.
 $$$$$ Section 6.2 compares the performance of these two methods.
 $$$$$ In this section, we broadly characterize and analyze the behavior of our system.
 $$$$$ As we run EM, we gradually increase the number of edges to retain.

The availability of parsers is a more stringent constraint, but our results suggest that more basic NLP methods may be sufficient for bilingual lexicon extraction. In this work, we have used a set of seed translations (unlike e.g., Haghighi et al (2008)). $$$$$ We can represent orthographic features of a word type w by assigning a feature to each substring of length G 3.
The availability of parsers is a more stringent constraint, but our results suggest that more basic NLP methods may be sufficient for bilingual lexicon extraction. In this work, we have used a set of seed translations (unlike e.g., Haghighi et al (2008)). $$$$$ Given our probabilistic model, we would like to maximize the log-likelihood of the observed data with respect to the model parameters 0 = (WS, WT, `pS, &T).
The availability of parsers is a more stringent constraint, but our results suggest that more basic NLP methods may be sufficient for bilingual lexicon extraction. In this work, we have used a set of seed translations (unlike e.g., Haghighi et al (2008)). $$$$$ We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.
The availability of parsers is a more stringent constraint, but our results suggest that more basic NLP methods may be sufficient for bilingual lexicon extraction. In this work, we have used a set of seed translations (unlike e.g., Haghighi et al (2008)). $$$$$ The target fT (tj) is generated analogously using WT and FT, conditionally independent of the source given zi,j (see figure 2).

For example, the paper by Haghighi et al (2008) (which demonstrates how orthography and contextual information can be successfully used) reports 61.7% accuracy on the 186 most confident predictions of nouns. $$$$$ Then the maximum likelihood estimates are as follows: WS = CSSUSP1/2, WT = CTTUTP1/2, q'S =CSS − WSWS> , and 'PT = CTT − WTWT> , where P is a d x d diagonal matrix of the canonical correlations, CSS = |m |E(i,j)∈m fS(si)fS(si)> is the empirical covariance matrix in the source domain, and CTT is defined analogously.
For example, the paper by Haghighi et al (2008) (which demonstrates how orthography and contextual information can be successfully used) reports 61.7% accuracy on the 186 most confident predictions of nouns. $$$$$ We manually examined the top 100 errors in the English-Spanish lexicon produced by our system on EN-ES-W. Of the top 100 errors: 21 were correct translations not contained in the Wiktionary lexicon (e.g. pintura to painting), 4 were purely morphological errors (e.g. airport to aeropuertos), 30 were semantically related (e.g. basketball to b´eisbol), 15 were words with strong orthographic similarities (e.g. coast to costas), and 30 were difficult to categorize and fell into none of these categories.
For example, the paper by Haghighi et al (2008) (which demonstrates how orthography and contextual information can be successfully used) reports 61.7% accuracy on the 186 most confident predictions of nouns. $$$$$ We also explored the relationships our model learns between features of different languages.
For example, the paper by Haghighi et al (2008) (which demonstrates how orthography and contextual information can be successfully used) reports 61.7% accuracy on the 186 most confident predictions of nouns. $$$$$ In this paper, we consider the problem of learning translations from monolingual sources alone.

Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al, 2008), we are able to find translations for otherwise OOV terms. $$$$$ For EnglishArabic, we extract a lexicon from 100k parallel sentences of UN parallel corpora by running the HMM intersected alignment model (Liang et al., 2008), adding (s, t) to the lexicon if s was aligned to t at least three times and more than any other word.
Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al, 2008), we are able to find translations for otherwise OOV terms. $$$$$ The increased accuracy may not be an accident: whether two words are translations is perhaps better characterized directly by how close their latent concepts are, whereas log-probability is more sensitive to perturbations in the source and target spaces.
Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al, 2008), we are able to find translations for otherwise OOV terms. $$$$$ The arbitrary covariance parameter FS �: 0 explains the sourcespecific variations which are not captured by WS; it does not play an explicit role in inference.
Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al, 2008), we are able to find translations for otherwise OOV terms. $$$$$ In particular, our method is robust to permutations of the sentences in the corpora.

Our dictionary mining approach is based on Canonical Correlation Analysis, as used previously by (Haghighi et al, 2008). $$$$$ We have presented a generative model for bilingual lexicon induction based on probabilistic CCA.
Our dictionary mining approach is based on Canonical Correlation Analysis, as used previously by (Haghighi et al, 2008). $$$$$ In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995).
Our dictionary mining approach is based on Canonical Correlation Analysis, as used previously by (Haghighi et al, 2008). $$$$$ There are many sources from which we can derive monolingual corpora, and MCCA performance depends on the degree of similarity between corpora.
Our dictionary mining approach is based on Canonical Correlation Analysis, as used previously by (Haghighi et al, 2008). $$$$$ Based on S and T, our goal is to output a matching m between s and t. We represent m as a set of integer pairs so that (i, j) E m if and only if si is matched with tj.

Here we describe the use of CCA to find the translations for the OOV German words (Haghighi et al, 2008). $$$$$ In table 5(a), we present some of the orthographic feature relationships learned by our system.
Here we describe the use of CCA to find the translations for the OOV German words (Haghighi et al, 2008). $$$$$ Based on S and T, our goal is to output a matching m between s and t. We represent m as a set of integer pairs so that (i, j) E m if and only if si is matched with tj.
Here we describe the use of CCA to find the translations for the OOV German words (Haghighi et al, 2008). $$$$$ It remains to be seen how such lexicons can be best utilized, but they invite new approaches to the statistical translation of resource-poor languages.
Here we describe the use of CCA to find the translations for the OOV German words (Haghighi et al, 2008). $$$$$ For each of the remaining unmatched source word types si which have not yet been generated, we draw the word type features from a baseline normal distribution with variance σ2IdS, with hyperparameter σ2 » 0; unmatched target words are similarly generated.

We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al, 2008). $$$$$ Nonetheless, even in the more difficult cases, a sizable set of high-precision translations can be extracted.
We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al, 2008). $$$$$ In table 5(b), we present context feature correspondences.
We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al, 2008). $$$$$ Based on S and T, our goal is to output a matching m between s and t. We represent m as a set of integer pairs so that (i, j) E m if and only if si is matched with tj.
We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al, 2008). $$$$$ Using this automatically derived seed lexicon, we ran our system on EN-ESD as before, evaluating on the top 2,000 noun word types not included in the automatic lexicon.14 Using the automated seed lexicon, and still evaluating against our Wiktionary lexicon, MCCA-AUTO yielded 91.8 p0.33 (see table 2(b)), indicating that our system can produce lexicons of comparable accuracy with a heuristically chosen seed.

We evaluate sets of high-probability words in each topic and multilingual "synsets" by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al (2008). $$$$$ Current statistical machine translation systems use parallel corpora to induce translation correspondences, whether those correspondences be at the level of phrases (Koehn, 2004), treelets (Galley et al., 2006), or simply single words (Brown et al., 1994).
We evaluate sets of high-probability words in each topic and multilingual "synsets" by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al (2008). $$$$$ We also explored how system performance varies for language pairs other than English-Spanish.
We evaluate sets of high-probability words in each topic and multilingual "synsets" by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al (2008). $$$$$ Since many of our ‘errors’ actually represent valid translation pairs not contained in our extracted dictionary, we supplemented our evaluation lexicon with one automatically derived from 100k sentences of parallel Europarl data.
We evaluate sets of high-probability words in each topic and multilingual "synsets" by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al (2008). $$$$$ Our experiments show that high-precision translations can be mined without any access to parallel corpora.

Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al (2008)). $$$$$ The arbitrary covariance parameter FS �: 0 explains the sourcespecific variations which are not captured by WS; it does not play an explicit role in inference.
Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al (2008)). $$$$$ We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.
Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al (2008)). $$$$$ If a zero edge is present in the solution, we remove the involved word types from the matching.6 Bootstrapping Recall that the E-step produces a partial matching of the word types.
Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al (2008)). $$$$$ Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings.

Haghighi et al (2008) also used this method to show how well translations could be learned from monolingual corpora under ideal conditions, where the contextual and temporal distribution of words in the two monolingual corpora are nearly identical. $$$$$ We take as input two monolingual corpora and perhaps some seed translations, and we produce as output a bilingual lexicon, defined as a list of word pairs deemed to be word-level translations.
Haghighi et al (2008) also used this method to show how well translations could be learned from monolingual corpora under ideal conditions, where the contextual and temporal distribution of words in the two monolingual corpora are nearly identical. $$$$$ As might be expected, the task is harder when no seed lexicon is provided, when the languages are strongly divergent, or when the monolingual corpora are from different domains.
Haghighi et al (2008) also used this method to show how well translations could be learned from monolingual corpora under ideal conditions, where the contextual and temporal distribution of words in the two monolingual corpora are nearly identical. $$$$$ In this section, we explore feature representations of word types in our model.
Haghighi et al (2008) also used this method to show how well translations could be learned from monolingual corpora under ideal conditions, where the contextual and temporal distribution of words in the two monolingual corpora are nearly identical. $$$$$ Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings.

Haghighi et al, (2008) made use of contextual and orthographic clues for learning a generative model from monolingual corpora and a seed lexicon. $$$$$ We have presented a generative model for bilingual lexicon induction based on probabilistic CCA.
Haghighi et al, (2008) made use of contextual and orthographic clues for learning a generative model from monolingual corpora and a seed lexicon. $$$$$ While orthographic features are clearly effective for historically related language pairs, they are more limited for other language pairs, where we need to appeal to other clues.
Haghighi et al, (2008) made use of contextual and orthographic clues for learning a generative model from monolingual corpora and a seed lexicon. $$$$$ In table 5(b), we present context feature correspondences.
Haghighi et al, (2008) made use of contextual and orthographic clues for learning a generative model from monolingual corpora and a seed lexicon. $$$$$ Nonetheless, even in the more difficult cases, a sizable set of high-precision translations can be extracted.

Haghighi et al (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. $$$$$ First, we generate a matching m E M, where M is the set of matchings in which each word type is matched to at most one other word type.2 We take MATCHING-PRIOR to be uniform over M.3 Then, for each matched pair of word types (i, j) E m, we need to generate the observed feature vectors of the source and target word types, fS(si) E RdS and fT (tj) E RdT .
Haghighi et al (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. $$$$$ One direct way to capture orthographic similarity between word pairs is edit distance.
Haghighi et al (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. $$$$$ Also, as in Koehn and Knight (2002), we make use of a seed lexicon, which consists of a small, and perhaps incorrect, set of initial translation pairs.
Haghighi et al (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. $$$$$ It remains to be seen how such lexicons can be best utilized, but they invite new approaches to the statistical translation of resource-poor languages.
