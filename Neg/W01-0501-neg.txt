Similarly, in a case study on co-training for natural language tasks, Pierce and Cardie (2001) find that the degradation in data quality from automatic labelling prevents these systems from performing comparably to their fully-supervised counterparts. $$$$$ Cardie and Pierce (1998)), memory-based sequence learning (e.g.
Similarly, in a case study on co-training for natural language tasks, Pierce and Cardie (2001) find that the degradation in data quality from automatic labelling prevents these systems from performing comparably to their fully-supervised counterparts. $$$$$ Continuing the above example, web pages pointed to by my advisor links can be used to train the page classifier, while web pages about research interests and publications can be used to train the link classifier.
Similarly, in a case study on co-training for natural language tasks, Pierce and Cardie (2001) find that the degradation in data quality from automatic labelling prevents these systems from performing comparably to their fully-supervised counterparts. $$$$$ Second, many NLL problems require hundreds of thousands of training examples, while the web page task can be learned using hundreds of examples.

Pierce and Cardie (2001) have shown, however, that for tasks which require large numbers of labeled examples such as most NLP tasks co training might be inadequate because it tends to generate noisy data. $$$$$ This case study explored issues involved with applying co-training to the natural language processing task of identifying base noun phrases, particularly, the scalability of cotraining for large-scale problems.
Pierce and Cardie (2001) have shown, however, that for tasks which require large numbers of labeled examples such as most NLP tasks co training might be inadequate because it tends to generate noisy data. $$$$$ Figure 2b repeats the example sentence, showing the JOB tag representation.
Pierce and Cardie (2001) have shown, however, that for tasks which require large numbers of labeled examples such as most NLP tasks co training might be inadequate because it tends to generate noisy data. $$$$$ Thanks to three anonymous reviewers for their comments and suggestions.
Pierce and Cardie (2001) have shown, however, that for tasks which require large numbers of labeled examples such as most NLP tasks co training might be inadequate because it tends to generate noisy data. $$$$$ Our experiments indicate that co-training is an effective method for learning bracketers from small amounts of labeled data.

We observed the same pattern in co-training; its accuracy peaked after two iterations (85.1%) and then performance degraded drastically (68% after five iterations) due in part to an increase in mislabeled data in the training set (as previously observed in (Pierce and Cardie, 2001)) and in part because the data skew is not controlled for. $$$$$ This work was supported in part by DARPA TIDES contract N66001-00-C-8009, and NSF Grants 9454149, 0081334, and 0074896.
We observed the same pattern in co-training; its accuracy peaked after two iterations (85.1%) and then performance degraded drastically (68% after five iterations) due in part to an increase in mislabeled data in the training set (as previously observed in (Pierce and Cardie, 2001)) and in part because the data skew is not controlled for. $$$$$ First, it is impractical for the co-training classifiers to predict labels for each instance from the enormous set of unlabeled data.
We observed the same pattern in co-training; its accuracy peaked after two iterations (85.1%) and then performance degraded drastically (68% after five iterations) due in part to an increase in mislabeled data in the training set (as previously observed in (Pierce and Cardie, 2001)) and in part because the data skew is not controlled for. $$$$$ This tension suggests that combining weakly supervised learning methods with active learning methods might be a fruitful endeavor.

For naive co-training, new samples will always be added, and so there is a possibility that the noise accumulated at later stages will start to degrade performance (see Pierce and Cardie (2001)). $$$$$ Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between classifiers and supervised clastrained on a labeled version all available data.
For naive co-training, new samples will always be added, and so there is a possibility that the noise accumulated at later stages will start to degrade performance (see Pierce and Cardie (2001)). $$$$$ This work was supported in part by DARPA TIDES contract N66001-00-C-8009, and NSF Grants 9454149, 0081334, and 0074896.
For naive co-training, new samples will always be added, and so there is a possibility that the noise accumulated at later stages will start to degrade performance (see Pierce and Cardie (2001)). $$$$$ For the JOB instances (vectors of part-of-speech tags indexed from —k to k) a view corresponds to a subset of the set of indices {—k, , k} .

The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001). $$$$$ Our moderately supervised variant, corrected co-training, maintains labeled data quality without unduly increasing the burden on the human annotator.
The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001). $$$$$ Thanks to three anonymous reviewers for their comments and suggestions.
The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001). $$$$$ Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data.
The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001). $$$$$ This case study of co-training for natural language learning addresses the scalability question using the task of base noun phrase identification.

 $$$$$ Our moderately supervised variant, corrected co-training, maintains labeled data quality without unduly increasing the burden on the human annotator.
 $$$$$ On the other hand, the goal of active learning is to process (unlabeled) training examples in the order in which they are most useful or informative to the classifier (Cohn et al., 1994).
 $$$$$ This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels.
 $$$$$ In particular, many natural language learning tasks contrast sharply with the classification tasks previously studied in conjunction with co-training in that they require hundreds of thousands, rather than hundreds, of training examples.

The drop in F-measure is potentially due to the pollution of the labeled data by mislabeled instances (Pierce and Cardie, 2001). $$$$$ Blum and Mitchell (1998) presented a PAC-style analysis of co-training, introducing the concept of compatibility between the target function and the unlabeled data: that is, the target function should assign the same label to an instance regardless of which view it sees.
The drop in F-measure is potentially due to the pollution of the labeled data by mislabeled instances (Pierce and Cardie, 2001). $$$$$ However, degradation in the quality of the bootstrapped data arises as an obstacle to further improvement.
The drop in F-measure is potentially due to the pollution of the labeled data by mislabeled instances (Pierce and Cardie, 2001). $$$$$ Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data.
The drop in F-measure is potentially due to the pollution of the labeled data by mislabeled instances (Pierce and Cardie, 2001). $$$$$ Instead, a smaller data pool is maintained, fed with randomly selected instances from the larger set.3 Second, the JOB tagging task is a ternary, rather than a binary, classification.

However, it has been shown (Pierce and Cardie, 2001) that semi-supervised learning is brittle for NLP tasks where typically large amounts of high quality annotations are needed to train appropriate classifiers. $$$$$ Co-Training (Blum and Mitchell, 1998) is a weakly supervised paradigm for learning a classification task from a small set of labeled data and a large set of unlabeled data, using separate, but redundant, views of the data.
However, it has been shown (Pierce and Cardie, 2001) that semi-supervised learning is brittle for NLP tasks where typically large amounts of high quality annotations are needed to train appropriate classifiers. $$$$$ Surprisingly, the error rate of the view classifiers per iteration remains essentially unchanged despite the reduction of the pool of unlabeled examples to choose from.
However, it has been shown (Pierce and Cardie, 2001) that semi-supervised learning is brittle for NLP tasks where typically large amounts of high quality annotations are needed to train appropriate classifiers. $$$$$ The modified CT algorithm is presented in Figure 3.

The use of the disagreement between taggers for selecting candidates for manual correction is reminiscent of corrected co-training (PierceandCardie, 2001). $$$$$ For this task, co-training reduces by 36% the difference in error between classifiers trained on 500 labeled examples and classifiers trained on 211,000 labeled examples.
The use of the disagreement between taggers for selecting candidates for manual correction is reminiscent of corrected co-training (PierceandCardie, 2001). $$$$$ Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data.
The use of the disagreement between taggers for selecting candidates for manual correction is reminiscent of corrected co-training (PierceandCardie, 2001). $$$$$ We also record the accuracy of the growing body of labeled data.
The use of the disagreement between taggers for selecting candidates for manual correction is reminiscent of corrected co-training (PierceandCardie, 2001). $$$$$ We seek to apply the co-training paradigm to problems in natural language learning, with the goal of reducing the amount of humanannotated data required for developing natural language processing components.

For other work on co-training, see (Muslea et al 200; Pierce and Cardie 2001). $$$$$ Furthermore, our experiments support the hypothesis that labeled data quality is a crucial issue for co-training.
For other work on co-training, see (Muslea et al 200; Pierce and Cardie 2001). $$$$$ This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data.
For other work on co-training, see (Muslea et al 200; Pierce and Cardie 2001). $$$$$ Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between classifiers and supervised clastrained on a labeled version all available data.
For other work on co-training, see (Muslea et al 200; Pierce and Cardie 2001). $$$$$ A straightforward solution to this problem is to have a human anized, as co-training achieves 95.03% accuracy, just 0.14% away from the goal, after 600 iterations (and reaches 95.12% after 800 iterations).

The fact that no improvement was obtained agrees with previous observations that classifiers that are too accurate can not be improved with bootstrapping (Pierce and Cardie,2001). $$$$$ For large-scale learning tasks, the effectiveness of co-training may be dulled over time.
The fact that no improvement was obtained agrees with previous observations that classifiers that are too accurate can not be improved with bootstrapping (Pierce and Cardie,2001). $$$$$ This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels.
The fact that no improvement was obtained agrees with previous observations that classifiers that are too accurate can not be improved with bootstrapping (Pierce and Cardie,2001). $$$$$ This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data.

A comparative analysis of words that benefit from basic/smoothed co-training with global parameter settings, versus words with little or no improvement obtained through bootstrapping reveals several observations: (1) Words with accurate basic classifiers can not be improved through co-training, which agrees with previous observations (Pierce and Cardie, 2001). $$$$$ Cardie and Pierce (1998)), memory-based sequence learning (e.g.
A comparative analysis of words that benefit from basic/smoothed co-training with global parameter settings, versus words with little or no improvement obtained through bootstrapping reveals several observations: (1) Words with accurate basic classifiers can not be improved through co-training, which agrees with previous observations (Pierce and Cardie, 2001). $$$$$ Thanks to three anonymous reviewers for their comments and suggestions.
A comparative analysis of words that benefit from basic/smoothed co-training with global parameter settings, versus words with little or no improvement obtained through bootstrapping reveals several observations: (1) Words with accurate basic classifiers can not be improved through co-training, which agrees with previous observations (Pierce and Cardie, 2001). $$$$$ (The right context classifier behaves similarly to the left, but its performance is slightly worse.)
A comparative analysis of words that benefit from basic/smoothed co-training with global parameter settings, versus words with little or no improvement obtained through bootstrapping reveals several observations: (1) Words with accurate basic classifiers can not be improved through co-training, which agrees with previous observations (Pierce and Cardie, 2001). $$$$$ Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between classifiers and supervised clastrained on a labeled version all available data.

Algorithms such as co-training (Blum and Mitchell, 1998) (Collins and Singer, 1999) (Pierce and Cardie, 2001) and the Yarowsky algorithm (Yarowsky, 1995) make assumptions about the data that permit such an approach. $$$$$ Corrected co-training bridges the gap in accuracy between weak initial classifiers and fully supervised classifiers.
Algorithms such as co-training (Blum and Mitchell, 1998) (Collins and Singer, 1999) (Pierce and Cardie, 2001) and the Yarowsky algorithm (Yarowsky, 1995) make assumptions about the data that permit such an approach. $$$$$ They introduced an algorithm, CoBoost, that directly minimizes mismatch between views of the unlabeled data, using a combination of ideas from co-training and AdaBoost (Freund and Shapire, 1997).
Algorithms such as co-training (Blum and Mitchell, 1998) (Collins and Singer, 1999) (Pierce and Cardie, 2001) and the Yarowsky algorithm (Yarowsky, 1995) make assumptions about the data that permit such an approach. $$$$$ The next section of this paper introduces issues and concerns surrounding co-training.

The table further indicates that co-training for machine translation suffers the same problem reported in Pierce and Cardie (2001): gains above the accuracy of the initial corpus are achieved, but decline as after acer tain number of machine translations are added to the training set. $$$$$ This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels.
The table further indicates that co-training for machine translation suffers the same problem reported in Pierce and Cardie (2001): gains above the accuracy of the initial corpus are achieved, but decline as after acer tain number of machine translations are added to the training set. $$$$$ The most natural views are perhaps {—k, , Of and {0, , k}, indicating that one classifier looks at the focus tag and the tags to its left, while the other looks at the focus tag and the tags to its right.
The table further indicates that co-training for machine translation suffers the same problem reported in Pierce and Cardie (2001): gains above the accuracy of the initial corpus are achieved, but decline as after acer tain number of machine translations are added to the training set. $$$$$ (Figure 2a illustrates base NPs with a short example.)
The table further indicates that co-training for machine translation suffers the same problem reported in Pierce and Cardie (2001): gains above the accuracy of the initial corpus are achieved, but decline as after acer tain number of machine translations are added to the training set. $$$$$ Analysis of corrected co-training illuminates an interesting tension within weakly supervised learning, between the need to bootstrap accurate labeled data, and the need to cover the desired task.

Pierce and Cardie (2001) showed that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a high performing model from being learned. $$$$$ Corrected Co-Training.
Pierce and Cardie (2001) showed that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a high performing model from being learned. $$$$$ Words to the left and right of the focus word are included for context.
Pierce and Cardie (2001) showed that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a high performing model from being learned. $$$$$ Cardie and Pierce (1998)), memory-based sequence learning (e.g.
Pierce and Cardie (2001) showed that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a high performing model from being learned. $$$$$ In contrast to previous investigations of the theoretical basis of co-training, this study is motivated by practical concerns about the application of weakly supervised learning to problems in natural language learning (NLL).

To address the problem of data pollution by tagging errors, Pierce and Cardie (2001) propose corrected co-training. $$$$$ Our moderately supervised variant, corrected co-training, maintains labeled data quality without unduly increasing the burden on the human annotator.
To address the problem of data pollution by tagging errors, Pierce and Cardie (2001) propose corrected co-training. $$$$$ This work was supported in part by DARPA TIDES contract N66001-00-C-8009, and NSF Grants 9454149, 0081334, and 0074896.
To address the problem of data pollution by tagging errors, Pierce and Cardie (2001) propose corrected co-training. $$$$$ To apply co-training, the base NP classification task must first be factored into views.

Thus, these bootstrapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility (Pierce and Cardie, 2001). $$$$$ On one hand, the goal of weakly supervised learning is to bootstrap a classifier from small amounts of labeled data and large amounts of unlabeled data, often by automatically labeling some of the unlabeled data.
Thus, these bootstrapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility (Pierce and Cardie, 2001). $$$$$ Corrected co-training bridges the gap in accuracy between weak initial classifiers and fully supervised classifiers.
Thus, these bootstrapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility (Pierce and Cardie, 2001). $$$$$ Thanks to three anonymous reviewers for their comments and suggestions.
Thus, these bootstrapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility (Pierce and Cardie, 2001). $$$$$ These measurements can be plotted to depict a learning curve, indicating the progress of cotraining as the classifier accuracy changes.

 $$$$$ Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between classifiers and supervised clastrained on a labeled version all available data.
 $$$$$ Figure 2b repeats the example sentence, showing the JOB tag representation.
 $$$$$ Cardie et al. (2000)) systems.

In their experiments on NP identifiers, Pierce and Cardie (2001) observed a similar effect. $$$$$ However, degradation in the quality of the bootstrapped data arises as an obstacle to further improvement.
In their experiments on NP identifiers, Pierce and Cardie (2001) observed a similar effect. $$$$$ To address this, we propose a moderately supervised variant of cotraining in which a human corrects the mistakes made during automatic labeling.
In their experiments on NP identifiers, Pierce and Cardie (2001) observed a similar effect. $$$$$ Thus, by limiting the number of unlabeled examples under consideration with the hope of forcing broader task coverage we achieve essentially the goal accuracy in fewer iterations and with fewer corrections!
In their experiments on NP identifiers, Pierce and Cardie (2001) observed a similar effect. $$$$$ For an instance x, the classifier determines the maximum a posteriori label as follows.

Co-training has been applied to a number of NLP applications ,including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). $$$$$ The modified CT algorithm is presented in Figure 3.
Co-training has been applied to a number of NLP applications ,including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). $$$$$ Initial studies of co-training focused on the applicability of the co-training paradigm, and in particular, on clarifying the assumptions needed to ensure the effectiveness of the CT algorithm.
Co-training has been applied to a number of NLP applications ,including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). $$$$$ Thanks to three anonymous reviewers for their comments and suggestions.
Co-training has been applied to a number of NLP applications ,including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). $$$$$ Naturally, the resulting classifier does not perform as well as a fully supervised classifier trained on hundreds of times as much labeled data, but if the difference in accuracy is less important than the effort required to produce the labeled training data, co-training is especially attractive.
