Here, we follow an approach introduced by Koehn and Knight (2003) $$$$$ Given the count of words in the corpus, we pick the split S with the highest geometric mean of word frequencies of its parts pi (n being the number of parts): Since this metric is purely defined in terms of German word frequencies, there is not necessarily a relationship between the selected option and correspondence to English words.
Here, we follow an approach introduced by Koehn and Knight (2003) $$$$$ Compound splitting is a well defined computational linguistics task.
Here, we follow an approach introduced by Koehn and Knight (2003) $$$$$ The insertion of function words is not our concern.
Here, we follow an approach introduced by Koehn and Knight (2003) $$$$$ But if we consistently break up the word Aktion into Akt and Ion in our training data, such a system will likely learn the translation of the word pair Akt Ion into the single English word action.

Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and UrduHindi (Lehal, 2010). $$$$$ For this application, we could dramatically improve the translation quality by up to 0.039 points as measured by the BLEU score.
Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and UrduHindi (Lehal, 2010). $$$$$ We also allow each English word to be considered only once: If it is taken as evidence for correspondence to the first part of the compound, it is excluded as evidence for the other parts.
Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and UrduHindi (Lehal, 2010). $$$$$ The easiest way to obtain a translation lexicon is to learn it from a parallel corpus.
Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and UrduHindi (Lehal, 2010). $$$$$ A typical error of the method presented so far is that prefixes and suffixes are often split off.

Correctsplitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). $$$$$ In the following we discuss methods that pick one of them as the correct splitting of the compound.
Correctsplitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). $$$$$ On the right side is the geometric mean score of these frequencies.
Correctsplitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). $$$$$ The words resulting from compound splitting could also be marked as such, and not just treated as regular words, as they are now.
Correctsplitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). $$$$$ As fillers we allow s and es when splitting German words, which covers almost all cases.

Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. $$$$$ We introduce methods to learn splitting rules from monolingual and parallel corpora.
Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. $$$$$ Our experimental results demonstrate that what constitutes the optimal splitting depends on the intended application.
Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. $$$$$ We evaluate them against a gold standard and measure their impact on performance of statistical MT systems.
Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. $$$$$ It is limited to breaking compounds into cognates and words found in a translation lexicon.

We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003). $$$$$ The task of deciding the granularity of good splits is deferred to the phrase based SMT system, which uses a statistical method to group phrases and rejoin split words.
We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003). $$$$$ When making splitting decisions for new texts, we follow the most frequent option based on the splitting knowledge.
We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003). $$$$$ Since there are no simple rules for when such letters may be inserted we allow them between any two words.
We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003). $$$$$ Thus, to enumerate all possible splittings of a compound, we consider all splits into known words.

In order to reduce the source vocabulary size for the German-English translation, the source side was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003). $$$$$ Monz and de Rijke [2001] and Hedlund et al. [2001] successfully use lexicon based approaches to compound splitting for information retrieval.
In order to reduce the source vocabulary size for the German-English translation, the source side was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003). $$$$$ One splitting method may mistakenly break up the word Aktionsplan into the three words Akt, Ion, and Plan.
In order to reduce the source vocabulary size for the German-English translation, the source side was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003). $$$$$ First, we measured the impact on a word based statistical machine translation system, the widely studied IBM Model 4 [Brown et al., 1990], for which training tools [Al-Onaizan et al., 19991 and decoders [Germann et al., 2001] are freely available.
In order to reduce the source vocabulary size for the German-English translation, the source side was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003). $$$$$ It is limited to breaking compounds into cognates and words found in a translation lexicon.

We used the frequency-based segmentation algorithm initially introduced in (Koehn and Knight, 2003) to handle compounding. $$$$$ Ultimately, the purpose of this work is to improve the quality of machine translation systems.
We used the frequency-based segmentation algorithm initially introduced in (Koehn and Knight, 2003) to handle compounding. $$$$$ The ability to group split words into phrases overcomes the many mistakes of maximal (eager) splitting of words and outperforms the more accurate methods. tem with the different flavors of our training data, and evaluated the performance as before.
We used the frequency-based segmentation algorithm initially introduced in (Koehn and Knight, 2003) to handle compounding. $$$$$ To exclude these mistakes, we use information about the parts-of-speech of words.

To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. $$$$$ While the linguistic properties of compounds are widely studied [Langer, 1998], there has been only limited work on empirical methods to split up compounds for specific applications.
To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. $$$$$ We introduce methods to learn splitting rules from monolingual and parallel corpora.
To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. $$$$$ Future machine translation models that are sensitive to such linguistic clues might benefit even more.

The empirical approach of Koehn and Knight (2003) splits German compounds into words found in a training corpus. $$$$$ Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task.
The empirical approach of Koehn and Knight (2003) splits German compounds into words found in a training corpus. $$$$$ The words resulting from compound splitting could also be marked as such, and not just treated as regular words, as they are now.
The empirical approach of Koehn and Knight (2003) splits German compounds into words found in a training corpus. $$$$$ Compounds are broken into either the smallest or the biggest words that can be found in a given lexicon.
The empirical approach of Koehn and Knight (2003) splits German compounds into words found in a training corpus. $$$$$ Our experimental results demonstrate that what constitutes the optimal splitting depends on the intended application.

Popovic et al. (2006) compared the approach of Nie? en and Ney (2000) with the corpus-driven splitting of Koehn and Knight (2003) in terms of performance on an SMT task. $$$$$ Compounded words are a challenge for NLP applications such as machine translation (MT).
Popovic et al. (2006) compared the approach of Nie? en and Ney (2000) with the corpus-driven splitting of Koehn and Knight (2003) in terms of performance on an SMT task. $$$$$ Such a translation only occurs when Grund is used as the first part of a compound.
Popovic et al. (2006) compared the approach of Nie? en and Ney (2000) with the corpus-driven splitting of Koehn and Knight (2003) in terms of performance on an SMT task. $$$$$ To summarize: We try to cover the entire length of the compound with known words and fillers between words.
Popovic et al. (2006) compared the approach of Nie? en and Ney (2000) with the corpus-driven splitting of Koehn and Knight (2003) in terms of performance on an SMT task. $$$$$ For each German NP/PP, we have a English translation.

We briefly introduce the computational morphology SMOR (section 3.1) and the corpus driven approach of Koehn and Knight (2003) (section 3.2), before we present our hybrid approach that combines the benefits of both in section 3.3. $$$$$ This poses challenges for a number of NLP applications such as machine translation, speech recognition, text classification, information extraction, or information retrieval.
We briefly introduce the computational morphology SMOR (section 3.1) and the corpus driven approach of Koehn and Knight (2003) (section 3.2), before we present our hybrid approach that combines the benefits of both in section 3.3. $$$$$ Future machine translation models that are sensitive to such linguistic clues might benefit even more.
We briefly introduce the computational morphology SMOR (section 3.1) and the corpus driven approach of Koehn and Knight (2003) (section 3.2), before we present our hybrid approach that combines the benefits of both in section 3.3. $$$$$ Following good engineering practice, the methods have been developed with a different development test set.
We briefly introduce the computational morphology SMOR (section 3.1) and the corpus driven approach of Koehn and Knight (2003) (section 3.2), before we present our hybrid approach that combines the benefits of both in section 3.3. $$$$$ Monz and de Rijke [2001] and Hedlund et al. [2001] successfully use lexicon based approaches to compound splitting for information retrieval.

Koehn and Knight (2003) describe a method requiring no linguistically motivated morphological analysis to split compounds. $$$$$ Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task.
Koehn and Knight (2003) describe a method requiring no linguistically motivated morphological analysis to split compounds. $$$$$ Another example for this is the word Voraussetzung (English: condition), which is split into vor and aussetzung.
Koehn and Knight (2003) describe a method requiring no linguistically motivated morphological analysis to split compounds. $$$$$ Compound splitting is a well defined computational linguistics task.
Koehn and Knight (2003) describe a method requiring no linguistically motivated morphological analysis to split compounds. $$$$$ Known words are words that exist in a training corpus, in our case the European parliament proceedings consisting of 20 million words of German [Koehn, 2002].

Taken from (Koehn and Knight, 2003) $$$$$ One splitting method may mistakenly break up the word Aktionsplan into the three words Akt, Ion, and Plan.
Taken from (Koehn and Knight, 2003) $$$$$ The more frequent a word occurs in a training corpus, the bigger the statistical basis to estimate translation probabilities, and the more likely the correct translation probability distribution is learned [Koehn and Knight, 20011.
Taken from (Koehn and Knight, 2003) $$$$$ We introduced various methods to split compound words into parts.

The one-to-one correspondence gold standard (Koehn and Knight, 2003) indicates only compounds that were translated compositionally by a human translator. $$$$$ We then obtain statistics on the parts-ofspeech of words in the corpus.
The one-to-one correspondence gold standard (Koehn and Knight, 2003) indicates only compounds that were translated compositionally by a human translator. $$$$$ One way to define the goal of compound splitting is to break up foreign words, so that a one-to-one correspondence to English can be established.
The one-to-one correspondence gold standard (Koehn and Knight, 2003) indicates only compounds that were translated compositionally by a human translator. $$$$$ Training and testing data was split consistently in the same way.

 $$$$$ Ultimately, the purpose of this work is to improve the quality of machine translation systems.
 $$$$$ We evaluate them against a gold standard and measure their impact on performance of statistical MT systems.
 $$$$$ Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task.
 $$$$$ Breaking up this compound would assist the translation into English as action plan.

 $$$$$ We evaluate them against a gold standard and measure their impact on performance of statistical MT systems.
 $$$$$ Again, the four options: Behind each part, we indicated its frequency in parenthesis.
 $$$$$ To account for this, we build a second translation lexicon as follows: First, we break up German words in the parallel corpus with the frequency method.
 $$$$$ Larson et al. [2000] propose a data-driven method that combines compound splitting and word recombination for speech recognition.

Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words. $$$$$ Its main remaining source of error is the lack of training data.
Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words. $$$$$ The training set for the experiments is a corpus of 650,000 noun phrases and prepositional phrases (NP/PP).
Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words. $$$$$ While this is nonsensical, it is easy to explain: The word the is commonly found in English sentences, and therefore taken as evidence for the existence of a translation for den.
Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words. $$$$$ Since there are no simple rules for when such letters may be inserted we allow them between any two words.

 $$$$$ The score for the unbroken compound (852) is higher than the preferred choice (825.6).
 $$$$$ While one of our method reached 99.1% accuracy compared against a gold standard of one-to-one correspondences to English, other methods show superior results in the context of statistical machine translation.

Koehn and Knight (2003) used a fixed set of two known fillers s and es for handling German compounds. $$$$$ For the word Aktionsplan, we find the following splitting options: We arrive at these splitting options, since all the parts — aktionsplan, aktions, aktion, akt, ion, and plan — have been observed as whole words in the training corpus.
Koehn and Knight (2003) used a fixed set of two known fillers s and es for handling German compounds. $$$$$ To judge this, we manually annotated the test set with correct splits.
Koehn and Knight (2003) used a fixed set of two known fillers s and es for handling German compounds. $$$$$ We evaluate the performance of the described methods on a blind test set of 1000 NP/PPs, which contain 3498 words.
Koehn and Knight (2003) used a fixed set of two known fillers s and es for handling German compounds. $$$$$ While one of our method reached 99.1% accuracy compared against a gold standard of one-to-one correspondences to English, other methods show superior results in the context of statistical machine translation.

We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). $$$$$ If multiple options match the English, we select the one(s) with the most splits and use word frequencies as the ultimate tie-breaker.
We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). $$$$$ But if we consistently break up the word Aktion into Akt and Ion in our training data, such a system will likely learn the translation of the word pair Akt Ion into the single English word action.
