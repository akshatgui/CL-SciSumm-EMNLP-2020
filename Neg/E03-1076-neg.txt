Here, we follow an approach introduced by Koehn and Knight (2003): First, we collect frequency statistics over words in our training corpus. $$$$$ Monz and de Rijke [2001] and Hedlund et al. [2001] successfully use lexicon based approaches to compound splitting for information retrieval.
Here, we follow an approach introduced by Koehn and Knight (2003): First, we collect frequency statistics over words in our training corpus. $$$$$ We introduced various methods to split compound words into parts.
Here, we follow an approach introduced by Koehn and Knight (2003): First, we collect frequency statistics over words in our training corpus. $$$$$ Recall the example of Aktionsplan, where the letter s was inserted between Aktion and Plan.
Here, we follow an approach introduced by Koehn and Knight (2003): First, we collect frequency statistics over words in our training corpus. $$$$$ To exclude these mistakes, we use information about the parts-of-speech of words.

Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and UrduHindi (Lehal, 2010). $$$$$ Other transformations at joints include dropping of letters, such as when Schweigen and Minute are joined into Schweigeminute, dropping an n. A extensive study of such transformations is carried out by Langer [1998] for German.
Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and UrduHindi (Lehal, 2010). $$$$$ This allows us to exclude words based on their POS as possible parts of compounds.
Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and UrduHindi (Lehal, 2010). $$$$$ The more frequent a word occurs in a training corpus, the bigger the statistical basis to estimate translation probabilities, and the more likely the correct translation probability distribution is learned [Koehn and Knight, 20011.

Correctsplitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). $$$$$ For instance, the word folgenden (English: following) is broken off into folgen (English: consequences) and den (English: the).
Correctsplitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). $$$$$ On the right side is the geometric mean score of these frequencies.
Correctsplitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). $$$$$ Compounding of words is common in a number of languages (German, Dutch, Finnish, Greek, etc.).

Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. $$$$$ While this method works well for the examples Aktionsplan and Freitag, it failed in our experiments for words such as Grundrechte (English: basic rights).
Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. $$$$$ We trained this sysbased statistical machine translation system.
Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. $$$$$ Also, we restrict known words to words of at least length three.
Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. $$$$$ These are called Fugenelemente in German.

We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003). $$$$$ This limitation is purely for computational reasons, since we expect most compounds to be nouns.
We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003). $$$$$ We do not want to break up a compound into parts that are prepositions or determiners, but only content words: nouns, adverbs, adjectives, and verbs.
We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003). $$$$$ For machine translation, the splitting of an unknown compound into its parts enables the translation of the compound by the translation of its parts.
We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003). $$$$$ To summarize: We try to cover the entire length of the compound with known words and fillers between words.

In order to reduce the source vocabulary size for the German-English translation, the source side was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003). $$$$$ If multiple biggest splits are possible, the one with the highest frequency score is taken. frequency based: split into most frequent words, as described in Section 4 using parallel: split guided by splitting knowledge from a parallel corpus, as described in Section 5 using parallel and POS: as previous, with an additional restriction on the POS of split parts, as described in Section 6 Since we developed our methods to improve on this metric, it comes as no surprise that the most sophisticated method that employs splitting knowledge from a parallel corpus and information about POS tags proves to be superior with 99.1% accuracy.
In order to reduce the source vocabulary size for the German-English translation, the source side was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003). $$$$$ Morphological analyzers such as Morphix [Finkler and Neumann, 19981 usually provide a variety of splitting options and leave it to the subsequent application to pick the best choice.
In order to reduce the source vocabulary size for the German-English translation, the source side was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003). $$$$$ As fillers we allow s and es when splitting German words, which covers almost all cases.

We used the frequency-based segmentation algorithm initially introduced in (Koehn and Knight, 2003) to handle compounding. $$$$$ With this translation lexicon we can perform the method alluded to above: For each German word, we consider all splitting options.
We used the frequency-based segmentation algorithm initially introduced in (Koehn and Knight, 2003) to handle compounding. $$$$$ This insight leads us to define a splitting metric based on word frequency.
We used the frequency-based segmentation algorithm initially introduced in (Koehn and Knight, 2003) to handle compounding. $$$$$ We trained this sysbased statistical machine translation system.

To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. $$$$$ Compounded words are a challenge for NLP applications such as machine translation (MT).
To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. $$$$$ Our experimental results demonstrate that what constitutes the optimal splitting depends on the intended application.
To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. $$$$$ For each splitting option, we check if it has translations on the English side.
To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. $$$$$ Future machine translation models that are sensitive to such linguistic clues might benefit even more.

The empirical approach of Koehn and Knight (2003) splits German compounds into words found in a training corpus. $$$$$ One splitting method may mistakenly break up the word Aktionsplan into the three words Akt, Ion, and Plan.
The empirical approach of Koehn and Knight (2003) splits German compounds into words found in a training corpus. $$$$$ For machine translation, the splitting of an unknown compound into its parts enables the translation of the compound by the translation of its parts.
The empirical approach of Koehn and Knight (2003) splits German compounds into words found in a training corpus. $$$$$ The more frequent a word occurs in a training corpus, the bigger the statistical basis to estimate translation probabilities, and the more likely the correct translation probability distribution is learned [Koehn and Knight, 20011.

Popovic et al. (2006) compared the approach of Nie? en and Ney (2000) with the corpus-driven splitting of Koehn and Knight (2003) in terms of performance on an SMT task. $$$$$ An algorithm to break up words in such a manner could be implemented using dynamic programming, but since computational complexity is not a problem, we employ an exhaustive recursive search.
Popovic et al. (2006) compared the approach of Nie? en and Ney (2000) with the corpus-driven splitting of Koehn and Knight (2003) in terms of performance on an SMT task. $$$$$ This poses challenges for a number of NLP applications such as machine translation, speech recognition, text classification, information extraction, or information retrieval.
Popovic et al. (2006) compared the approach of Nie? en and Ney (2000) with the corpus-driven splitting of Koehn and Knight (2003) in terms of performance on an SMT task. $$$$$ Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task.
Popovic et al. (2006) compared the approach of Nie? en and Ney (2000) with the corpus-driven splitting of Koehn and Knight (2003) in terms of performance on an SMT task. $$$$$ Again, the four options: Behind each part, we indicated its frequency in parenthesis.

We briefly introduce the computational morphology SMOR (section 3.1) and the corpus driven approach of Koehn and Knight (2003) (section 3.2), before we present our hybrid approach that combines the benefits of both in section 3.3. $$$$$ While this method works well for the examples Aktionsplan and Freitag, it failed in our experiments for words such as Grundrechte (English: basic rights).
We briefly introduce the computational morphology SMOR (section 3.1) and the corpus driven approach of Koehn and Knight (2003) (section 3.2), before we present our hybrid approach that combines the benefits of both in section 3.3. $$$$$ By splitting all the words on the German side of the parallel corpus, we acquire a vast amount of splitting knowledge (for our data, this covers 75,055 different words).
We briefly introduce the computational morphology SMOR (section 3.1) and the corpus driven approach of Koehn and Knight (2003) (section 3.2), before we present our hybrid approach that combines the benefits of both in section 3.3. $$$$$ Future machine translation models that are sensitive to such linguistic clues might benefit even more.
We briefly introduce the computational morphology SMOR (section 3.1) and the corpus driven approach of Koehn and Knight (2003) (section 3.2), before we present our hybrid approach that combines the benefits of both in section 3.3. $$$$$ To summarize: We try to cover the entire length of the compound with known words and fillers between words.

Koehn and Knight (2003) describe a method requiring no linguistically motivated morphological analysis to split compounds. $$$$$ Our experimental results demonstrate that what constitutes the optimal splitting depends on the intended application.
Koehn and Knight (2003) describe a method requiring no linguistically motivated morphological analysis to split compounds. $$$$$ One splitting method may mistakenly break up the word Aktionsplan into the three words Akt, Ion, and Plan.
Koehn and Knight (2003) describe a method requiring no linguistically motivated morphological analysis to split compounds. $$$$$ Another example for this is the word Voraussetzung (English: condition), which is split into vor and aussetzung.
Koehn and Knight (2003) describe a method requiring no linguistically motivated morphological analysis to split compounds. $$$$$ Known words are words that exist in a training corpus, in our case the European parliament proceedings consisting of 20 million words of German [Koehn, 2002].

Taken from (Koehn and Knight, 2003): S= split, pi= part, n= number of parts. $$$$$ 7.3 Translation Quality with Phrase Based Machine Translation Compound words violate the bias for one-to-one word correspondences of word based SMT systems.
Taken from (Koehn and Knight, 2003): S= split, pi= part, n= number of parts. $$$$$ Compounds are broken into either the smallest or the biggest words that can be found in a given lexicon.
Taken from (Koehn and Knight, 2003): S= split, pi= part, n= number of parts. $$$$$ For this application, we could dramatically improve the translation quality by up to 0.039 points as measured by the BLEU score.
Taken from (Koehn and Knight, 2003): S= split, pi= part, n= number of parts. $$$$$ Its main remaining source of error is the lack of training data.

The one-to-one correspondence gold standard (Koehn and Knight, 2003) indicates only compounds that were translated compositionally by a human translator. $$$$$ This is one of the motivations for phrase based systems that translate groups of words.
The one-to-one correspondence gold standard (Koehn and Knight, 2003) indicates only compounds that were translated compositionally by a human translator. $$$$$ This insight leads us to define a splitting metric based on word frequency.
The one-to-one correspondence gold standard (Koehn and Knight, 2003) indicates only compounds that were translated compositionally by a human translator. $$$$$ Splitting options for the German word Aktionsplan Aktionsplan Aktion actionplan action plan Akt ion s plan act ion plan
The one-to-one correspondence gold standard (Koehn and Knight, 2003) indicates only compounds that were translated compositionally by a human translator. $$$$$ Since there are no simple rules for when such letters may be inserted we allow them between any two words.

 $$$$$ This limitation is purely for computational reasons, since we expect most compounds to be nouns.
 $$$$$ This would lead us to break up Aktionsplan, but not Freitag.
 $$$$$ Morphological analyzers such as Morphix [Finkler and Neumann, 19981 usually provide a variety of splitting options and leave it to the subsequent application to pick the best choice.
 $$$$$ To exclude these mistakes, we use information about the parts-of-speech of words.

 $$$$$ For machine translation, the splitting of an unknown compound into its parts enables the translation of the compound by the translation of its parts.
 $$$$$ In fact, this is the case for the example Aktionsplan.
 $$$$$ While one of our method reached 99.1% accuracy compared against a gold standard of one-to-one correspondences to English, other methods show superior results in the context of statistical machine translation.
 $$$$$ Also, we restrict known words to words of at least length three.

Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words. $$$$$ Splitting options for the German word Aktionsplan Aktionsplan Aktion actionplan action plan Akt ion s plan act ion plan
Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words. $$$$$ Since words may be joined freely, this vastly increases the vocabulary size, leading to sparse data problems.
Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words. $$$$$ Our experimental results demonstrate that what constitutes the optimal splitting depends on the intended application.
Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words. $$$$$ While one of our method reached 99.1% accuracy compared against a gold standard of one-to-one correspondences to English, other methods show superior results in the context of statistical machine translation.

 $$$$$ Known words are words that exist in a training corpus, in our case the European parliament proceedings consisting of 20 million words of German [Koehn, 2002].
 $$$$$ These splitting options are the basis of our work.
 $$$$$ This restrains us from over-fitting to a specific test set.

Koehn and Knight (2003) used a fixed set of two known fillers s and es for handling German compounds. $$$$$ Table 3 shows the results.
Koehn and Knight (2003) used a fixed set of two known fillers s and es for handling German compounds. $$$$$ One reason for this is that the system recovers more easily from words that are split too much than from words that are not split up sufficiently.
Koehn and Knight (2003) used a fixed set of two known fillers s and es for handling German compounds. $$$$$ Compounds are broken into either the smallest or the biggest words that can be found in a given lexicon.

We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). $$$$$ To judge this, we manually annotated the test set with correct splits.
We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). $$$$$ Larson et al. [2000] propose a data-driven method that combines compound splitting and word recombination for speech recognition.
We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). $$$$$ The ability to group split words into phrases overcomes the many mistakes of maximal (eager) splitting of words and outperforms the more accurate methods. tem with the different flavors of our training data, and evaluated the performance as before.
We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). $$$$$ By splitting all the words on the German side of the parallel corpus, we acquire a vast amount of splitting knowledge (for our data, this covers 75,055 different words).
