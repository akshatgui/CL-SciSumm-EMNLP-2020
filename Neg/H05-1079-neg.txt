However, this method does not work for realworld datasets such as PASCAL RTE (Dagan et al., 2006), because of the knowledge bottleneck: it is often the case that the lack of sufficient linguistic knowledge causes failure of inference, thus the system outputs "no entailment" for almost all pairs (Bos and Markert, 2005). $$$$$ In Section 4 we test their accuracy and robustness on the RTE datasets as one of the few currently available datasets for textual inference.
However, this method does not work for realworld datasets such as PASCAL RTE (Dagan et al., 2006), because of the knowledge bottleneck: it is often the case that the lack of sufficient linguistic knowledge causes failure of inference, thus the system outputs "no entailment" for almost all pairs (Bos and Markert, 2005). $$$$$ via ?murderer?

On PASCAL RTE datasets, strict logical inference is known to have very low recall (Bos and Markert, 2005), so on-the-fly knowledge is crucial in this setting. $$$$$ Recognising textual entailment (RTE) is the task to find out whether some text T entails a hypothesis H. This task has recently been the focus of a challenge organised by the PASCAL network in 2004/5.1 In Example 1550 H follows from T whereas this is not the case in Example 731.
On PASCAL RTE datasets, strict logical inference is known to have very low recall (Bos and Markert, 2005), so on-the-fly knowledge is crucial in this setting. $$$$$ It is similar to some of the recent approaches that were pro posed in the context of the PASCAL RTE workshop, i.e. using the OTTER theorem prover (Akhmatova, 2005; Fowler et al, 2005), using EPILOG (Bayer et al., 2005), or abduction (Raina et al, 2005).
On PASCAL RTE datasets, strict logical inference is known to have very low recall (Bos and Markert, 2005), so on-the-fly knowledge is crucial in this setting. $$$$$ Does thehybrid system using both shallow and deep se mantic analysis improve over the individual use of these methods?
On PASCAL RTE datasets, strict logical inference is known to have very low recall (Bos and Markert, 2005), so on-the-fly knowledge is crucial in this setting. $$$$$ The approximation achieved can be seen in the differenttreatment of Example 1049 (see Section 3.5) in Ex periments 3 and 4.

Bos and Markert (2005) proposes features from a model builder; Raina et al (2005) proposes an abduction process; Tatu and Moldovan (2006) shows handcrafted rules could drastically improve the performance of a logic-based RTE system. $$$$$ We are also grateful to Valentin Jijkoun and Bonnie Webber for discussion and Steve Clark and James Curran for help on using the CCG-parser.
Bos and Markert (2005) proposes features from a model builder; Raina et al (2005) proposes an abduction process; Tatu and Moldovan (2006) shows handcrafted rules could drastically improve the performance of a logic-based RTE system. $$$$$ to ?liquidator?).
Bos and Markert (2005) proposes features from a model builder; Raina et al (2005) proposes an abduction process; Tatu and Moldovan (2006) shows handcrafted rules could drastically improve the performance of a logic-based RTE system. $$$$$ This is partly due to our hy brid approach which is more robust across different datasets.
Bos and Markert (2005) proposes features from a model builder; Raina et al (2005) proposes an abduction process; Tatu and Moldovan (2006) shows handcrafted rules could drastically improve the performance of a logic-based RTE system. $$$$$ As the performance of theorem proving turnsout to be highly dependent on not read ily available background knowledge, we incorporate model building, a technique borrowed from automated reasoning, and show that it is a useful robust method to approximate entailment.

NutCracker (Bos and Markert, 2005) is a system based on logical representation and automatic theorem proving, but utilizes only WordNet (Fellbaum, 1998) as a lexical knowledge resource. $$$$$ entities in drs(T).
NutCracker (Bos and Markert, 2005) is a system based on logical representation and automatic theorem proving, but utilizes only WordNet (Fellbaum, 1998) as a lexical knowledge resource. $$$$$ The exam ple numbers have also been kept.
NutCracker (Bos and Markert, 2005) is a system based on logical representation and automatic theorem proving, but utilizes only WordNet (Fellbaum, 1998) as a lexical knowledge resource. $$$$$ As the performance of theorem proving turnsout to be highly dependent on not read ily available background knowledge, we incorporate model building, a technique borrowed from automated reasoning, and show that it is a useful robust method to approximate entailment.
NutCracker (Bos and Markert, 2005) is a system based on logical representation and automatic theorem proving, but utilizes only WordNet (Fellbaum, 1998) as a lexical knowledge resource. $$$$$ and ?slay?), are related via WordNet derivations (e.g. ?murder?
