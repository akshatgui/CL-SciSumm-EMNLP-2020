Our approach to QC follows that of (Li and Roth, 2002). $$$$$ Then each coarse class label in C 1 is expanded to a fixed set of fine classesdetermined by the class hierarchy.
Our approach to QC follows that of (Li and Roth, 2002). $$$$$ For this reason, advanced natural language techniques rather than simple key term extraction are needed.One of the important stages in this process is analyz ing the question to a degree that allows determining the ?type?
Our approach to QC follows that of (Li and Roth, 2002). $$$$$ We developed a hierarchicalclassifier that is guided by a layered semantic hierarchy of answers types, and used it to classify questions into fine-grained classes.

We will use the TREC dataset provided by Li and Roth (2002), which assigns 6000 questions with both a coarse and a fine-grained label. $$$$$ What attracts tourists to Reims?
We will use the TREC dataset provided by Li and Roth (2002), which assigns 6000 questions with both a coarse and a fine-grained label. $$$$$ Second, it provides information that downstream processes may use in determining answer se lection strategies that may be answer type specific,rather than uniform.
We will use the TREC dataset provided by Li and Roth (2002), which assigns 6000 questions with both a coarse and a fine-grained label. $$$$$ Training is done on 5,500 questions.
We will use the TREC dataset provided by Li and Roth (2002), which assigns 6000 questions with both a coarse and a fine-grained label. $$$$$ One difficulty in the question classification task is that there is no completely clear boundary between classes.

This scheme is more suitable here than other common answer-typing schemata such as the one in Li and Roth (2002), which tend to focus on questions asking for factual knowledge. $$$$$ D[n,n] is so sparse that most parts of the graph areblank.
This scheme is more suitable here than other common answer-typing schemata such as the one in Li and Roth (2002), which tend to focus on questions asking for factual knowledge. $$$$$ This paper presents a machine learning approach to question classification.

This is important because while large sets of existing questions can be obtained (Li and Roth, 2002), there are many fewer questions with available answers. Our experiments demonstrate that how-question specific unit lists consistently achieve higher answer identification performance than fixed-type, general purpose answer typing (which propose all numerical entities as answer candidates). $$$$$ These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.
This is important because while large sets of existing questions can be obtained (Li and Roth, 2002), there are many fewer questions with available answers. Our experiments demonstrate that how-question specific unit lists consistently achieve higher answer identification performance than fixed-type, general purpose answer typing (which propose all numerical entities as answer candidates). $$$$$ Our experimental re sults prove that the question classification problemcan be solved quite accurately using a learning ap proach, and exhibit the benefits of features based on semantic analysis.
This is important because while large sets of existing questions can be obtained (Li and Roth, 2002), there are many fewer questions with available answers. Our experiments demonstrate that how-question specific unit lists consistently achieve higher answer identification performance than fixed-type, general purpose answer typing (which propose all numerical entities as answer candidates). $$$$$ 2.1 Classification Standard.

For example, Li and Roth (2002) assign one of fifty possible types to a question based on features present in the question. $$$$$ Identifying that the target of this question is a definition, strategies that are specific fordefinitions (e.g., using predefined templates) may be use ful.
For example, Li and Roth (2002) assign one of fifty possible types to a question based on features present in the question. $$$$$ If a query contains Which or What, the head noun phrase determines the class, as for What X questions.While the rules used have large coverage and rea sonable accuracy, they are not sufficient to supportfine-grained classification.
For example, Li and Roth (2002) assign one of fifty possible types to a question based on features present in the question. $$$$$ Our goal is to categorize questions into different semantic classes that impose constraints on potential answers, so that they can be utilized in later stages of the question answeringprocess.

(Li and Roth, 2002) propose a system based on SNoW. $$$$$ 2.
(Li and Roth, 2002) propose a system based on SNoW. $$$$$ This paper presents a machine learning approach to question classification.
(Li and Roth, 2002) propose a system based on SNoW. $$$$$ The difficulty is more acute in tasks such as story comprehension in which the target text is less likely to overlap with the text in the questions.

The same dataset has been used in other investigations, such as in (Li and Roth, 2002). The distribution of these 5500 training questions, with respect to its interrogative pronoun or the initial word is showed in Table 1. $$$$$ This paper presents a machine learning approach to question classification.
The same dataset has been used in other investigations, such as in (Li and Roth, 2002). The distribution of these 5500 training questions, with respect to its interrogative pronoun or the initial word is showed in Table 1. $$$$$ Wesuggest that it is useful to consider this classifica tion task as a multi-label classification and find that it is possible to achieve good classification results(over 90%) despite the fact that the number of dif ferent labels used is fairly large, 50.
The same dataset has been used in other investigations, such as in (Li and Roth, 2002). The distribution of these 5500 training questions, with respect to its interrogative pronoun or the initial word is showed in Table 1. $$$$$ Indeed, all the reformulation questions that we exemplified in Sec.

(Li and Roth, 2002) obtain a better performance for English, around a 92.5% in terms of accuracy. $$$$$ We can see that there is no good cluster ing of fine classes mistakes within a coarse class,which explains intuitively why the hierarchical clas sifier with an additional level coarse classes does not work much better.
(Li and Roth, 2002) obtain a better performance for English, around a 92.5% in terms of accuracy. $$$$$ Good learning methods with appropriate features, on the other hand, may not suffer from the fact that the number of potential features (derived from wordsand syntactic structures) is so large and would gen eralize and classify these cases correctly.

It could be also interested to test the combination between a better QC system, the current one by Li and Roth's for instance (Li and Roth, 2002), and our machine translation method. $$$$$ In future work we plan to investigate further the application of deeper semantic analysis (including better named entity and semantic categorization) to feature extraction, automate the generation of thesemantic features and develop a better understand ing to some of the learning issues involved in thedifference between a flat and a hierarchical classi fier.
It could be also interested to test the combination between a better QC system, the current one by Li and Roth's for instance (Li and Roth, 2002), and our machine translation method. $$$$$ We show accurate results on a large col lection of free-form questions used in TREC 10.

Li and Roth (2002) have developed a machine learning approach which uses the SNoW learning architecture. $$$$$ Its parameters and deci sion model are the same as those of the hierarchicalone.
Li and Roth (2002) have developed a machine learning approach which uses the SNoW learning architecture. $$$$$ Open-domain question answering (Lehnert, 1986; Harabagiu et al, 2001; Light et al, 2001) and storycomprehension (Hirschman et al, 1999) have become important directions in natural language pro cessing.
Li and Roth (2002) have developed a machine learning approach which uses the SNoW learning architecture. $$$$$ Q: What Canadian city has the largest population?

Compared to the over feature size of 200000 in Li and Roth (2002), our feature space is much more compact, yet turned out to be more informative as suggested by the experiments. $$$$$ We observe thatlocal features are not sufficient to support this accu racy, and that inducing semantic features is crucial for good performance.
Compared to the over feature size of 200000 in Li and Roth (2002), our feature space is much more compact, yet turned out to be more informative as suggested by the experiments. $$$$$ Chunks are extracted using a previously learned classifier (Punyakanok and Roth, 2001; Li and Roth, 2001).
Compared to the over feature size of 200000 in Li and Roth (2002), our feature space is much more compact, yet turned out to be more informative as suggested by the experiments. $$$$$ Similarly, in: Q: Why is the sun yellow?

With the increasing popularity of statistical NLP, Li and Roth (2002), Hacioglu and Ward (2003) and Zhang and Lee (2003) used supervised learning for question classification on a data set from UIUC that is now standard1. $$$$$ We use several types of features and investigate below their contribution to the QC accuracy.
With the increasing popularity of statistical NLP, Li and Roth (2002), Hacioglu and Ward (2003) and Zhang and Lee (2003) used supervised learning for question classification on a data set from UIUC that is now standard1. $$$$$ In future work we plan to investigate further the application of deeper semantic analysis (including better named entity and semantic categorization) to feature extraction, automate the generation of thesemantic features and develop a better understand ing to some of the learning issues involved in thedifference between a flat and a hierarchical classi fier.

Li and Roth (2002) used a Sparse Network of Winnows (SNoW) (Khardon et al, 1999). $$$$$ We developed a hierarchicalclassifier that is guided by a layered semantic hierarchy of answers types, and used it to classify questions into fine-grained classes.
Li and Roth (2002) used a Sparse Network of Winnows (SNoW) (Khardon et al, 1999). $$$$$ We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into finegrained classes.
Li and Roth (2002) used a Sparse Network of Winnows (SNoW) (Khardon et al, 1999). $$$$$ These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.

Our findings corroborate Li and Roth (2002), who report little benefit from adding head chunk features for the fine classification task. $$$$$ 3 Learning a Question Classifier.
Our findings corroborate Li and Roth (2002), who report little benefit from adding head chunk features for the fine classification task. $$$$$ Therefore, the classification of a specific question can be quite ambiguous.
Our findings corroborate Li and Roth (2002), who report little benefit from adding head chunk features for the fine classification task. $$$$$ The paper is organized as follows: Sec.

in (Li and Roth, 2002) to our basic QA system, YourQA (Quarteroni and Manandhar, 2008) and by gathering the top 20 answer paragraphs. $$$$$ Each coarse class contains anon-overlapping set of fine classes.
in (Li and Roth, 2002) to our basic QA system, YourQA (Quarteroni and Manandhar, 2008) and by gathering the top 20 answer paragraphs. $$$$$ In future work we plan to investigate further the application of deeper semantic analysis (including better named entity and semantic categorization) to feature extraction, automate the generation of thesemantic features and develop a better understand ing to some of the learning issues involved in thedifference between a flat and a hierarchical classi fier.
in (Li and Roth, 2002) to our basic QA system, YourQA (Quarteroni and Manandhar, 2008) and by gathering the top 20 answer paragraphs. $$$$$ Question answering is a retrieval task morechallenging than common search engine tasks be cause its purpose is to find an accurate and conciseanswer to a question rather than a relevant docu ment.

 $$$$$ These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.
 $$$$$ The approach we adoptedis a multi-level learning approach: some of our fea tures rely on finer analysis of the questions that are outcomes of learned classifiers; the QC module then applies learning with these as input features.
 $$$$$ We show accurate results on a large col lection of free-form questions used in TREC 10.
 $$$$$ This paper presents a machine learning approach toquestion classification.

Answer types are determined using classification rules similar to Li and Roth (2002). $$$$$ Comparison of the hierarchical and the flat classifier The flat classifier consists of one classifier which isalmost the same as the fine classifier in the hierar chical case, except that its initial confusion set is the whole set of fine classes.
Answer types are determined using classification rules similar to Li and Roth (2002). $$$$$ This paper presents a machine learning approach to question classification.
Answer types are determined using classification rules similar to Li and Roth (2002). $$$$$ Re sults are evaluated using P 1 and P 5 . Tables 3 and 4 show the P 1 and P 5 accuracyof the hierarchical classifier on training sets of dif ferent sizes and exhibit the learning curve for this problem.We note that the average numbers of labels out put by the coarse and fine classifiers are 1.54 and 2.05 resp., (using the feature set RelWord and 5,500 training examples), which shows the decision model is accurate as well as efficient.

The classification scheme we propose is based on one dynamic 1 and one static layer, contrasting with previous work that uses static taxonomies (Li and Roth, 2002). $$$$$ Training is done on 5,500 questions.
The classification scheme we propose is based on one dynamic 1 and one static layer, contrasting with previous work that uses static taxonomies (Li and Roth, 2002). $$$$$ We developed a hierarchicalclassifier that is guided by a layered semantic hierarchy of answers types, and used it to classify questions into fine-grained classes.
The classification scheme we propose is based on one dynamic 1 and one static layer, contrasting with previous work that uses static taxonomies (Li and Roth, 2002). $$$$$ This paper presents a machine learning approach toquestion classification.
The classification scheme we propose is based on one dynamic 1 and one static layer, contrasting with previous work that uses static taxonomies (Li and Roth, 2002). $$$$$ We developed a hierarchicalclassifier that is guided by a layered semantic hierarchy of answers types, and used it to classify questions into fine-grained classes.

1500 of those questions come from the Li and Roth corpus (Li and Roth, 2002), 500 questions were taken from the TREC-10 questions and 100 questions were asked over the Italian Opera topic map. $$$$$ To avoid this problem,we allow our classifiers to assign multiple class la bels for a single question.
1500 of those questions come from the Li and Roth corpus (Li and Roth, 2002), 500 questions were taken from the TREC-10 questions and 100 questions were asked over the Italian Opera topic map. $$$$$ We developed a hierarchicalclassifier that is guided by a layered semantic hierarchy of answers types, and used it to classify questions into fine-grained classes.
1500 of those questions come from the Li and Roth corpus (Li and Roth, 2002), 500 questions were taken from the TREC-10 questions and 100 questions were asked over the Italian Opera topic map. $$$$$ Overall, we get a98.80% precision for coarse classes with all the fea tures and 95% for the fine classes.
1500 of those questions come from the Li and Roth corpus (Li and Roth, 2002), 500 questions were taken from the TREC-10 questions and 100 questions were asked over the Italian Opera topic map. $$$$$ 2 presents the question classification problem; Sec.

We followed Li and Roth (Li and Roth, 2002) to implement the features for the EAT classifier. $$$$$ As a result, existing approaches, as in (Singhal et al, 2000), have adopted a small set of simple answer entitytypes, which consisted of the classes: Person, Location, Organization, Date, Quantity, Duration, Lin ear Measure.
We followed Li and Roth (Li and Roth, 2002) to implement the features for the EAT classifier. $$$$$ The primitive feature types extracted for eachquestion include words, pos tags, chunks (non overlapping phrases) (Abney, 1991), named entities,head chunks (e.g., the first noun chunk in a sen tence) and semantically related words (words that often occur with a specific question class).
We followed Li and Roth (Li and Roth, 2002) to implement the features for the EAT classifier. $$$$$ These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.
We followed Li and Roth (Li and Roth, 2002) to implement the features for the EAT classifier. $$$$$ We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into finegrained classes.
