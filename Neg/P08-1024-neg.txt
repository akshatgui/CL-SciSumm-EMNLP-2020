Coming up-to-date, (Blunsom et al, 2008) attempt a related estimation problem to (Marcu and Wong, 2002), using the expanded phrase pair set of (Chiang, 2005a), working with an exponential model and concentrating on marginalizing out the latent segmentation variable. $$$$$ Firstly we show the relative scores of our model against Hiero without using reverse translation or lexical features.7 This allows us to directly study the differences between the two translation models without the added complication of the other features.
Coming up-to-date, (Blunsom et al, 2008) attempt a related estimation problem to (Marcu and Wong, 2002), using the expanded phrase pair set of (Chiang, 2005a), working with an exponential model and concentrating on marginalizing out the latent segmentation variable. $$$$$ Existing discriminative models require a reference derivation to optimise against, however no parallel corpora annotated for derivations exist.

 $$$$$ Such approaches have been shown to be effective in log-linear wordalignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006).
 $$$$$ However, doing so exactly is NP-complete.
 $$$$$ Having demonstrated the efficacy of our model with very simple features, the logical next step is to investigate more expressive features.

Secondly, as (Blunsom et al, 2008) show, marginalizing out the different segmentations during decoding leads to improved performance. $$$$$ Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters.
Secondly, as (Blunsom et al, 2008) show, marginalizing out the different segmentations during decoding leads to improved performance. $$$$$ We argue that a principle reason for this failure is not dealing with multiple, equivalent translations.

The version presented in (Blunsom et al., 2008) scales to more than a hundred thousand short training sentences, but does not integrate a language model and thus has performance that improves upon Hiero without a language model only. $$$$$ The paper is structured as follows.
The version presented in (Blunsom et al., 2008) scales to more than a hundred thousand short training sentences, but does not integrate a language model and thus has performance that improves upon Hiero without a language model only. $$$$$ Our estimation method is theoretically sound, avoiding the biases of the heuristic relative frequency estimates length and the average number of derivations (on a log scale) for each reference sentence in our training corpus.
The version presented in (Blunsom et al., 2008) scales to more than a hundred thousand short training sentences, but does not integrate a language model and thus has performance that improves upon Hiero without a language model only. $$$$$ Even with a very tight beam of 100, max-translation decoding outperforms maximum-derivation decoding, and performance is increasing even at a width of 10k.
The version presented in (Blunsom et al., 2008) scales to more than a hundred thousand short training sentences, but does not integrate a language model and thus has performance that improves upon Hiero without a language model only. $$$$$ The proportion of discarded sentences is a function of the grammar used.

In practice, this problem can be circumvented by discarding the training sentence pairs with unreachable reference translations, but this may mean a significant reduction in the amount of training data (24% in (Blunsom et al., 2008)). $$$$$ However, doing so exactly is NP-complete.
In practice, this problem can be circumvented by discarding the training sentence pairs with unreachable reference translations, but this may mean a significant reduction in the amount of training data (24% in (Blunsom et al., 2008)). $$$$$ Finally, while in this paper we have focussed on the science of discriminative machine translation, we believe that with suitable engineering this model will advance the state-of-the-art.
In practice, this problem can be circumvented by discarding the training sentence pairs with unreachable reference translations, but this may mean a significant reduction in the amount of training data (24% in (Blunsom et al., 2008)). $$$$$ There is no one reference derivation.

Instead of using expected BLEU as a training objective, Blunsom et al (2008) trained their model to directly maximise the log-likelihood of the discriminative model, estimating feature expectations from a packed chart. $$$$$ We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised.
Instead of using expected BLEU as a training objective, Blunsom et al (2008) trained their model to directly maximise the log-likelihood of the discriminative model, estimating feature expectations from a packed chart. $$$$$ Progress within these approaches however has been less dramatic.
Instead of using expected BLEU as a training objective, Blunsom et al (2008) trained their model to directly maximise the log-likelihood of the discriminative model, estimating feature expectations from a packed chart. $$$$$ The space of translation sub-strings is exponential in each cell’s span, and our algorithm can only sum over a small fraction of the possible strings.
Instead of using expected BLEU as a training objective, Blunsom et al (2008) trained their model to directly maximise the log-likelihood of the discriminative model, estimating feature expectations from a packed chart. $$$$$ The distribution is globally normalised by the partition function, ZA(f), which sums out the numerator in (1) for every derivation (and therefore every translation) of f: Given (1), the conditional probability of a target translation given the source is the sum over all of its derivations: where O(e, f) is the set of all derivations of the target sentence e from the source f. Most prior work in SMT, both generative and discriminative, has approximated the sum over derivations by choosing a single ‘best’ derivation using a Viterbi or beam search algorithm.

This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al, 2008). $$$$$ Given these two charts we can calculate the loglikelihood of the reference translation as the insidescore from the sentence spanning cell of the reference chart, normalised by the inside-score of the spanning cell from the full chart.
This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al, 2008). $$$$$ Figure 1 illustrates the exponential relationship between sentence length and the number of derivations.
This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al, 2008). $$$$$ Finally, while in this paper we have focussed on the science of discriminative machine translation, we believe that with suitable engineering this model will advance the state-of-the-art.
This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al, 2008). $$$$$ Here we approximate the sum over derivations directly using a beam search in which we produce a beam of high probability translation sub-strings for each cell in the parse chart.

In recent years, great research has shown the strength of latent variable models for natural language processing (Blunsom et al, 2008). $$$$$ Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.
In recent years, great research has shown the strength of latent variable models for natural language processing (Blunsom et al, 2008). $$$$$ We suggest that is a result of the degenerate solutions of the conditional maximum likelihood estimate, as described in DeNero et al. (2006).

We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al (2008). $$$$$ Scaling In Figure 6 we plot the scaling characteristics of our models.
We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al (2008). $$$$$ (Koehn et al., 2003).
We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al (2008). $$$$$ We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised.
We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al (2008). $$$$$ These systems all include regularisation, thereby addressing Problem 2.

Of ten, there are many derivations that are distinct yet produce the same translation. Blunsom et al (2008) present a latent variable model that describes the relationship between translation and derivation clearly. $$$$$ A particular sequence of SCFG rule applications which produces a translation from a source sentence is referred to as a derivation, and each translation may be produced by many different derivations.
Of ten, there are many derivations that are distinct yet produce the same translation. Blunsom et al (2008) present a latent variable model that describes the relationship between translation and derivation clearly. $$$$$ Having demonstrated the efficacy of our model with very simple features, the logical next step is to investigate more expressive features.
Of ten, there are many derivations that are distinct yet produce the same translation. Blunsom et al (2008) present a latent variable model that describes the relationship between translation and derivation clearly. $$$$$ The development and test data was taken from the 2006 NAACL and 2007 ACL workshops on machine translation, also filtered for sentence length.4 Tuning of the regularisation parameter and MERT training of the benchmark models was performed on dev2006, while the test set was the concatenation of devtest2006, test2006 and test2007, amounting to 315 development and 1164 test sentences.
Of ten, there are many derivations that are distinct yet produce the same translation. Blunsom et al (2008) present a latent variable model that describes the relationship between translation and derivation clearly. $$$$$ Our model addresses all three of the above problems within a global model, without resorting to nbest lists or local independence assumptions.

(6) as max-derivation decoding, which are first termed by Blunsom et al (2008). $$$$$ Past work on discriminative SMT only address some of these problems.
(6) as max-derivation decoding, which are first termed by Blunsom et al (2008). $$$$$ In decoding we can search for the maximum probability derivation, which is the standard practice in SMT, or for the maximum probability translation which is what we actually want from our model, i.e. the best translation.

Blunsom et al (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. $$$$$ Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.
Blunsom et al (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. $$$$$ Discriminative models of translation can include such features without making assumptions of independence or explicitly modelling their interdependence.
Blunsom et al (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. $$$$$ However, as demonstrated in Section 4, this algorithm is considerably more effective than maximum derivation (Viterbi) decoding.

Both Mi et al (2008) and Blunsom et al (2008) use a translation hyper graph to represent search space. $$$$$ To our knowledge no systems directly address Problem 1, instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list (Liang et al., 2006; Watanabe et al., 2007), or else making local independence assumptions which side-step the issue (Ittycheriah and Roukos, 2007; Tillmann and Zhang, 2007; Wellington et al., 2006).
Both Mi et al (2008) and Blunsom et al (2008) use a translation hyper graph to represent search space. $$$$$ There were many grammar rules that the discriminative model did not observe in a reference derivation, and thus could not assign their feature a positive weight.

For the first model, which includes the sparse parse features, we learn weights in order to optimize penalized conditional log likelihood (Blunsom et al, 2008). $$$$$ Progress within these approaches however has been less dramatic.
For the first model, which includes the sparse parse features, we learn weights in order to optimize penalized conditional log likelihood (Blunsom et al, 2008). $$$$$ An interesting counterpoint is the work of DeNero et al. (2006), who show that their unregularised model finds degenerate solutions.
For the first model, which includes the sparse parse features, we learn weights in order to optimize penalized conditional log likelihood (Blunsom et al, 2008). $$$$$ Results show that accounting for multiple derivations does indeed improve performance.
For the first model, which includes the sparse parse features, we learn weights in order to optimize penalized conditional log likelihood (Blunsom et al, 2008). $$$$$ We believe this is because these frequency count based' models cannot easily incorporate non-independent and overlapping features, which are extremely useful in describing the translation process.

See Blunsom et al (2008) for more information. $$$$$ Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions.
See Blunsom et al (2008) for more information. $$$$$ Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.
See Blunsom et al (2008) for more information. $$$$$ This error in the reordering of card (which is an acceptable translation of carte) is due to the rule (X) —* (carte X 1, X 1 card) being the highest scoring rule for carte.

We might also want to calculate the total probability of all possible derivations, which is useful for parameter estimation (Blunsom et al, 2008). $$$$$ Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions.
We might also want to calculate the total probability of all possible derivations, which is useful for parameter estimation (Blunsom et al, 2008). $$$$$ Our log-linear translation model defines a conditional probability distribution over the target translations of a given source sentence.

Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al, 2008). $$$$$ As expected, the language model makes a significant difference to BLEU, however we believe that this effect is orthogonal to the choice of base translation model, thus we would expect a similar gain when integrating a language model into the discriminative system.
Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al, 2008). $$$$$ Here we approximate the sum over derivations directly using a beam search in which we produce a beam of high probability translation sub-strings for each cell in the parse chart.

For the hierarchical phrase-based approach, (Blunsom et al, 2008) present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations. $$$$$ Achieving full coverage implies inducing a grammar which generates all observed source-target pairs, which is difficult in practise.
For the hierarchical phrase-based approach, (Blunsom et al, 2008) present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations. $$$$$ As such, our base model includes a single binary identity feature per-rule, equivalent to the p(e|f) parameters defined on each rule in standard models.
For the hierarchical phrase-based approach, (Blunsom et al, 2008) present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations. $$$$$ We argue that a principle reason for this failure is not dealing with multiple, equivalent translations.

The first method maximizes data likelihood as is standard in EM, while the second method maximizes conditional likelihood for a log linear model following Blunsom et al (2008). $$$$$ Unfortunately marginalising over derivations in decoding is NP-complete.
The first method maximizes data likelihood as is standard in EM, while the second method maximizes conditional likelihood for a log linear model following Blunsom et al (2008). $$$$$ Again, we use the inside-outside algorithm to find the ‘reference’ feature expectations from this chart.
The first method maximizes data likelihood as is standard in EM, while the second method maximizes conditional likelihood for a log linear model following Blunsom et al (2008). $$$$$ Such approaches have been shown to be effective in log-linear wordalignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006).

Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al, 2008). $$$$$ To do so would require integrating a language model feature into the max-translation decoding algorithm.
Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al, 2008). $$$$$ However, while discriminative models promise much, they have not been shown to deliver significant gains 'We class approaches using minimum error rate training (Och, 2003) frequency count based as these systems re-scale a handful of generative features estimated from frequency counts and do not support large sets of non-independent features. over their simpler cousins.
Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al, 2008). $$$$$ We argue that a principle reason for this failure is not dealing with multiple, equivalent translations.
Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al, 2008). $$$$$ We argue that a principle reason for this failure is not dealing with multiple, equivalent translations.
