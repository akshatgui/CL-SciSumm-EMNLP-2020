Coming up-to-date, (Blunsom et al, 2008) attempt a related estimation problem to (Marcu and Wong, 2002), using the expanded phrase pair set of (Chiang, 2005a), working with an exponential model and concentrating on marginalizing out the latent segmentation variable. $$$$$ From this we can conclude that the maximum likelihood model is overfitting the training set.
Coming up-to-date, (Blunsom et al, 2008) attempt a related estimation problem to (Marcu and Wong, 2002), using the expanded phrase pair set of (Chiang, 2005a), working with an exponential model and concentrating on marginalizing out the latent segmentation variable. $$$$$ This is encouraging as our model was trained to optimise likelihood rather than BLEU, yet it is still competitive on that metric.
Coming up-to-date, (Blunsom et al, 2008) attempt a related estimation problem to (Marcu and Wong, 2002), using the expanded phrase pair set of (Chiang, 2005a), working with an exponential model and concentrating on marginalizing out the latent segmentation variable. $$$$$ These occur when there are many ways to translate a source sentence to the same target sentence by applying a sequence of steps (a derivation) of either phrase translations or synchronous grammar rules, depending on the type of system.

 $$$$$ Results show that accounting for multiple derivations does indeed improve performance.
 $$$$$ Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.
 $$$$$ In decoding we can search for the maximum probability derivation, which is the standard practice in SMT, or for the maximum probability translation which is what we actually want from our model, i.e. the best translation.
 $$$$$ However, while Chiang (2005) stores an abbreviated context composed of the n âˆ’ 1 target words on the left and right edge of the target substring, here we store the entire target string.

Secondly, as (Blunsom et al, 2008) show, marginalizing out the different segmentations during decoding leads to improved performance. $$$$$ Results show that accounting for multiple derivations does indeed improve performance.
Secondly, as (Blunsom et al, 2008) show, marginalizing out the different segmentations during decoding leads to improved performance. $$$$$ Such approaches have been shown to be effective in log-linear wordalignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006).

The version presented in (Blunsom et al., 2008) scales to more than a hundred thousand short training sentences, but does not integrate a language model and thus has performance that improves upon Hiero without a language model only. $$$$$ We argue that a principle reason for this failure is not dealing with multiple, equivalent translations.
The version presented in (Blunsom et al., 2008) scales to more than a hundred thousand short training sentences, but does not integrate a language model and thus has performance that improves upon Hiero without a language model only. $$$$$ Often there are thousands of ways of translating a source sentence into the reference translation.
The version presented in (Blunsom et al., 2008) scales to more than a hundred thousand short training sentences, but does not integrate a language model and thus has performance that improves upon Hiero without a language model only. $$$$$ We present efficient methods for training and prediction, demonstrating their scaling properties by training on more than a hundred thousand training sentences.

In practice, this problem can be circumvented by discarding the training sentence pairs with unreachable reference translations, but this may mean a significant reduction in the amount of training data (24% in (Blunsom et al., 2008)). $$$$$ Such approaches have been shown to be effective in log-linear wordalignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006).
In practice, this problem can be circumvented by discarding the training sentence pairs with unreachable reference translations, but this may mean a significant reduction in the amount of training data (24% in (Blunsom et al., 2008)). $$$$$ Figure 3 shows a simple instance of a hierarchical grammar with two non-terminals.
In practice, this problem can be circumvented by discarding the training sentence pairs with unreachable reference translations, but this may mean a significant reduction in the amount of training data (24% in (Blunsom et al., 2008)). $$$$$ We present efficient methods for training and prediction, demonstrating their scaling properties by training on more than a hundred thousand training sentences.

Instead of using expected BLEU as a training objective, Blunsom et al (2008) trained their model to directly maximise the log-likelihood of the discriminative model, estimating feature expectations from a packed chart. $$$$$ An interesting counterpoint is the work of DeNero et al. (2006), who show that their unregularised model finds degenerate solutions.

This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al, 2008). $$$$$ A synchronous context free grammar (SCFG) consists of paired CFG rules with co-indexed nonterminals (Lewis II and Stearns, 1968).
This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al, 2008). $$$$$ The max-translation model finds a good translation card on the table.
This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al, 2008). $$$$$ There is no one reference derivation.
This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al, 2008). $$$$$ Rule frequency features extracted from large training corpora would help the model to overcome the issue of unreachable reference sentences.

In recent years, great research has shown the strength of latent variable models for natural language processing (Blunsom et al, 2008). $$$$$ We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised.
In recent years, great research has shown the strength of latent variable models for natural language processing (Blunsom et al, 2008). $$$$$ We use a synchronous context free grammar (SCFG) translation system (Chiang, 2007), a model which has yielded state-of-the-art results on many translation tasks.

We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al (2008). $$$$$ Ideally, a model would account for this ambiguity by marginalising out the derivations, thus predicting the best translation rather than the best derivation.
We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al (2008). $$$$$ We argue that a principle reason for this failure is not dealing with multiple, equivalent translations.
We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al (2008). $$$$$ As well as both modelling the same distribution, when our model is trained with a single parameter per-rule these systems have the same parameter space, differing only in the manner of estimation.
We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al (2008). $$$$$ This method has been demonstrated to be effective for (non-convex) log-linear models with latent variables (Clark and Curran, 2004; Petrov et al., 2007).

Of ten, there are many derivations that are distinct yet produce the same translation. Blunsom et al (2008) present a latent variable model that describes the relationship between translation and derivation clearly. $$$$$ Therefore the resulting probabilities are only estimates.
Of ten, there are many derivations that are distinct yet produce the same translation. Blunsom et al (2008) present a latent variable model that describes the relationship between translation and derivation clearly. $$$$$ We argue that a principle reason for this failure is not dealing with multiple, equivalent translations.

(6) as max-derivation decoding, which are first termed by Blunsom et al (2008). $$$$$ To our knowledge no systems directly address Problem 1, instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list (Liang et al., 2006; Watanabe et al., 2007), or else making local independence assumptions which side-step the issue (Ittycheriah and Roukos, 2007; Tillmann and Zhang, 2007; Wellington et al., 2006).
(6) as max-derivation decoding, which are first termed by Blunsom et al (2008). $$$$$ First, we develop a log-linear model of translation which is globally trained on a significant number of parallel sentences.
(6) as max-derivation decoding, which are first termed by Blunsom et al (2008). $$$$$ We argue that a principle reason for this failure is not dealing with multiple, equivalent translations.
(6) as max-derivation decoding, which are first termed by Blunsom et al (2008). $$$$$ The proportion of discarded sentences is a function of the grammar used.

Blunsom et al (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. $$$$$ Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions.
Blunsom et al (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. $$$$$ To do this we use our own implementation of Hiero (Chiang, 2007), with the same grammar but with the traditional generative feature set trained in a linear model with minimum BLEU training.
Blunsom et al (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. $$$$$ We argue that this is due to a number of inherent problems that discriminative models for SMT must address, in particular the problems of spurious ambiguity and degenerate solutions.

Both Mi et al (2008) and Blunsom et al (2008) use a translation hyper graph to represent search space. $$$$$ This is due to the many rules that enforce monotone ordering around sur la, (X) â€”* (X 1 sur, X 1 in) (X) â€”* (X 1 sur la X 2, X 1 in the X 2) etc.
Both Mi et al (2008) and Blunsom et al (2008) use a translation hyper graph to represent search space. $$$$$ We show empirically that this treatment results in significant improvements over a maximum-derivation model.
Both Mi et al (2008) and Blunsom et al (2008) use a translation hyper graph to represent search space. $$$$$ The features must decompose with the derivation, as shown in (2).

For the first model, which includes the sparse parse features, we learn weights in order to optimize penalized conditional log likelihood (Blunsom et al, 2008). $$$$$ These limits were chosen as a compromise between experiment turnaround time and leaving a large enough corpus to obtain indicative results.
For the first model, which includes the sparse parse features, we learn weights in order to optimize penalized conditional log likelihood (Blunsom et al, 2008). $$$$$ Even with a very tight beam of 100, max-translation decoding outperforms maximum-derivation decoding, and performance is increasing even at a width of 10k.
For the first model, which includes the sparse parse features, we learn weights in order to optimize penalized conditional log likelihood (Blunsom et al, 2008). $$$$$ We believe this is because these frequency count based' models cannot easily incorporate non-independent and overlapping features, which are extremely useful in describing the translation process.
For the first model, which includes the sparse parse features, we learn weights in order to optimize penalized conditional log likelihood (Blunsom et al, 2008). $$$$$ In this paper we directly address the problem of spurious ambiguity in discriminative models.

See Blunsom et al (2008) for more information. $$$$$ This is illustrated in Table 2, which shows the conditional probabilities for rules, obtained by locally normalising the rule feature weights for a simple grammar extracted from the ambiguous pair of sentences presented in DeNero et al. (2006).
See Blunsom et al (2008) for more information. $$$$$ These results could â€“ and should â€“ be applied to other models, discriminative and generative, phrase- and syntax-based, to further progress the state-of-the-art in machine translation.
See Blunsom et al (2008) for more information. $$$$$ Results show that accounting for multiple derivations does indeed improve performance.
See Blunsom et al (2008) for more information. $$$$$ The intuition is that small rules are likely to appear more frequently, and thus generalise better to a test set.

We might also want to calculate the total probability of all possible derivations, which is useful for parameter estimation (Blunsom et al, 2008). $$$$$ Derivational ambiguity Table 1 shows the impact of accounting for derivational ambiguity in training and decoding.5 There are two options for training, we could use our latent variable model and optimise the probability of all derivations of the reference translation, or choose a single derivation that yields the reference and optimise its probability alone.
We might also want to calculate the total probability of all possible derivations, which is useful for parameter estimation (Blunsom et al, 2008). $$$$$ Here we see that the model finds the desired solution in which the true ambiguity of the translation rules is preserved.
We might also want to calculate the total probability of all possible derivations, which is useful for parameter estimation (Blunsom et al, 2008). $$$$$ Having demonstrated the efficacy of our model with very simple features, the logical next step is to investigate more expressive features.
We might also want to calculate the total probability of all possible derivations, which is useful for parameter estimation (Blunsom et al, 2008). $$$$$ A particular sequence of SCFG rule applications which produces a translation from a source sentence is referred to as a derivation, and each translation may be produced by many different derivations.

Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al, 2008). $$$$$ Furthermore, our model explicitly accounts for spurious ambiguity without altering the model structure or arbitrarily selecting one derivation.
Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al, 2008). $$$$$ The feature set includes: a trigram language model (lm) trained on the English side of the unfiltered Europarl corpus; direct and reverse translation scores estimated as relative frequencies (pd, pr); lexical translation scores (plex d , plex r ), a binary flag for the glue rule which allows the model to (dis)favour monotone translation (gr); and rule and target word counts (rc, wc).
Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al, 2008). $$$$$ These occur when there are many ways to translate a source sentence to the same target sentence by applying a sequence of steps (a derivation) of either phrase translations or synchronous grammar rules, depending on the type of system.

For the hierarchical phrase-based approach, (Blunsom et al, 2008) present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations. $$$$$ Some of these discriminative systems have been trained on large training sets (Problem 3); these systems are the local models, for which training is much simpler.
For the hierarchical phrase-based approach, (Blunsom et al, 2008) present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations. $$$$$ We present two main contributions.
For the hierarchical phrase-based approach, (Blunsom et al, 2008) present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations. $$$$$ These occur when there are many ways to translate a source sentence to the same target sentence by applying a sequence of steps (a derivation) of either phrase translations or synchronous grammar rules, depending on the type of system.
For the hierarchical phrase-based approach, (Blunsom et al, 2008) present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations. $$$$$ All the models we present use the grammar extraction technique described in Chiang (2007), and are bench-marked against our own implementation of this hierarchical model (Hiero).

The first method maximizes data likelihood as is standard in EM, while the second method maximizes conditional likelihood for a log linear model following Blunsom et al (2008). $$$$$ We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised.
The first method maximizes data likelihood as is standard in EM, while the second method maximizes conditional likelihood for a log linear model following Blunsom et al (2008). $$$$$ Finally, while in this paper we have focussed on the science of discriminative machine translation, we believe that with suitable engineering this model will advance the state-of-the-art.

Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al, 2008). $$$$$ Our model addresses all three of the above problems within a global model, without resorting to nbest lists or local independence assumptions.
Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al, 2008). $$$$$ Rule frequency features extracted from large training corpora would help the model to overcome the issue of unreachable reference sentences.
Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al, 2008). $$$$$ We show empirically that this treatment results in significant improvements over a maximum-derivation model.
Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al, 2008). $$$$$ A particular sequence of SCFG rule applications which produces a translation from a source sentence is referred to as a derivation, and each translation may be produced by many different derivations.
