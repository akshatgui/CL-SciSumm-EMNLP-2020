Si and Callan (2001) and Collins-Thompson and Callan (2004) have demonstrated the use of language models is more robust for web documents and passages. $$$$$ We derive L(Gi  |T) from (1) via Bayes’ Rule, which is: However, we first make two further assumptions: Substituting (1) into (2), simplifying, and taking logarithms, we obtain: where log Z represents combined factors involving passage length and the uniform prior P(Gi) which, according to our assumptions, do not influence the prediction outcome and may be ignored.
Si and Callan (2001) and Collins-Thompson and Callan (2004) have demonstrated the use of language models is more robust for web documents and passages. $$$$$ Any opinions, findings, conclusions, or recommendations expressed in this material are the authors’, and do not necessarily reflect those of the sponsors.
Si and Callan (2001) and Collins-Thompson and Callan (2004) have demonstrated the use of language models is more robust for web documents and passages. $$$$$ This work was supported by NSF grant IIS-0096139 and Dept. of Education grant R305G03123.
Si and Callan (2001) and Collins-Thompson and Callan (2004) have demonstrated the use of language models is more robust for web documents and passages. $$$$$ Indeed, trying to learn absolute type probabilities would be undesirable since it would fit the model too closely to whatever specific topics were in the training set.

 $$$$$ In this paper we examine feature selection as well as our model's ability to generalize.
 $$$$$ The sum in (3) is easily computed: for each token in T, we simply look up its log probability in the language model of Gi and sum over all tokens to obtain the total likelihood of the passage given the grade.
 $$$$$ In the extreme case, consider two single-word ‘passages’: ‘bunny’ and ‘bulkheads’.
 $$$$$ An example of the set of log-likelihoods calculated across all 12 grade models, with a maximum point clearly evident, is shown in Figure 2.

In order to adjust search result presentation to the user's reading ability, we estimate the reading difficulty of each retrieved document using the Smoothed Unigram Model, a variation of a Multinomial Bayes classifier (Collins-Thompson and Callan, 2004). $$$$$ Sections 7 and 8 discuss the evaluation results and give our observations and conclusions.
In order to adjust search result presentation to the user's reading ability, we estimate the reading difficulty of each retrieved document using the Smoothed Unigram Model, a variation of a Multinomial Bayes classifier (Collins-Thompson and Callan, 2004). $$$$$ The language models we use are simple: they are based on unigrams and assume that the probability of a token is independent of the surrounding tokens, given the grade language model.
In order to adjust search result presentation to the user's reading ability, we estimate the reading difficulty of each retrieved document using the Smoothed Unigram Model, a variation of a Multinomial Bayes classifier (Collins-Thompson and Callan, 2004). $$$$$ To our knowledge, the only previous work which has considered a language modeling approach to readability is a preliminary study by Si and Callan (2001).

The latter has been found to be more effective as the former when approaching the reading level of subjects in primary and secondary school age (Collins-Thompson and Callan, 2004). $$$$$ Other words like ‘perimeter’ attain maximum probability in a specific grade range, perhaps corresponding to the period in which these concepts are emphasized in the curriculum.
The latter has been found to be more effective as the former when approaching the reading level of subjects in primary and secondary school age (Collins-Thompson and Callan, 2004). $$$$$ The model we present below may be thought of as a generalization of the vocabulary-based approach, in which we build multiple language models - in this study, one for each grade - that capture more fine-grained information about vocabulary usage.
The latter has been found to be more effective as the former when approaching the reading level of subjects in primary and secondary school age (Collins-Thompson and Callan, 2004). $$$$$ An example of the set of log-likelihoods calculated across all 12 grade models, with a maximum point clearly evident, is shown in Figure 2.
The latter has been found to be more effective as the former when approaching the reading level of subjects in primary and secondary school age (Collins-Thompson and Callan, 2004). $$$$$ First, rather than choosing the single most likely grade language model, we calculate the average grade level of the top N results, weighted by the relative differences in likelihood (essentially the expected class).

Collins-Thompson and Callan (2004) adopted a similar approach and used a smoothed unigram model to predict the grade levels of short passages and web documents. $$$$$ All results reported here use this averaging method, with N=2.
Collins-Thompson and Callan (2004) adopted a similar approach and used a smoothed unigram model to predict the grade levels of short passages and web documents. $$$$$ For comparison, using the type/token ratio gave a mean correlation of 0.48.
Collins-Thompson and Callan (2004) adopted a similar approach and used a smoothed unigram model to predict the grade levels of short passages and web documents. $$$$$ We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures.
Collins-Thompson and Callan (2004) adopted a similar approach and used a smoothed unigram model to predict the grade levels of short passages and web documents. $$$$$ The word ‘the’ is very common and varies less in frequency across grade levels.

First, a language modeling approach generally gives much better accuracy for Web documents and short passages (Collins-Thompson and Callan, 2004). $$$$$ It was also unknown how much training data would be required to get good vocabulary coverage on Web data.
First, a language modeling approach generally gives much better accuracy for Web documents and short passages (Collins-Thompson and Callan, 2004). $$$$$ While simple methods like modified naïve Bayes give reasonably good results, more sophisticated techniques may give more accurate predictions, especially at lower grades, where vocabulary progress is measured in months, not years.
First, a language modeling approach generally gives much better accuracy for Web documents and short passages (Collins-Thompson and Callan, 2004). $$$$$ There are numerous commercial reading comprehension tests available that have graded passages, but this would have reduced the emphasis we wanted on Web documents.

However, there are other important criteria for the user besides relevance, such as readability (Collins-Thompson and Callan, 2004), novelty (Harman, 2003), and authority (Kleinberg, 1998). $$$$$ Statistical models of text rely on training data, so in Section 2 we describe our Web training corpus and note some trends that are evident in word usage.
However, there are other important criteria for the user besides relevance, such as readability (Collins-Thompson and Callan, 2004), novelty (Harman, 2003), and authority (Kleinberg, 1998). $$$$$ While traditional formulas are based on linear regression with two or three variables, statistical language models can capture more detailed patterns of individual word usage.
However, there are other important criteria for the user besides relevance, such as readability (Collins-Thompson and Callan, 2004), novelty (Harman, 2003), and authority (Kleinberg, 1998). $$$$$ Chall, 1983, p. 63) have also observed that concrete words like ‘red’ become less likely in higher grades.

 $$$$$ Their work was limited to a single subject domain - science - and three broad ranges of difficulty.
 $$$$$ There is a significant body of work on readability that spans the last 70 years.
 $$$$$ Second, to account for vocabulary variation within longer documents, we partition the document text into passages of 100 tokens each.
 $$$$$ We thank the anonymous reviewers for their comments and Luo Si for helpful discussions.

Advanced NLP-based readability metrics developed so far typically deal with English, with a few attempts devoted to other languages, namely French (Collins-Thompson and Callan, 2004), Portuguese (Aluisio et al, 2010) and German (Bruck, 2008). $$$$$ There is a significant body of work on readability that spans the last 70 years.
Advanced NLP-based readability metrics developed so far typically deal with English, with a few attempts devoted to other languages, namely French (Collins-Thompson and Callan, 2004), Portuguese (Aluisio et al, 2010) and German (Bruck, 2008). $$$$$ While this data is less-rigorously graded, such material also greatly reduces the cost of creating a readability measure, making it easy to modify for specific tasks or populations.
Advanced NLP-based readability metrics developed so far typically deal with English, with a few attempts devoted to other languages, namely French (Collins-Thompson and Callan, 2004), Portuguese (Aluisio et al, 2010) and German (Bruck, 2008). $$$$$ The DRS files were noise-free.
Advanced NLP-based readability metrics developed so far typically deal with English, with a few attempts devoted to other languages, namely French (Collins-Thompson and Callan, 2004), Portuguese (Aluisio et al, 2010) and German (Bruck, 2008). $$$$$ We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling.

Collins-Thompson and Callan (2004) used a smoothed unigram language model to predict the grade reading levels of web page documents and short passages. $$$$$ All prediction methods used a 100-token window size. ative to a large English corpus.
Collins-Thompson and Callan (2004) used a smoothed unigram language model to predict the grade reading levels of web page documents and short passages. $$$$$ The word ‘the’ is very common and varies less in frequency across grade levels.
Collins-Thompson and Callan (2004) used a smoothed unigram language model to predict the grade reading levels of web page documents and short passages. $$$$$ Most readability formulas become unreliable for passages of less than 100 tokens (Fry 1990).

As a learning model, we use unigram language modelling introduced in (Collins-Thompson and Callan, 2004) to model the reading level of subjects in primary and secondary school. $$$$$ We also included a fourth predictor: the FleschKincaid score (Kincaid et al. 1975), which is a linear combination of the text’s average sentence length (in tokens), and the average number of syllables per token.
As a learning model, we use unigram language modelling introduced in (Collins-Thompson and Callan, 2004) to model the reading level of subjects in primary and secondary school. $$$$$ We do this for all language models, and select the one with maximum likelihood.

In some of the early works on statistical readability assessment, Si and Callan (2001) and Collins-Thompson and Callan (2004) reported the impact of using unigram language models to estimate the grade level of a given text. $$$$$ We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling.
In some of the early works on statistical readability assessment, Si and Callan (2001) and Collins-Thompson and Callan (2004) reported the impact of using unigram language models to estimate the grade level of a given text. $$$$$ In contrast, our model is not specific to any subject and uses 12 individual grade models trained on a greatly expanded training set.
In some of the early works on statistical readability assessment, Si and Callan (2001) and Collins-Thompson and Callan (2004) reported the impact of using unigram language models to estimate the grade level of a given text. $$$$$ We investigated scoring each remaining type based on its estimated ability to predict (positively or negatively) a particular grade.

The widely used Flesch-Kincaid Grade Level index is based on the average number of syllables per word and the average sentence length in a passage of text (Kincaid et al, 1975) (as cited in (Collins-Thompson and Callan, 2004)). $$$$$ The model we present below may be thought of as a generalization of the vocabulary-based approach, in which we build multiple language models - in this study, one for each grade - that capture more fine-grained information about vocabulary usage.
The widely used Flesch-Kincaid Grade Level index is based on the average number of syllables per word and the average sentence length in a passage of text (Kincaid et al, 1975) (as cited in (Collins-Thompson and Callan, 2004)). $$$$$ We derive a measure based on an extension of multinomial naïve Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage.
The widely used Flesch-Kincaid Grade Level index is based on the average number of syllables per word and the average sentence length in a passage of text (Kincaid et al, 1975) (as cited in (Collins-Thompson and Callan, 2004)). $$$$$ The model we present below may be thought of as a generalization of the vocabulary-based approach, in which we build multiple language models - in this study, one for each grade - that capture more fine-grained information about vocabulary usage.

classifier to better capture the variance in word usage across grade levels (Collins-Thompson and Callan, 2004). $$$$$ We used two methods for assessing how well our classifier generalizes beyond the Web training data.
classifier to better capture the variance in word usage across grade levels (Collins-Thompson and Callan, 2004). $$$$$ There are numerous commercial reading comprehension tests available that have graded passages, but this would have reduced the emphasis we wanted on Web documents.
classifier to better capture the variance in word usage across grade levels (Collins-Thompson and Callan, 2004). $$$$$ Because we smooth across grade models, we perform a modified version of this step, removing from all models any types occurring less than 3 times in the entire corpus.
classifier to better capture the variance in word usage across grade levels (Collins-Thompson and Callan, 2004). $$$$$ Earlier researchers (e.g.

Other related work has used models of vocabulary (Collins-Thompson and Callan, 2004). $$$$$ With training, we found the optimal kernel width to be 2.5 grade levels.
Other related work has used models of vocabulary (Collins-Thompson and Callan, 2004). $$$$$ With training, we found the optimal kernel width to be 2.5 grade levels.
Other related work has used models of vocabulary (Collins-Thompson and Callan, 2004). $$$$$ As an example of retraining, we showed that the classifier obtained good correlation with difficulty for at least two languages, English and French, with the only algorithm difference being a change in the morphology handling during feature processing.
Other related work has used models of vocabulary (Collins-Thompson and Callan, 2004). $$$$$ First, smoothing across classes greatly reduces the training data required for individual grade models.

Early work on automatic readability analysis framed the problem as a classification task: creating multiple classifiers for labeling a text as being one of several elementary school grade levels (Collins-Thompson and Callan, 2004). $$$$$ Further study is required to explore just how much this model of vocabulary usage can be generalized to other languages.
Early work on automatic readability analysis framed the problem as a classification task: creating multiple classifiers for labeling a text as being one of several elementary school grade levels (Collins-Thompson and Callan, 2004). $$$$$ We do this for all language models, and select the one with maximum likelihood.
Early work on automatic readability analysis framed the problem as a classification task: creating multiple classifiers for labeling a text as being one of several elementary school grade levels (Collins-Thompson and Callan, 2004). $$$$$ By ‘borrowing’ word frequency data from nearby grades, the effective number of types for each grade model is multiplied by a factor of five or more.
Early work on automatic readability analysis framed the problem as a classification task: creating multiple classifiers for labeling a text as being one of several elementary school grade levels (Collins-Thompson and Callan, 2004). $$$$$ With Web applications, it is not uncommon for samples to contain as few as 10 tokens or less.

It is also the last formula published for French L1, if we except the adaptation of the model by Collins-Thompson and Callan (2004) to French. $$$$$ We derive a measure based on an extension of multinomial naïve Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage.
It is also the last formula published for French L1, if we except the adaptation of the model by Collins-Thompson and Callan (2004) to French. $$$$$ We show that with minimal changes, the classifier may be retrained for use with French Web documents.
It is also the last formula published for French L1, if we except the adaptation of the model by Collins-Thompson and Callan (2004) to French. $$$$$ Our statistical model is based on a variation of the multinomial naïve Bayes classifier, which we call the ‘Smoothed Unigram’ model.
It is also the last formula published for French L1, if we except the adaptation of the model by Collins-Thompson and Callan (2004) to French. $$$$$ Any opinions, findings, conclusions, or recommendations expressed in this material are the authors’, and do not necessarily reflect those of the sponsors.
