In bitext parsing, we can use the information based on "bilingual constraints" (Burkett and Klein, 2008), which do not exist in monolingual sentences. $$$$$ As MT motivates this work, another valuable evaluation is the effect of joint selection on downstream MT quality.
In bitext parsing, we can use the information based on "bilingual constraints" (Burkett and Klein, 2008), which do not exist in monolingual sentences. $$$$$ We consider a set of core features which capture the scores of monolingual parsers as well as measures of syntactic alignment.
In bitext parsing, we can use the information based on "bilingual constraints" (Burkett and Klein, 2008), which do not exist in monolingual sentences. $$$$$ We do not observe the alignments a which link these parses.
In bitext parsing, we can use the information based on "bilingual constraints" (Burkett and Klein, 2008), which do not exist in monolingual sentences. $$$$$ The articles were split into training, development, and test sets according to the standard breakdown for Chinese parsing evaluations.

Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. $$$$$ To capture basic monolingual parse quality, we begin with a single source and a single target feature whose values are the log likelihood of the source tree t and the target tree t', respectively, as given by our baseline monolingual parsers.
Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. $$$$$ These sentences have parse trees t (respectively t') taken from candidate sets T (T').
Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. $$$$$ (2006), which extracts tree-to-string transducer rules based on target-side trees.

Following the studies of Burkett and Klein (2008), Huang et al. (2009) and Chen et al. (2010), we used the exact same data split $$$$$ For the latter evaluation, sentences that were not in the bilingual corpus were simply parsed with the baseline parsers.
Following the studies of Burkett and Klein (2008), Huang et al. (2009) and Chen et al. (2010), we used the exact same data split $$$$$ We selected 25 as it showed the best performance/speed tradeoff, on average performing as well as if we had done no pruning at all, while requiring only a quarter the memory and CPU time.
Following the studies of Burkett and Klein (2008), Huang et al. (2009) and Chen et al. (2010), we used the exact same data split $$$$$ Our model is a general log-linear (maximum entropy) distribution over triples (t, a, t') for sentence pairs (s, s'): Features are thus defined over (t, a, t') triples; we discuss specific features below.
Following the studies of Burkett and Klein (2008), Huang et al. (2009) and Chen et al. (2010), we used the exact same data split $$$$$ In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them.

 $$$$$ By jointly parsing (and aligning) sentences in a translation pair, it is possible to exploit mutual constraints that improve the quality of syntactic analyses over independent monolingual parsing.
 $$$$$ Similarly, the complement, the outside span, will be denoted o(n) (o(n')), and comprises the tokens not dominated by that node.
 $$$$$ The full training setup used the iterative training procedure on all 2298 training sentence pairs.
 $$$$$ The full training setup used the iterative training procedure on all 2298 training sentence pairs.

Burkett and Klein (2008) use the additional knowledge from Chinese-English parallel Treebank to improve Chinese parsing accuracy. $$$$$ We presented a joint log-linear model over source trees, target trees, and node-to-node alignments between them, which is used to select an optimal tree pair from a k-best list.
Burkett and Klein (2008) use the additional knowledge from Chinese-English parallel Treebank to improve Chinese parsing accuracy. $$$$$ Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables.

It also improves over the discriminative, bilingual parsing model of Burkett and Klein (2008), yielding the highest joint parsing F1 numbers on this data set. $$$$$ Unfortunately, parsing general bitexts well can be a challenge for newswiretrained treebank parsers for many reasons, including out-of-domain input and tokenization issues.
It also improves over the discriminative, bilingual parsing model of Burkett and Klein (2008), yielding the highest joint parsing F1 numbers on this data set. $$$$$ To test this limitation, we evaluated performance on the dev set using baseline k-best lists of varying length.
It also improves over the discriminative, bilingual parsing model of Burkett and Klein (2008), yielding the highest joint parsing F1 numbers on this data set. $$$$$ The geometric mean of span lengths was a superior measure of bispan “area” than the true area because word-level alignments tend to be broadly one-to-one in our word alignment model.

We compare our parsing results to the monolingual parsing models and to the English-Chinese bilingual reranker of Burkett and Klein (2008), trained on the same dataset. $$$$$ We also consider features that measure correspondences between the tree structures themselves.
We compare our parsing results to the monolingual parsing models and to the English-Chinese bilingual reranker of Burkett and Klein (2008), trained on the same dataset. $$$$$ Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables.
We compare our parsing results to the monolingual parsing models and to the English-Chinese bilingual reranker of Burkett and Klein (2008), trained on the same dataset. $$$$$ We show that jointly parsing a bitext can substantially improve parse quality on both sides.

In addition, our English parsing results are better than those of the Burkett and Klein (2008) bilingual reranker, the current top-performing English-Chinesebilingual parser, despite ours using a much simpler set of synchronization features. $$$$$ Then, we simply remove all tree pairs whose ranking falls below some empirically determined cutoff.
In addition, our English parsing results are better than those of the Burkett and Klein (2008) bilingual reranker, the current top-performing English-Chinesebilingual parser, despite ours using a much simpler set of synchronization features. $$$$$ Furthermore, by using this joint parsing technique to preprocess the input to a syntactic MT system, we obtain a 2.4 BLEU improvement.
In addition, our English parsing results are better than those of the Burkett and Klein (2008) bilingual reranker, the current top-performing English-Chinesebilingual parser, despite ours using a much simpler set of synchronization features. $$$$$ (2006), which extracts tree-to-string transducer rules based on target-side trees.

For example, Burkett and Klein (2008) show that parsing with joint models on bitexts improves performance on either or both sides. $$$$$ By jointly parsing (and aligning) sentences in a translation pair, it is possible to exploit mutual constraints that improve the quality of syntactic analyses over independent monolingual parsing.
For example, Burkett and Klein (2008) show that parsing with joint models on bitexts improves performance on either or both sides. $$$$$ In our model, we consider pairs of sentences (s, s'), where we use the convention that unprimed variables are source domain and primed variables are target domain.
For example, Burkett and Klein (2008) show that parsing with joint models on bitexts improves performance on either or both sides. $$$$$ In our model, we consider pairs of sentences (s, s'), where we use the convention that unprimed variables are source domain and primed variables are target domain.

We did not compare our system with the joint model of Burkett and Klein (2008) because they reported the results on phrase structures. $$$$$ We show that jointly parsing a bitext can substantially improve parse quality on both sides.

In bitext parsing, Burkett and Klein (2008) and Fraser et al (2009) used feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy, requiring translated tree banks. $$$$$ These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality.
In bitext parsing, Burkett and Klein (2008) and Fraser et al (2009) used feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy, requiring translated tree banks. $$$$$ These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality.
In bitext parsing, Burkett and Klein (2008) and Fraser et al (2009) used feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy, requiring translated tree banks. $$$$$ Similarly, there should be few alignments that violate that bispan.
In bitext parsing, Burkett and Klein (2008) and Fraser et al (2009) used feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy, requiring translated tree banks. $$$$$ The results are in Table 7, showing that joint parsing yields a BLEU increase of 2.4.9

 $$$$$ Note that we will also mention word alignments in feature definitions; a and the unqualified term alignment will always refer to node alignments.
 $$$$$ In our model, we consider pairs of sentences (s, s'), where we use the convention that unprimed variables are source domain and primed variables are target domain.
 $$$$$ We show that jointly parsing a bitext can substantially improve parse quality on both sides.
 $$$$$ On Chinese treebank data, this procedure improves F1 by 1.8 on Chinese sentences and by 2.5 on out-of-domain English sentences.

However, although it is not essentially different, we only focus on dependency parsing itself, while the parsing scheme in (Burkett and Klein, 2008) based on a constituent representation. $$$$$ Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables.
However, although it is not essentially different, we only focus on dependency parsing itself, while the parsing scheme in (Burkett and Klein, 2008) based on a constituent representation. $$$$$ As MT motivates this work, another valuable evaluation is the effect of joint selection on downstream MT quality.
However, although it is not essentially different, we only focus on dependency parsing itself, while the parsing scheme in (Burkett and Klein, 2008) based on a constituent representation. $$$$$ To verify that all our features were contributing to the model’s performance, we did an ablation study, removing one group of features at a time.
However, although it is not essentially different, we only focus on dependency parsing itself, while the parsing scheme in (Burkett and Klein, 2008) based on a constituent representation. $$$$$ Alignment features: Of course, some alignments are better than others.

 $$$$$ To test the impact of joint parsing on syntactic MT systems, we compared the results of training an MT system with two different sets of trees: those produced by the baseline parsers, and those produced by our joint parser.
 $$$$$ The resulting bitext parser outperforms state-of-the-art monoparser baselines by 2.5 predicting side trees and 1.8 predicting Chinese side trees (the highest published numbers on these corpora).
 $$$$$ For F1 evaluation, which is on a very small set of sentences, we selected 500 as the value with the best speed/performance tradeoff.
 $$$$$ This modified grammar was used to generate the k-best lists that we trained our model on.

For this task, we follow the setup of Burkett and Klein (2008), who improved Chinese and English monolingual parsers using parallel, hand-parsed text. $$$$$ Non-terminal nodes in trees will be denoted by n (n') and we abuse notation by equating trees with their node sets.
For this task, we follow the setup of Burkett and Klein (2008), who improved Chinese and English monolingual parsers using parallel, hand-parsed text. $$$$$ Formally, we present a log-linear model over triples of source trees, target trees, and node-tonode tree alignments between them.
For this task, we follow the setup of Burkett and Klein (2008), who improved Chinese and English monolingual parsers using parallel, hand-parsed text. $$$$$ We initialize the procedure by setting w0 as defined above.
For this task, we follow the setup of Burkett and Klein (2008), who improved Chinese and English monolingual parsers using parallel, hand-parsed text. $$$$$ Furthermore, by using this joint parsing technique to preprocess the input to a syntactic MT system, we obtain a 2.4 BLEU improvement.

Procedurally, our work is most closely related to that of Burkett and Klein (2008). $$$$$ In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them.
Procedurally, our work is most closely related to that of Burkett and Klein (2008). $$$$$ To compute such features, we define a(v, v') to be the posterior probability assigned to the word alignment between v and v' by an independent word aligner.2 Before defining alignment features, we need to define some additional variables.
Procedurally, our work is most closely related to that of Burkett and Klein (2008). $$$$$ By jointly parsing (and aligning) sentences in a translation pair, it is possible to exploit mutual constraints that improve the quality of syntactic analyses over independent monolingual parsing.
Procedurally, our work is most closely related to that of Burkett and Klein (2008). $$$$$ Unless otherwise specified, the maximum value of k was set to 100 for both training and testing, and all experiments used a value of 25 as the c parameter for training set pruning and a cutoff rank of 500 for test set pruning.

We parameterize the bilingual view using at most one-to-one matchings between nodes of structured labels in each language (Burkett and Klein, 2008). $$$$$ Words in a sentence are denoted by v (v').
We parameterize the bilingual view using at most one-to-one matchings between nodes of structured labels in each language (Burkett and Klein, 2008). $$$$$ Note that we will also mention word alignments in feature definitions; a and the unqualified term alignment will always refer to node alignments.
We parameterize the bilingual view using at most one-to-one matchings between nodes of structured labels in each language (Burkett and Klein, 2008). $$$$$ On Chinese treebank data, this procedure improves F1 by 1.8 on Chinese sentences and by 2.5 on out-of-domain English sentences.
We parameterize the bilingual view using at most one-to-one matchings between nodes of structured labels in each language (Burkett and Klein, 2008). $$$$$ Our model is a general log-linear (maximum entropy) distribution over triples (t, a, t') for sentence pairs (s, s'): Features are thus defined over (t, a, t') triples; we discuss specific features below.

We approximate this sum using the maximum-scoring matching (Burkett and Klein, 2008). $$$$$ These sentences have parse trees t (respectively t') taken from candidate sets T (T').
We approximate this sum using the maximum-scoring matching (Burkett and Klein, 2008). $$$$$ Again, for fixed alignments a, optimizing w is easy.
We approximate this sum using the maximum-scoring matching (Burkett and Klein, 2008). $$$$$ To make the process efficient and modular to existing monolingual parsers, we introduce several approximations: use of k-best lists in candidate generation, an adaptive bound to avoid considering all k2 combinations, and Viterbi approximations to alignment posteriors.

First, we use the word alignment density features from Burkett and Klein (2008), which measure how well the aligned entity pair matches up with alignments from an independent word aligner. $$$$$ To combat this, we use a simple pruning technique to limit the number of tree pairs under consideration.
First, we use the word alignment density features from Burkett and Klein (2008), which measure how well the aligned entity pair matches up with alignments from an independent word aligner. $$$$$ To compute such features, we define a(v, v') to be the posterior probability assigned to the word alignment between v and v' by an independent word aligner.2 Before defining alignment features, we need to define some additional variables.
First, we use the word alignment density features from Burkett and Klein (2008), which measure how well the aligned entity pair matches up with alignments from an independent word aligner. $$$$$ Posterior word alignment probabilities were obtained from the word aligner of Liang et al. (2006) and DeNero and Klein (2007)6, trained on approximately 1.7 million sentence pairs.

For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). $$$$$ We just find: Note that with no additional cost, we can also find the optimal alignment between t∗ and t0∗: Because the size of (T, T0) grows as O(k�), the time spent iterating through all these tree pairs can grow unreasonably long, particularly when reranking a set of sentence pairs the size of a typical MT corpus.
For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). $$$$$ Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables.
For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). $$$$$ Smith and Smith (2004) previously showed that such bilingual constraints can be leveraged to transfer parse quality from a resource-rich language to a resourceimpoverished one.
For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). $$$$$ In the rapid training setup, only 1000 sentence pairs from the training set were used, and we used fixed alignments for each tree pair rather than iterating (see §4.1).
