In bitext parsing, we can use the information based on "bilingual constraints" (Burkett and Klein, 2008), which do not exist in monolingual sentences. $$$$$ In our model, we consider pairs of sentences (s, s'), where we use the convention that unprimed variables are source domain and primed variables are target domain.
In bitext parsing, we can use the information based on "bilingual constraints" (Burkett and Klein, 2008), which do not exist in monolingual sentences. $$$$$ To facilitate relative comparison, the Moses (Koehn et al., 2007) number listed reflects the default Moses configuration, including its full distortion model, and standard training pipeline. guage model for decoding.
In bitext parsing, we can use the information based on "bilingual constraints" (Burkett and Klein, 2008), which do not exist in monolingual sentences. $$$$$ To use our model, we need features of a triple (t, a, t') which encode both the monolingual quality of the trees as well as the quality of the alignment between them.
In bitext parsing, we can use the information based on "bilingual constraints" (Burkett and Klein, 2008), which do not exist in monolingual sentences. $$$$$ To combat this, we use a simple pruning technique to limit the number of tree pairs under consideration.

Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. $$$$$ The articles were split into training, development, and test sets according to the standard breakdown for Chinese parsing evaluations.
Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. $$$$$ The resulting bitext parser outperforms state-of-the-art monoparser baselines by 2.5 predicting side trees and 1.8 predicting Chinese side trees (the highest published numbers on these corpora).
Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. $$$$$ The resulting bitext parser outperforms state-of-the-art monoparser baselines by 2.5 predicting side trees and 1.8 predicting Chinese side trees (the highest published numbers on these corpora).
Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. $$$$$ Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables.

Following the studies of Burkett and Klein (2008), Huang et al. (2009) and Chen et al. (2010), we used the exact same data split: 1-270 for training, 301-325 for development, and 271-300 for testing. $$$$$ Similarly, there should be few alignments that violate that bispan.
Following the studies of Burkett and Klein (2008), Huang et al. (2009) and Chen et al. (2010), we used the exact same data split: 1-270 for training, 301-325 for development, and 271-300 for testing. $$$$$ Features include monolingual parse scores and various measures of syntactic divergence.
Following the studies of Burkett and Klein (2008), Huang et al. (2009) and Chen et al. (2010), we used the exact same data split: 1-270 for training, 301-325 for development, and 271-300 for testing. $$$$$ (2006), which extracts tree-to-string transducer rules based on target-side trees.
Following the studies of Burkett and Klein (2008), Huang et al. (2009) and Chen et al. (2010), we used the exact same data split: 1-270 for training, 301-325 for development, and 271-300 for testing. $$$$$ Training parameters were fixed (full training setup with k = 100) and test set pruning was disabled for these experiments.

 $$$$$ Our model is a general log-linear (maximum entropy) distribution over triples (t, a, t') for sentence pairs (s, s'): Features are thus defined over (t, a, t') triples; we discuss specific features below.
 $$$$$ We used the English and Chinese parsers in Petrov and Klein (2007)5 to generate all k-best lists and as our evaluation baseline.
 $$$$$ Words in a sentence are denoted by v (v').

Burkett and Klein (2008) use the additional knowledge from Chinese-English parallel Treebank to improve Chinese parsing accuracy. $$$$$ Training parameters were fixed (full training setup with k = 100) and test set pruning was disabled for these experiments.
Burkett and Klein (2008) use the additional knowledge from Chinese-English parallel Treebank to improve Chinese parsing accuracy. $$$$$ Then, we simply remove all tree pairs whose ranking falls below some empirically determined cutoff.

It also improves over the discriminative, bilingual parsing model of Burkett and Klein (2008), yielding the highest joint parsing F1 numbers on this data set. $$$$$ We tuned and evaluated BLEU (Papineni et al., 2001) on separate held-out sets of sentences of up to length 40 from the same corpus.
It also improves over the discriminative, bilingual parsing model of Burkett and Klein (2008), yielding the highest joint parsing F1 numbers on this data set. $$$$$ Because all our features can be factored to individual node pairs, this can be done with the Hungarian algorithm in cubic time.4 Note that we do not enforce any kind of domination consistency in the matching: for example, the optimal alignment might in principle have the source root aligning to a target non-root and vice versa.
It also improves over the discriminative, bilingual parsing model of Burkett and Klein (2008), yielding the highest joint parsing F1 numbers on this data set. $$$$$ Furthermore, by using this joint parsing technique to preprocess the input to a syntactic MT system, we obtain a 2.4 BLEU improvement.
It also improves over the discriminative, bilingual parsing model of Burkett and Klein (2008), yielding the highest joint parsing F1 numbers on this data set. $$$$$ Furthermore, by using this joint parsing technique to preprocess the input to a syntactic MT system, we obtain a 2.4 BLEU improvement.

We compare our parsing results to the monolingual parsing models and to the English-Chinese bilingual reranker of Burkett and Klein (2008), trained on the same dataset. $$$$$ To facilitate relative comparison, the Moses (Koehn et al., 2007) number listed reflects the default Moses configuration, including its full distortion model, and standard training pipeline. guage model for decoding.
We compare our parsing results to the monolingual parsing models and to the English-Chinese bilingual reranker of Burkett and Klein (2008), trained on the same dataset. $$$$$ On the other hand, the presence of translation pairs offers a new source of information: bilingual constraints.

In addition, our English parsing results are better than those of the Burkett and Klein (2008) bilingual reranker, the current top-performing English-Chinesebilingual parser, despite ours using a much simpler set of synchronization features. $$$$$ These sentences have parse trees t (respectively t') taken from candidate sets T (T').
In addition, our English parsing results are better than those of the Burkett and Klein (2008) bilingual reranker, the current top-performing English-Chinesebilingual parser, despite ours using a much simpler set of synchronization features. $$$$$ By iterating, we perform the following optimization: Note that (4) is just (2) with summation replaced by maximization.
In addition, our English parsing results are better than those of the Burkett and Klein (2008) bilingual reranker, the current top-performing English-Chinesebilingual parser, despite ours using a much simpler set of synchronization features. $$$$$ When training, we replace the sum over all tree pairs in (T, T0) in the denominator of (6) with a sum over all tree pairs in (Tpruned, T0pruned).
In addition, our English parsing results are better than those of the Burkett and Klein (2008) bilingual reranker, the current top-performing English-Chinesebilingual parser, despite ours using a much simpler set of synchronization features. $$$$$ As we show in ยง6.3, by using this technique we are able to speed up reranking by a factor of almost 20 without an appreciable loss of performance.

For example, Burkett and Klein (2008) show that parsing with joint models on bitexts improves performance on either or both sides. $$$$$ Not all sentence pairs could be included for various reasons, including one-to-many Chinese-English sentence alignments, sentences omitted from the English translations, and low-fidelity translations.
For example, Burkett and Klein (2008) show that parsing with joint models on bitexts improves performance on either or both sides. $$$$$ One indicator of a good nodeto-node alignment between n and n' is that a good word alignment model thinks that there are many word-to-word alignments in their bispan.
For example, Burkett and Klein (2008) show that parsing with joint models on bitexts improves performance on either or both sides. $$$$$ To encode some of this information, we compute indicators of the number of children c that the nodes have in t and t'.

We did not compare our system with the joint model of Burkett and Klein (2008) because they reported the results on phrase structures. $$$$$ (2006), which extracts tree-to-string transducer rules based on target-side trees.
We did not compare our system with the joint model of Burkett and Klein (2008) because they reported the results on phrase structures. $$$$$ Note that with only these two features, little can be learned: all positive weights w cause the jointly optimal parse pair (t, t') to comprise the two top-1 monolingual outputs (the baseline).
We did not compare our system with the joint model of Burkett and Klein (2008) because they reported the results on phrase structures. $$$$$ Furthermore, by using this joint parsing technique to preprocess the input to a syntactic MT system, we obtain a 2.4 BLEU improvement.
We did not compare our system with the joint model of Burkett and Klein (2008) because they reported the results on phrase structures. $$$$$ In our model, we consider pairs of sentences (s, s'), where we use the convention that unprimed variables are source domain and primed variables are target domain.

In bitext parsing, Burkett and Klein (2008) and Fraser et al (2009) used feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy, requiring translated tree banks. $$$$$ For both English and Chinese, we begin with the state-of-the-art parsers presented in Petrov and Klein (2007) as a baseline.
In bitext parsing, Burkett and Klein (2008) and Fraser et al (2009) used feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy, requiring translated tree banks. $$$$$ Features include monolingual parse scores and various measures of syntactic divergence.
In bitext parsing, Burkett and Klein (2008) and Fraser et al (2009) used feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy, requiring translated tree banks. $$$$$ We had two training setups: rapid and full.
In bitext parsing, Burkett and Klein (2008) and Fraser et al (2009) used feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy, requiring translated tree banks. $$$$$ Because our bilingual data is from the Chinese treebank, and the data typically used to train a Chinese parser contains the Chinese side of our bilingual training data, we had to train a new Chinese grammar using only articles 400-1151 (omitting articles 1-270).

 $$$$$ By jointly parsing (and aligning) sentences in a translation pair, it is possible to exploit mutual constraints that improve the quality of syntactic analyses over independent monolingual parsing.
 $$$$$ Formally, we present a log-linear model over triples of source trees, target trees, and node-tonode tree alignments between them.
 $$$$$ Joint parsing improves F1 by 2.5 points on out-of-domain English sentences and by 1.8 points on in-domain Chinese sentences; this represents the best published Chinese treebank parsing performance, even after sentences that lack a translation are taken into account.
 $$$$$ We evaluate our system primarily as a parser and secondarily as a component in a machine translation pipeline.

However, although it is not essentially different, we only focus on dependency parsing itself, while the parsing scheme in (Burkett and Klein, 2008) based on a constituent representation. $$$$$ For this evaluation, we used a syntactic system based on Galley et al. (2004) and Galley et al.
However, although it is not essentially different, we only focus on dependency parsing itself, while the parsing scheme in (Burkett and Klein, 2008) based on a constituent representation. $$$$$ These two features are called SOURCELL and TARGETLL respectively.
However, although it is not essentially different, we only focus on dependency parsing itself, while the parsing scheme in (Burkett and Klein, 2008) based on a constituent representation. $$$$$ On Chinese treebank data, this procedure improves F1 by 1.8 on Chinese sentences and by 2.5 on out-of-domain English sentences.

 $$$$$ Though we do not know of any guarantees for this EM-like algorithm, in practice it converges after a few iterations given sufficient training data.
 $$$$$ On Chinese treebank data, this procedure improves F1 by 1.8 on Chinese sentences and by 2.5 on out-of-domain English sentences.
 $$$$$ To facilitate relative comparison, the Moses (Koehn et al., 2007) number listed reflects the default Moses configuration, including its full distortion model, and standard training pipeline. guage model for decoding.

For this task, we follow the setup of Burkett and Klein (2008), who improved Chinese and English monolingual parsers using parallel, hand-parsed text. $$$$$ Joint parsing improves F1 by 2.5 points on out-of-domain English sentences and by 1.8 points on in-domain Chinese sentences; this represents the best published Chinese treebank parsing performance, even after sentences that lack a translation are taken into account.
For this task, we follow the setup of Burkett and Klein (2008), who improved Chinese and English monolingual parsers using parallel, hand-parsed text. $$$$$ Furthermore, by using this joint parsing technique to preprocess the input to a syntactic MT system, we obtain a 2.4 BLEU improvement.
For this task, we follow the setup of Burkett and Klein (2008), who improved Chinese and English monolingual parsers using parallel, hand-parsed text. $$$$$ We presented a joint log-linear model over source trees, target trees, and node-to-node alignments between them, which is used to select an optimal tree pair from a k-best list.

Procedurally, our work is most closely related to that of Burkett and Klein (2008). $$$$$ We therefore define a scaled count which measures density rather than totals.
Procedurally, our work is most closely related to that of Burkett and Klein (2008). $$$$$ We presented a joint log-linear model over source trees, target trees, and node-to-node alignments between them, which is used to select an optimal tree pair from a k-best list.
Procedurally, our work is most closely related to that of Burkett and Klein (2008). $$$$$ The resulting bitext parser outperforms state-of-the-art monoparser baselines by 2.5 predicting side trees and 1.8 predicting Chinese side trees (the highest published numbers on these corpora).

We parameterize the bilingual view using at most one-to-one matchings between nodes of structured labels in each language (Burkett and Klein, 2008). $$$$$ For this evaluation, we used a syntactic system based on Galley et al. (2004) and Galley et al.
We parameterize the bilingual view using at most one-to-one matchings between nodes of structured labels in each language (Burkett and Klein, 2008). $$$$$ We do not observe the alignments a which link these parses.
We parameterize the bilingual view using at most one-to-one matchings between nodes of structured labels in each language (Burkett and Klein, 2008). $$$$$ (2006), which extracts tree-to-string transducer rules based on target-side trees.
We parameterize the bilingual view using at most one-to-one matchings between nodes of structured labels in each language (Burkett and Klein, 2008). $$$$$ We show that jointly parsing a bitext can substantially improve parse quality on both sides.

We approximate this sum using the maximum-scoring matching (Burkett and Klein, 2008). $$$$$ To combat this, we use a simple pruning technique to limit the number of tree pairs under consideration.
We approximate this sum using the maximum-scoring matching (Burkett and Klein, 2008). $$$$$ For this evaluation, we used a syntactic system based on Galley et al. (2004) and Galley et al.
We approximate this sum using the maximum-scoring matching (Burkett and Klein, 2008). $$$$$ On Chinese treebank data, this procedure improves F1 by 1.8 on Chinese sentences and by 2.5 on out-of-domain English sentences.
We approximate this sum using the maximum-scoring matching (Burkett and Klein, 2008). $$$$$ Our model is a general log-linear (maximum entropy) distribution over triples (t, a, t') for sentence pairs (s, s'): Features are thus defined over (t, a, t') triples; we discuss specific features below.

First, we use the word alignment density features from Burkett and Klein (2008), which measure how well the aligned entity pair matches up with alignments from an independent word aligner. $$$$$ As MT motivates this work, another valuable evaluation is the effect of joint selection on downstream MT quality.
First, we use the word alignment density features from Burkett and Klein (2008), which measure how well the aligned entity pair matches up with alignments from an independent word aligner. $$$$$ This bias allows the model to learn a general preference for denser alignments.
First, we use the word alignment density features from Burkett and Klein (2008), which measure how well the aligned entity pair matches up with alignments from an independent word aligner. $$$$$ Non-terminal nodes in trees will be denoted by n (n') and we abuse notation by equating trees with their node sets.

For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). $$$$$ Features include monolingual parse scores and various measures of syntactic divergence.
For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). $$$$$ (2006), which extracts tree-to-string transducer rules based on target-side trees.
For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). $$$$$ These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality.
For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). $$$$$ These sentences have parse trees t (respectively t') taken from candidate sets T (T').
