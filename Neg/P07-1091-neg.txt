Li et al (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents. $$$$$ Although there is no limit to the length of rewriting patterns, due to data sparseness most patterns being applied would be short ones.
Li et al (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents. $$$$$ chl, dozhang@microsoft.com mhli@insun.hit.edu.cn muli, mingzhou@microsoft.com guanyi@insun.hit.edu.cn Abstract Inspired by previous preprocessing approaches to SMT, this paper proposes a novel, probabilistic approach to reordering which combines the merits of syntax and phrase-based SMT.
Li et al (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents. $$$$$ Experiments show that, for the NIST MT05 task of Chinese-to-English translation, the proposal leads to BLEU improvement of 1.56%.
Li et al (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents. $$$$$ The case of 3-ary nodes is a bit more complicated as there are six.5 In general, an n-ary node has n! possible reorderings of its children.

More recently, Li et al (2007) use a maximum entropy system to learn reordering rules for binary trees (i.e., whether to keep or reorder for each node). $$$$$ On the other hand, local reordering is preserved and even strengthened in our approach.
More recently, Li et al (2007) use a maximum entropy system to learn reordering rules for binary trees (i.e., whether to keep or reorder for each node). $$$$$ However, long-distance reordering is problematic in phrase-based SMT.
More recently, Li et al (2007) use a maximum entropy system to learn reordering rules for binary trees (i.e., whether to keep or reorder for each node). $$$$$ In step 4 the best translation Tj is
More recently, Li et al (2007) use a maximum entropy system to learn reordering rules for binary trees (i.e., whether to keep or reorder for each node). $$$$$ Although there is no limit to the length of rewriting patterns, due to data sparseness most patterns being applied would be short ones.

This is one of the main problems addressed in the present work. (Li et al, 2007) use weighted n-best lists as in put for the decoder. $$$$$ Since a ranking of S' is needed, we need some way to score each S'.
This is one of the main problems addressed in the present work. (Li et al, 2007) use weighted n-best lists as in put for the decoder. $$$$$ To overcome this weakness, we suggest a method to ‘soften’ the hard decisions in preprocessing.
This is one of the main problems addressed in the present work. (Li et al, 2007) use weighted n-best lists as in put for the decoder. $$$$$ The next question is what kind of reordering knowledge can be formed out of these training instances.
This is one of the main problems addressed in the present work. (Li et al, 2007) use weighted n-best lists as in put for the decoder. $$$$$ After learning reordering knowledge, the preprocessing module can apply it to the parse tree, tS, of an SL sentence S and obtain the n-best list of S'.

A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ For example, the distancebased reordering model (Koehn et al., 2003) allows a decoder to translate in non-monotonous order, under the constraint that the distance between two phrases translated consecutively does not exceed a limit known as distortion limit.
A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ As pointed out in (Al-Onaizan and Papineni, 2006), these strategies make hard decisions in reordering which cannot be undone during decoding.
A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ On the one hand, global reordering, which cannot be accomplished by the phrase-based model, is enabled by the tree operations in preprocessing.
A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ The training data for both reordering knowledge and translation table is the one for NIST MT-2005.

In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (Zhang et al, 2007a) and the impact of syntax on reordering is difficult to single out (Li et al, 2007). $$$$$ Obviously it will greatly reduce the amount of training input.
In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (Zhang et al, 2007a) and the impact of syntax on reordering is difficult to single out (Li et al, 2007). $$$$$ However, long-distance reordering is problematic in phrase-based SMT.
In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (Zhang et al, 2007a) and the impact of syntax on reordering is difficult to single out (Li et al, 2007). $$$$$ Our second contribution is our definition of the best translation: where Fi are the features in the standard phrasebased model and Pr(S → S') is our new feature, viz. the probability of reordering S as S'.
In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (Zhang et al, 2007a) and the impact of syntax on reordering is difficult to single out (Li et al, 2007). $$$$$ For example, for the last two Chinese phrases in figure 1(a), simply swapping the two children of the NP node will produce the correct word order on the English side.

This paper follows the term convention of global reordering and local reordering of Li et al (2007), between which the distinction is solely defined by reordering distance (whether beyond four source words) (Li et al, 2007). $$$$$ It should be noted that the second Chinese block “R#, HI” and its English counterpart “at the end of” are not constituents at all.
This paper follows the term convention of global reordering and local reordering of Li et al (2007), between which the distinction is solely defined by reordering distance (whether beyond four source words) (Li et al, 2007). $$$$$ In sum, given a bilingual training corpus, a parser for the SL, and a word alignment tool, we can collect all binary parse tree nodes, each of which may be an instance of the required reordering knowledge.
This paper follows the term convention of global reordering and local reordering of Li et al (2007), between which the distinction is solely defined by reordering distance (whether beyond four source words) (Li et al, 2007). $$$$$ Given a source sentence and its parse tree, our method generates, tree operations, an list of reordered inputs, which are then fed to standard phrase-based decoder to produce the optimal translation.
This paper follows the term convention of global reordering and local reordering of Li et al (2007), between which the distinction is solely defined by reordering distance (whether beyond four source words) (Li et al, 2007). $$$$$ Note that T (ps) may contain words not in the set {ti}.

Rules created from a syntactic parser are also utilized to form weighted n-best lists which are fed into the decoder (Li etal., 2007). $$$$$ In this paper, our interest is the value of syntax in reordering, and the major statement is that syntactic information is useful in handling global reordering The lines and nodes on the top half of the figures show the phrase structure of the Chinese sentences, while the links on the bottom half of the figures show the alignments between Chinese and English phrases.
Rules created from a syntactic parser are also utilized to form weighted n-best lists which are fed into the decoder (Li etal., 2007). $$$$$ In order to conform to the log-linear model used in the decoder, we integrate the two factors by defining the total score of T as formula 3: The first term corresponds to the contribution of syntax-based reordering, while the second term that of the features Fi used in the decoder.
Rules created from a syntactic parser are also utilized to form weighted n-best lists which are fed into the decoder (Li etal., 2007). $$$$$ This method parses training data and uses some heuristics to align SL phrases with TL ones.
Rules created from a syntactic parser are also utilized to form weighted n-best lists which are fed into the decoder (Li etal., 2007). $$$$$ Experiments show that, for the NIST MT-05 task of Chinese-to- English translation, the proposal leads to BLEU improvement of 1.56%.

To overcome the problem, Li et al (2007) proposed k-best approach. $$$$$ The case of binary nodes is simple as there are only two possible reorderings.
To overcome the problem, Li et al (2007) proposed k-best approach. $$$$$ The data needed by our Pharaoh-like decoder are translation table and language model.
To overcome the problem, Li et al (2007) proposed k-best approach. $$$$$ The case of 3-ary nodes is a bit more complicated as there are six.5 In general, an n-ary node has n! possible reorderings of its children.
To overcome the problem, Li et al (2007) proposed k-best approach. $$$$$ In test (a), the constraint is that the module does not consider any reordering of a node if the yield of this node contains not more than four words.

A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ That is, our greedy reordering algorithm (c.f. section 4) has a tendency to focus only on a particular clause of a long sentence.
A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ Step 3(a)(ii) is trivial, but there is a subtle point about the calculation of language model score: the language model score of a translated clause is not independent from other clauses; it should take into account the last few words of the previous translated clause.
A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ In sum, given a bilingual training corpus, a parser for the SL, and a word alignment tool, we can collect all binary parse tree nodes, each of which may be an instance of the required reordering knowledge.

Li et al (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation. $$$$$ Let first(t) be the first word in this sorted set and last(t) be the last word.
Li et al (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation. $$$$$ The features that we investigated include the leftmost, rightmost, head, and context words4, and their POSs, of the SL phrases, as well as the phrase labels of the SL phrases and their parent.
Li et al (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation. $$$$$ For a phrase p yielded by a binary node N, whose left child N1 has reorderings pi1 and right child N2 has the reorderings p�2 (1 G i, j G n), p' has the form pi1pe or p�2pi1.
Li et al (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation. $$$$$ The case of binary nodes is simple as there are only two possible reorderings.

Li et al (2007) used N-best reordering hypotheses to overcome the reordering ambiguity. $$$$$ Our experiments are about Chinese-to-English translation.
Li et al (2007) used N-best reordering hypotheses to overcome the reordering ambiguity. $$$$$ The form of reordering rules, and the calculation of reordering probability for a particular node, can also be generalized easily.6 The only problem for the generalized reordering knowledge is that, as there are more classes, data sparseness becomes more severe.
Li et al (2007) used N-best reordering hypotheses to overcome the reordering ambiguity. $$$$$ The problem was remedied by modifying our decoder such that it no longer translates a sentence at once; instead the new decoder does: Step 1 is done by checking the parse tree if there are any IP or CP nodes7 immediately under the root node.
Li et al (2007) used N-best reordering hypotheses to overcome the reordering ambiguity. $$$$$ Experiments show that, for the NIST MT05 task of Chinese-to-English translation, the proposal leads to BLEU improvement of 1.56%.

Syntax-based SMT is better suited to cope with long-distance dependencies, however there also are problems, some of them originated from the linguistic motivation itself, incorrect parse trees, or reordering that might involve blocks that are not constituents (Li et al, 2007). $$$$$ The details of this model are elaborated in sections 3 to 6.
Syntax-based SMT is better suited to cope with long-distance dependencies, however there also are problems, some of them originated from the linguistic motivation itself, incorrect parse trees, or reordering that might involve blocks that are not constituents (Li et al, 2007). $$$$$ To generate the correct translation, a phrasebased decoder should, after translating the word “Ihn” as “increase”, jump to the last word “R 13T(investment)”.
Syntax-based SMT is better suited to cope with long-distance dependencies, however there also are problems, some of them originated from the linguistic motivation itself, incorrect parse trees, or reordering that might involve blocks that are not constituents (Li et al, 2007). $$$$$ Therefore, while short-distance reordering is under the scope of the distance-based model, long-distance reordering is simply out of the question.

 $$$$$ In order to conform to the log-linear model used in the decoder, we integrate the two factors by defining the total score of T as formula 3: The first term corresponds to the contribution of syntax-based reordering, while the second term that of the features Fi used in the decoder.
 $$$$$ The training data for both reordering knowledge and translation table is the one for NIST MT-2005.
 $$$$$ However, long-distance reordering is problematic in phrase-based SMT.
 $$$$$ It is not difficult to generalize the theory of reordering knowledge to nodes of other branching factors.

Li et al (2007) used a parser to get the syntactic tree of the source language sentence. $$$$$ However, some ideal translations exhibit reorderings longer than such distortion limit.
Li et al (2007) used a parser to get the syntactic tree of the source language sentence. $$$$$ In sum, given a bilingual training corpus, a parser for the SL, and a word alignment tool, we can collect all binary parse tree nodes, each of which may be an instance of the required reordering knowledge.
Li et al (2007) used a parser to get the syntactic tree of the source language sentence. $$$$$ Let us first look into the scoring of a particular reordering.
Li et al (2007) used a parser to get the syntactic tree of the source language sentence. $$$$$ It is not difficult to generalize the theory of reordering knowledge to nodes of other branching factors.
