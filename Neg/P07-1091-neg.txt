Li et al (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents. $$$$$ Therefore, Pr(p—*p') = The figures Pr(IN-ORDER) and Pr(INVERTED) are obtained from the learned reordering knowledge.
Li et al (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents. $$$$$ T(ps) is defined as the phrase first(t) ... last(t) in the TL sentence.
Li et al (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents. $$$$$ To prove it, we developed a hybrid approach which preserves the strength of phrase-based SMT in local reordering as well as the strength of syntax in global reordering.

More recently, Li et al (2007) use a maximum entropy system to learn reordering rules for binary trees (i.e., whether to keep or reorder for each node). $$$$$ This paper proposes a novel, probabilistic approach to reordering which combines the merits of syntax and phrase-based SMT.
More recently, Li et al (2007) use a maximum entropy system to learn reordering rules for binary trees (i.e., whether to keep or reorder for each node). $$$$$ The learned rewriting rules are then applied to rewrite SL sentences before monotonous translation.
More recently, Li et al (2007) use a maximum entropy system to learn reordering rules for binary trees (i.e., whether to keep or reorder for each node). $$$$$ Some local reordering model is still needed during decoding.
More recently, Li et al (2007) use a maximum entropy system to learn reordering rules for binary trees (i.e., whether to keep or reorder for each node). $$$$$ The 3ary nodes occupy a certain proportion of the distribution, and their impact on translation performance will be shown in our experiments.

This is one of the main problems addressed in the present work. (Li et al, 2007) use weighted n-best lists as in put for the decoder. $$$$$ Consider the sentence pair in NIST MT2005 test set shown in figure 1(a): after translating the word “•V/mend”, the decoder should ‘jump’ across six words and translate the last phrase “)� ,* R /fissures in the relationship”.
This is one of the main problems addressed in the present work. (Li et al, 2007) use weighted n-best lists as in put for the decoder. $$$$$ In step 4 the best translation Tj is
This is one of the main problems addressed in the present work. (Li et al, 2007) use weighted n-best lists as in put for the decoder. $$$$$ The maximum entropy model has the same form as in the binary case, except that there are more classes of reordering patterns as n increases.
This is one of the main problems addressed in the present work. (Li et al, 2007) use weighted n-best lists as in put for the decoder. $$$$$ Two forms of reordering knowledge are investigated: where Z is the phrase label of a binary node and X and Y are the phrase labels of Z’s children, and Pr(INVERTED) and Pr(IN-ORDER) are the probability that X and Y are inverted on TL side and that not inverted, respectively.

A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ This paper proposes a novel, probabilistic approach to reordering which combines the merits of syntax and phrase-based SMT.
A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ Given a source sentence and its parse tree, our method generates, tree operations, an list of reordered inputs, which are then fed to standard phrase-based decoder to produce the optimal translation.
A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ Consider the sentence pair in NIST MT2005 test set shown in figure 1(a): after translating the word “•V/mend”, the decoder should ‘jump’ across six words and translate the last phrase “)� ,* R /fissures in the relationship”.
A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ Since a ranking of S' is needed, we need some way to score each S'.

In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (Zhang et al, 2007a) and the impact of syntax on reordering is difficult to single out (Li et al, 2007). $$$$$ Therefore, while short-distance reordering is under the scope of the distance-based model, long-distance reordering is simply out of the question.
In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (Zhang et al, 2007a) and the impact of syntax on reordering is difficult to single out (Li et al, 2007). $$$$$ Starting from the leaves of tS, for each node N covering phrase p, we only keep track of the n p's that have the highest reordering probability.
In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (Zhang et al, 2007a) and the impact of syntax on reordering is difficult to single out (Li et al, 2007). $$$$$ If reordering knowledge is represented as rules, then the required probability is the probability associated with the rule that can apply to N. If reordering knowledge is represented as an ME model, then the required probability is: where r2lIN-ORDER, INVERTED}, and fi’s are features used in the ME model.

This paper follows the term convention of global reordering and local reordering of Li et al (2007), between which the distinction is solely defined by reordering distance (whether beyond four source words) (Li et al, 2007). $$$$$ If no, then the entire input is treated as one single clause.
This paper follows the term convention of global reordering and local reordering of Li et al (2007), between which the distinction is solely defined by reordering distance (whether beyond four source words) (Li et al, 2007). $$$$$ Obviously it will greatly reduce the amount of training input.

Rules created from a syntactic parser are also utilized to form weighted n-best lists which are fed into the decoder (Li etal., 2007). $$$$$ (Xia and McCord, 2004) propose a way to learn rewriting patterns, nevertheless the units of such patterns are words and their POSs.
Rules created from a syntactic parser are also utilized to form weighted n-best lists which are fed into the decoder (Li etal., 2007). $$$$$ Finally, test (c) shows that translation performance does not improve significantly by raising the number of reorderings.
Rules created from a syntactic parser are also utilized to form weighted n-best lists which are fed into the decoder (Li etal., 2007). $$$$$ The details of this model are elaborated in sections 3 to 6.

To overcome the problem, Li et al (2007) proposed k-best approach. $$$$$ If no, then the entire input is treated as one single clause.
To overcome the problem, Li et al (2007) proposed k-best approach. $$$$$ Given a source sentence and its parse tree, our method generates, tree operations, an list of reordered inputs, which are then fed to standard phrase-based decoder to produce the optimal translation.
To overcome the problem, Li et al (2007) proposed k-best approach. $$$$$ To avoid this problem, we give up using rewriting patterns and design a form of reordering knowledge which can be directly applied to parse tree nodes.
To overcome the problem, Li et al (2007) proposed k-best approach. $$$$$ Experiments show that, for the NIST MT05 task of Chinese-to-English translation, the proposal leads to BLEU improvement of 1.56%.

A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ In future we need to investigate their interaction and identify the contribution of each component.
A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ However, long-distance reordering is problematic in phrase-based SMT.
A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ The n-best reorderings of S, i.e. the n-best reorderings of the yield of the root node of tS, can be obtained by this efficient bottom-up method.

Li et al (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation. $$$$$ This is obviously out of the capability of the baseline model, and our approach can accomplish the desired reordering as expected.
Li et al (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation. $$$$$ To remedy data sparseness, less probable alignment points are removed so as to minimize overlapping phrases, since, after removing some alignment point, one of the TL phrases may become shorter and the two phrases may no longer overlap.
Li et al (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation. $$$$$ There have been various attempts to syntaxbased SMT, such as (Yamada and Knight, 2001) and (Quirk et al., 2005).

Li et al (2007) used N-best reordering hypotheses to overcome the reordering ambiguity. $$$$$ The settings and results of experiments on this new model are given in section 7.
Li et al (2007) used N-best reordering hypotheses to overcome the reordering ambiguity. $$$$$ Step 2 and step 3(a)(i) still follow the algorithm in section 4.
Li et al (2007) used N-best reordering hypotheses to overcome the reordering ambiguity. $$$$$ chl, dozhang@microsoft.com mhli@insun.hit.edu.cn muli, mingzhou@microsoft.com guanyi@insun.hit.edu.cn Abstract Inspired by previous preprocessing approaches to SMT, this paper proposes a novel, probabilistic approach to reordering which combines the merits of syntax and phrase-based SMT.

Syntax-based SMT is better suited to cope with long-distance dependencies, however there also are problems, some of them originated from the linguistic motivation itself, incorrect parse trees, or reordering that might involve blocks that are not constituents (Li et al, 2007). $$$$$ Despite the encouraging experiment results, it is still not very clear how the syntax-based and distance-based models complement each other in improving word reordering.
Syntax-based SMT is better suited to cope with long-distance dependencies, however there also are problems, some of them originated from the linguistic motivation itself, incorrect parse trees, or reordering that might involve blocks that are not constituents (Li et al, 2007). $$$$$ The form of reordering rules, and the calculation of reordering probability for a particular node, can also be generalized easily.6 The only problem for the generalized reordering knowledge is that, as there are more classes, data sparseness becomes more severe.
Syntax-based SMT is better suited to cope with long-distance dependencies, however there also are problems, some of them originated from the linguistic motivation itself, incorrect parse trees, or reordering that might involve blocks that are not constituents (Li et al, 2007). $$$$$ Step 2 and step 3(a)(i) still follow the algorithm in section 4.
Syntax-based SMT is better suited to cope with long-distance dependencies, however there also are problems, some of them originated from the linguistic motivation itself, incorrect parse trees, or reordering that might involve blocks that are not constituents (Li et al, 2007). $$$$$ The maximum entropy model has the same form as in the binary case, except that there are more classes of reordering patterns as n increases.

 $$$$$ (Collins et al., 2005) count on manual rules and it is suspicious if reordering rules for other language pairs can be easily made.
 $$$$$ Since a word alignment matrix usually contains a lot of noises as well as one-to-many and many-to-many alignments, two TL phrases may overlap with each other.
 $$$$$ It is not difficult to generalize the theory of reordering knowledge to nodes of other branching factors.
 $$$$$ Obviously it will greatly reduce the amount of training input.

Li et al (2007) used a parser to get the syntactic tree of the source language sentence. $$$$$ To prove it, we developed a hybrid approach which preserves the strength of phrase-based SMT in local reordering as well as the strength of syntax in global reordering.
Li et al (2007) used a parser to get the syntactic tree of the source language sentence. $$$$$ Maximum Entropy (ME) Model, which does the binary classification whether a binary node’s children are inverted or not, based on a set of features over the SL phrases corresponding to the two children nodes.
Li et al (2007) used a parser to get the syntactic tree of the source language sentence. $$$$$ In future we need to investigate their interaction and identify the contribution of each component.
