More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009). $$$$$ To our knowledge, algorithms for these problems have not been presented before.
More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009). $$$$$ We write T(e) to denote the set of antecedent nodes of a hyperedge e. We write I(v) for the hypergraph, a trigram language model is integrated.
More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009). $$$$$ Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs).
More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009). $$$$$ That is, we ask how they are affected when B changes slightly from its current value.

Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyper arcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). $$$$$ The reader can easily check these axioms (as well as the closure axioms in footnote 2).
Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyper arcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). $$$$$ We use these semirings in an open-source machine translation toolkit, enabling minimum-risk training a benefit of up to 1.0
Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyper arcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). $$$$$ The ⊕ operation is used to sum over all derivations d in the hypergraph to obtain the total weight of the hypergraph HG, which is � �eEd ke.2 Figure 2 shows how to dED compute the total weight of an acyclic hypergraph HG.3 In general, the total weight is a sum over exponentially many derivations d. But Figure 2 sums over these derivations in time only linear on the size of the hypergraph.
Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyper arcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). $$$$$ Tables 1–2 will not assume any commutativity. graph.

In order to learn the weights on the hyper arcs we perform the following procedure iteratively in an Em fashion (Li and Eisner, 2009). $$$$$ Below we compute gradient of entropy or Bayes risk.
In order to learn the weights on the hyper arcs we perform the following procedure iteratively in an Em fashion (Li and Eisner, 2009). $$$$$ This gives much more dynamic range than the 11-bit exponent of a 64-bit doubleprecision floating-point number, if vastly less than in Table 3.
In order to learn the weights on the hyper arcs we perform the following procedure iteratively in an Em fashion (Li and Eisner, 2009). $$$$$ This enables efficient computation of many interesting quantities over the exponentially many derivations encoded in a hypergraph: second derivatives (Hessians), expectations of products (covariances), and expectations such as risk and entropy along with their derivatives.

We converted the grammar into a hypergraph, and learned its probability distributions using a dynamic program similar to the inside-outside algorithm (Liand Eisner, 2009). $$$$$ To our knowledge, algorithms for these problems have not been presented before.
We converted the grammar into a hypergraph, and learned its probability distributions using a dynamic program similar to the inside-outside algorithm (Liand Eisner, 2009). $$$$$ This table shows the operations between two values a = sa2ea and b = sb2eb, assuming `a ≥ `b.
We converted the grammar into a hypergraph, and learned its probability distributions using a dynamic program similar to the inside-outside algorithm (Liand Eisner, 2009). $$$$$ The reader can easily check these axioms (as well as the closure axioms in footnote 2).

Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). $$$$$ Goodman (1999) describes many useful semirings (e.g., Viterbi, inside, and Viterbin-best).
Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). $$$$$ To compute equations (1)–(4) in this more general setting, we must still require multiplicative or additive decomposability, defining p(d) def = HeEd pe, r(d) def EeEd re, s(d) def EeEd se as before.
Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). $$$$$ The KL divergence to p from q can be computed as KL(p II q) = H(p, q) − H(p).
Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). $$$$$ This method computes the expectation of the gradient rather than the gradient of the expectation—they are equal. relied on the fact that this relationship still holds even when the scalars Z, are replaced by more complex objects that we wish to differentiate.

A generic first-order expectation semiring is also provided (Li and Eisner, 2009). $$$$$ The variance semiring is essential for many interesting training paradigms such as deterministic annealing (Rose, 1998), minimum risk (Smith and Eisner, 2006), active and semi-supervised learning (Grandvalet and Bengio, 2004; Jiao et al., 2006).
A generic first-order expectation semiring is also provided (Li and Eisner, 2009). $$$$$ Given a source sentence, Hiero uses a CKY parser to generate a hypergraph, encoding many derivation trees along with the translation strings.
A generic first-order expectation semiring is also provided (Li and Eisner, 2009). $$$$$ Many statistical translation models can be regarded as weighted logical deduction.
A generic first-order expectation semiring is also provided (Li and Eisner, 2009). $$$$$ It is often convenient to permit this probability distribution to be unnormalized, i.e., one may have to divide it through by some Z to get a proper distribution that sums to 1.

Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al, 2009). $$$$$ The linear coefficient ke is the “exclusive weight” for hyperedge e, meaning that the product keke is the total weight in K of all derivations d ∈ D that include e. of all derivations in the hypergraph), p is a measure over this space, and r, s : D — R are random variables.
Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al, 2009). $$$$$ Recall from Section 3.1 that this defines a probability distribution over all derivations d in the hypergraph, namely p(d)/Z where p(d) def = 11eEd pe.
Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al, 2009). $$$$$ We then propose a novel second-order expectation semiring, nicknamed the “variance semiring.” The original first-order expectation semiring allows us to efficiently compute a vector of firstorder statistics (expectations; first derivatives) on the set of paths in a lattice or the set of trees in a hypergraph.

As usual in natural language processing applications, we can exploit appropriate semirings and compute several useful statistical parameters through Mw (T?? T?), as for instance the highest weight of a computation, the inside probability and the rule expectations; see (Li and Eisner, 2009) for further discussion. $$$$$ The popular machine translation metric, BLEU (Papineni et al., 2001), is not additively decomposable, and thus we are not able to compute the expected loss for it.
As usual in natural language processing applications, we can exploit appropriate semirings and compute several useful statistical parameters through Mw (T?? T?), as for instance the highest weight of a computation, the inside probability and the rule expectations; see (Li and Eisner, 2009) for further discussion. $$$$$ In the first stage, we train a baseline system as usual.
As usual in natural language processing applications, we can exploit appropriate semirings and compute several useful statistical parameters through Mw (T?? T?), as for instance the highest weight of a computation, the inside probability and the rule expectations; see (Li and Eisner, 2009) for further discussion. $$$$$ We present details on how to compute many interesting quantities over the hypergraph using the expectation and variance semirings.
As usual in natural language processing applications, we can exploit appropriate semirings and compute several useful statistical parameters through Mw (T?? T?), as for instance the highest weight of a computation, the inside probability and the rule expectations; see (Li and Eisner, 2009) for further discussion. $$$$$ Tromble et al. (2008) develop the following loss function, of which a linear approximation to BLEU is a special case, provided that we define re = log pe (so that r(d) = EeEd re = log p(d)).

The sufficient statistics for graph expected BLEU can be computed using expectation semirings (Li and Eisner, 2009). $$$$$ Solving (14) for a given T requires computing the entropy H(p) and risk R(p) and their gradients with respect to θ and γ. Smith and Eisner (2006) followed MERT in constraining their decoder to only an n-best list, so for them, computing these quantities did not involve dynamic programming.
The sufficient statistics for graph expected BLEU can be computed using expectation semirings (Li and Eisner, 2009). $$$$$ In general, p may be an element of some other semiring, and r and s may be vectors or other algebraic objects.
The sufficient statistics for graph expected BLEU can be computed using expectation semirings (Li and Eisner, 2009). $$$$$ We then introduce novel semiring, which computes second-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy).

For inside-outside algorithm, see (Li and Eisner, 2009). $$$$$ Thus, the methods of this paper apply directly to the simpler case of hypothesis lattices as well.
For inside-outside algorithm, see (Li and Eisner, 2009). $$$$$ Thus, for generality, we conclude this section by stating the precise technical conditions needed to construct EP,R and EP,R,S,T: As a matter of notation, note that above and in Tables 1–2, we overload “+” to denote any of the addition operations within P, R, 5, T; overload “0” to denote their respective additive identities; and overload concatenation to denote any of the multiplication operations within or between P, R, S, T. “1” refers to the multiplicative identity of P. We continue to use distinguished symbols ®, ®, 0,1 for the operations and identities in our “main semiring of interest,” EP,R or EP,R,S,T .
For inside-outside algorithm, see (Li and Eisner, 2009). $$$$$ One might therefore wonder why the expectation semiring and its operations are still needed.
For inside-outside algorithm, see (Li and Eisner, 2009). $$$$$ The semirings can also be used for second-order gradient optimization algorithms.

The risk and its gradient on a hypergraph can be computed by using a second-order expectation semiring (Li and Eisner, 2009). $$$$$ We then introduce novel semiring, which computes second-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy).
The risk and its gradient on a hypergraph can be computed by using a second-order expectation semiring (Li and Eisner, 2009). $$$$$ Table 4 shows that it is indeed isomorphic to EP,R,S,T, with corresponding elements (p, r, s, t).
The risk and its gradient on a hypergraph can be computed by using a second-order expectation semiring (Li and Eisner, 2009). $$$$$ In fact, that We now observe that the second-order expectation semiring EP,R,S,T can be obtained indirectly by nesting one first-order expectation semiring inside another!
The risk and its gradient on a hypergraph can be computed by using a second-order expectation semiring (Li and Eisner, 2009). $$$$$ We used it practically to enable a new form of minimum-risk training that improved Chinese-English MT by 1.0 BLEU point.

There lative weights of these 10 features are tuned via hypergraph-based minimum risk training (Li and Eisner, 2009) on the bilingual data Set. $$$$$ When r and s are vectors, especially highdimensional vectors, the basic “inside algorithm” of Figure 2 will be slow.
There lative weights of these 10 features are tuned via hypergraph-based minimum risk training (Li and Eisner, 2009) on the bilingual data Set. $$$$$ At the end, we derive as a special case the well-known relationship between gradients and expectations in log-linear models.
There lative weights of these 10 features are tuned via hypergraph-based minimum risk training (Li and Eisner, 2009) on the bilingual data Set. $$$$$ The elementary values pe, re, se are now assumed to implicitly be functions of B.
There lative weights of these 10 features are tuned via hypergraph-based minimum risk training (Li and Eisner, 2009) on the bilingual data Set. $$$$$ We built a translation model on a corpus for IWSLT 2005 Chinese-to-English translation task (Eck and Hori, 2005), which consists of 40k pairs of sentences.

One is a variant of the F-B on the expectation semiring proposed in Li and Eisner (2009). $$$$$ Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs).
One is a variant of the F-B on the expectation semiring proposed in Li and Eisner (2009). $$$$$ The popular machine translation metric, BLEU (Papineni et al., 2001), is not additively decomposable, and thus we are not able to compute the expected loss for it.
One is a variant of the F-B on the expectation semiring proposed in Li and Eisner (2009). $$$$$ A hyperedge represents an SCFG rule that has been “instantiated” at a particular position, so that the nonterminals on the right and left sides have been replaced by particular antecedent and consequent items; this corresponds to storage of backpointers in the chart.
One is a variant of the F-B on the expectation semiring proposed in Li and Eisner (2009). $$$$$ Example 1: r(d) is the length of the translation corresponding to derivation d (arranged by setting re to the number of target-side terminal words in the SCFG rule associated with e).

For the detailed description, see Li and Eisner (2009) and its references. $$$$$ The second and third rows define the operations between two elements (p1, r1) and (p2, r2), and the last two rows define the identities.
For the detailed description, see Li and Eisner (2009) and its references. $$$$$ We used it practically to enable a new form of minimum-risk training that improved Chinese-English MT by 1.0 BLEU point.
For the detailed description, see Li and Eisner (2009) and its references. $$$$$ Our implementation will be released within the open-source MT toolkit Joshua (Li et al., 2009a).
For the detailed description, see Li and Eisner (2009) and its references. $$$$$ The ⊗ operation is used to obtain the weight of each derivation d by multiplying the weights of its component hyperedges e, that is, kd = ®eEd ke.

We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Liand Eisner, 2009). $$$$$ Therefore, we represent real numbers as ordered pairs.
We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Liand Eisner, 2009). $$$$$ Of course, we can compute (Z, r) as explained in Section 3.2.
We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Liand Eisner, 2009). $$$$$ MR without DA just fixes T = 0 and γ = 1 in (14).
We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Liand Eisner, 2009). $$$$$ Each derivation d is scored by where 4b(d) E RI is a vector of features of d. We then define the unnormalized distribution p(d) as where the scale factor γ adjusts how sharply the distribution favors the highest-scoring hypotheses.

We have implemented the hypergraph based minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. $$$$$ 16It is easy to verify that the gradient of a function f (e.g. entropy or risk) with respect to γ can be written as a weighted sum of gradients with respect to the feature weights θi, i.e.
We have implemented the hypergraph based minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. $$$$$ But in practice, the nodes’ inside weights ,3(v) are usually accumulated as the hypergraph is being built, so that pruning heuristics can consult them.
We have implemented the hypergraph based minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. $$$$$ Many statistical translation models can be regarded as weighted logical deduction.
We have implemented the hypergraph based minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. $$$$$ Recall from Section 3.1 that this defines a probability distribution over all derivations d in the hypergraph, namely p(d)/Z where p(d) def = 11eEd pe.

The work of Smith and Eisner was extended by Li and Eisner (2009) who were able to obtain much better estimates of feature expectations by using a packed chart instead of an n-best list. $$$$$ Our implementation will be released within the open-source MT toolkit Joshua (Li et al., 2009a).
The work of Smith and Eisner was extended by Li and Eisner (2009) who were able to obtain much better estimates of feature expectations by using a packed chart instead of an n-best list. $$$$$ All MR or MR+DA uses an approximated BLEU (Tromble et al., 2008) (for training only), while MERT uses the exact corpus BLEU in training.
The work of Smith and Eisner was extended by Li and Eisner (2009) who were able to obtain much better estimates of feature expectations by using a packed chart instead of an n-best list. $$$$$ We assume a hypergraph HG, which compactly encodes many derivation trees d E D. Given HG, we wish to extract the best derivations—or other aggregate properties of the forest of derivations.

Instead of n-best approximations, we may directly employ forests for a better conditional log-likelihood estimation (Li and Eisner,2009). $$$$$ The base cases are where d is a single hyperedge e, in which case (p(d), p(d)r(d)) = ke (thanks to our choice of ke), and where d is empty, in which case 5However, in a more tricky way, the second-order expectation semiring can be constructed using the first-order expectation semiring, as will be seen in Section 4.3. ing is a pair (p, r).
Instead of n-best approximations, we may directly employ forests for a better conditional log-likelihood estimation (Li and Eisner,2009). $$$$$ In Section 3, we show how to compute the expected hypothesis length or expected feature counts, using the algorithm of Figure 2 with a first-order expectation semiring ER,R.
Instead of n-best approximations, we may directly employ forests for a better conditional log-likelihood estimation (Li and Eisner,2009). $$$$$ But, INSIDE(HG, EK,X) could accomplish the same thing.

Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). $$$$$ Given a hypergraph HG whose hyperedges e are annotated with values pe.
Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). $$$$$ This second-order semiring is essential for many interesting training paradigms such as minimum risk, deterministic annealing, active learning, and semi-supervised learning, where gradient descent optimization requires computing the gradient of entropy or risk.
Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). $$$$$ Our approach is theoretically elegant, like other work in this vein (Goodman, 1999; Lopez, 2009; Gimpel and Smith, 2009).

This goal is different from the minimum risk training of Li and Eisner (2009) in a subtle but important way. $$$$$ And the linear coefficients ke, as well as ˆk, are computed entirely within the cheap semiring K. They are based on β and α values obtained by first running INSIDE(HG, K) and OUTSIDE(HG, K), which use only the ke part of the weights and ignore the more expensive xe.
This goal is different from the minimum risk training of Li and Eisner (2009) in a subtle but important way. $$$$$ We then introduce novel semiring, which computes second-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy).
This goal is different from the minimum risk training of Li and Eisner (2009) in a subtle but important way. $$$$$ The computations should be clear from earlier discussion.
This goal is different from the minimum risk training of Li and Eisner (2009) in a subtle but important way. $$$$$ To our knowledge, algorithms for these problems have not been presented before.
