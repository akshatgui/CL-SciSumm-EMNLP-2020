More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009). $$$$$ We use these semirings in an open-source machine translation toolkit, enabling minimum-risk training a benefit of up to 1.0
More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009). $$$$$ Rectangles represent items, where each item is identified by the non-terminal symbol, source span, and left- and right-side language model states.
More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009). $$$$$ Recall from Section 3.1 that this defines a probability distribution over all derivations d in the hypergraph, namely p(d)/Z where p(d) def = 11eEd pe.

Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyper arcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). $$$$$ Given a source sentence, Hiero uses a CKY parser to generate a hypergraph, encoding many derivation trees along with the translation strings.
Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyper arcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). $$$$$ But Li et al. (2009b, section 5.4) do estimate the gap between derivational and string entropies. where w is an n-gram type, N is a set of n-gram types with n E [1, 4], #w(y) is the number of occurrence of the n-gram w in y, δw(y*) is an indicator to check if y* contains at least one occurrence of w, and θn is the weight indicating the relative importance of an n-gram match.

In order to learn the weights on the hyper arcs we perform the following procedure iteratively in an Em fashion (Li and Eisner, 2009). $$$$$ By exploiting this sharing, a hypergraph can compactly represent exponentially many trees.
In order to learn the weights on the hyper arcs we perform the following procedure iteratively in an Em fashion (Li and Eisner, 2009). $$$$$ To our knowledge, algorithms for these problems have not been presented before.
In order to learn the weights on the hyper arcs we perform the following procedure iteratively in an Em fashion (Li and Eisner, 2009). $$$$$ It is noteworthy that the expectation semiring is not used at all by Figure 4.
In order to learn the weights on the hyper arcs we perform the following procedure iteratively in an Em fashion (Li and Eisner, 2009). $$$$$ In this paper, we apply the expectation semiring (Eisner, 2002) to a hypergraph (or packed forest) rather than just a lattice.

We converted the grammar into a hypergraph, and learned its probability distributions using a dynamic program similar to the inside-outside algorithm (Liand Eisner, 2009). $$$$$ To understand more specifically how (s, t) gets computed, observe in analogy to the end of Section 4.2 that 10Modulo the trivial isomorphism from ((p, r), (s, t)) to (p, r, s, t) (see Section 4.3), the intended semiring both here and in Case 3 is the one that was defined at the start of Section 4.1, in which r, s are vectors and their product is defined = (Z, VZ, VZ, V2Z) will be returned by INSIDE(HG, ER,R—,R—,R—x—), or more efficiently by INSIDE-OUTSIDE(HG, ER,R—, Rm x Rm×m).
We converted the grammar into a hypergraph, and learned its probability distributions using a dynamic program similar to the inside-outside algorithm (Liand Eisner, 2009). $$$$$ However, Eisner (2002, section 5) observes that this is inefficient when n is large.

Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). $$$$$ We then introduce novel semiring, which computes second-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy).
Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). $$$$$ This second-order semiring is essential for many interesting training paradigms such as minimum risk, deterministic annealing, active learning, and semi-supervised learning, where gradient descent optimization requires computing the gradient of entropy or risk.
Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). $$$$$ Define ke = (pe, pere).
Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). $$$$$ In this condition, we need our new secondorder semiring methods and must also approximate BLEU (during training only) by an additively decomposable loss (Tromble et al., 2008).15 Our algorithms require that p(d) of (13) is multiplicatively decomposable.

A generic first-order expectation semiring is also provided (Li and Eisner, 2009). $$$$$ In this paper, we apply the expectation semiring (Eisner, 2002) to a hypergraph (or packed forest) rather than just a lattice.
A generic first-order expectation semiring is also provided (Li and Eisner, 2009). $$$$$ This enables efficient computation of many interesting quantities over the exponentially many derivations encoded in a hypergraph: second derivatives (Hessians), expectations of products (covariances), and expectations such as risk and entropy along with their derivatives.
A generic first-order expectation semiring is also provided (Li and Eisner, 2009). $$$$$ Example 2: r(d) evaluates the loss of d compared to a reference translation, using some additively decomposable loss function.
A generic first-order expectation semiring is also provided (Li and Eisner, 2009). $$$$$ Cross-Entropy and KL Divergence We may be interested in computing the cross-entropy or KL divergence between two distributions p and q.

Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al, 2009). $$$$$ Why does inside-outside work?
Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al, 2009). $$$$$ Many statistical translation models can be regarded as weighted logical deduction.
Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al, 2009). $$$$$ To our knowledge, algorithms for these problems have not been presented before.
Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al, 2009). $$$$$ The second and third rows define the operations between two elements (p1, r1) and (p2, r2), and the last two rows define the identities.

As usual in natural language processing applications, we can exploit appropriate semirings and compute several useful statistical parameters through Mw (T?? T?), as for instance the highest weight of a computation, the inside probability and the rule expectations; see (Li and Eisner, 2009) for further discussion. $$$$$ Why does inside-outside work?
As usual in natural language processing applications, we can exploit appropriate semirings and compute several useful statistical parameters through Mw (T?? T?), as for instance the highest weight of a computation, the inside probability and the rule expectations; see (Li and Eisner, 2009) for further discussion. $$$$$ 4Actually, the notation ®eEd ke assumes that ® is commutative as well, as does the notation “for u E T(e)” in our algorithms; neither specifies a loop order.
As usual in natural language processing applications, we can exploit appropriate semirings and compute several useful statistical parameters through Mw (T?? T?), as for instance the highest weight of a computation, the inside probability and the rule expectations; see (Li and Eisner, 2009) for further discussion. $$$$$ Under the first-order expectation semiring ER,R-, the inside algorithm of Figure 2 will return (Z, r) where r is a vector of n feature expectations.

The sufficient statistics for graph expected BLEU can be computed using expectation semirings (Li and Eisner, 2009). $$$$$ Our implementation will be released within the open-source MT toolkit Joshua (Li et al., 2009a).
The sufficient statistics for graph expected BLEU can be computed using expectation semirings (Li and Eisner, 2009). $$$$$ However, DA’s improvement on the dev set did not transfer to the test set.
The sufficient statistics for graph expected BLEU can be computed using expectation semirings (Li and Eisner, 2009). $$$$$ Define ke = (pe, pere, pese, perese).

For inside-outside algorithm, see (Li and Eisner, 2009). $$$$$ To our knowledge, algorithms for these problems have not been presented before.
For inside-outside algorithm, see (Li and Eisner, 2009). $$$$$ The KL divergence to p from q can be computed as KL(p II q) = H(p, q) − H(p).
For inside-outside algorithm, see (Li and Eisner, 2009). $$$$$ As H(p) —* 0 (e.g., large γ), the Bayes risk does approach the MERT objective (i.e. minimizing 1-best error).The objective is minimize R(p) − T · H(p) (14) where the “temperature” T starts high and is explicitly decreased as optimization proceeds.

The risk and its gradient on a hypergraph can be computed by using a second-order expectation semiring (Li and Eisner, 2009). $$$$$ Our approach is theoretically elegant, like other work in this vein (Goodman, 1999; Lopez, 2009; Gimpel and Smith, 2009).
The risk and its gradient on a hypergraph can be computed by using a second-order expectation semiring (Li and Eisner, 2009). $$$$$ Below, we present our novel semirings.
The risk and its gradient on a hypergraph can be computed by using a second-order expectation semiring (Li and Eisner, 2009). $$$$$ The ⊕ operation is used to sum over all derivations d in the hypergraph to obtain the total weight of the hypergraph HG, which is � �eEd ke.2 Figure 2 shows how to dED compute the total weight of an acyclic hypergraph HG.3 In general, the total weight is a sum over exponentially many derivations d. But Figure 2 sums over these derivations in time only linear on the size of the hypergraph.
The risk and its gradient on a hypergraph can be computed by using a second-order expectation semiring (Li and Eisner, 2009). $$$$$ This may be mainly because MR or MR+DA uses an approximated BLEU while MERT doesn’t.

There lative weights of these 10 features are tuned via hypergraph-based minimum risk training (Li and Eisner, 2009) on the bilingual data Set. $$$$$ Many statistical translation models can be regarded as weighted logical deduction.
There lative weights of these 10 features are tuned via hypergraph-based minimum risk training (Li and Eisner, 2009) on the bilingual data Set. $$$$$ The semirings can also be used for second-order gradient optimization algorithms.
There lative weights of these 10 features are tuned via hypergraph-based minimum risk training (Li and Eisner, 2009) on the bilingual data Set. $$$$$ Then the second-order statistic t/Z is the second moment of the length distribution, so the variance of hypothesis length can be found as t/Z − (r/Z)2.
There lative weights of these 10 features are tuned via hypergraph-based minimum risk training (Li and Eisner, 2009) on the bilingual data Set. $$$$$ It turns out that these semirings can also compute first- and second-order partial derivatives of all the above results, with respect to a parameter vector B E Rm.

One is a variant of the F-B on the expectation semiring proposed in Li and Eisner (2009). $$$$$ MR without DA just fixes T = 0 and γ = 1 in (14).
One is a variant of the F-B on the expectation semiring proposed in Li and Eisner (2009). $$$$$ The application described at the start of this subsection is the classical inside-outside algorithm.
One is a variant of the F-B on the expectation semiring proposed in Li and Eisner (2009). $$$$$ Define ke = (pe, pere).
One is a variant of the F-B on the expectation semiring proposed in Li and Eisner (2009). $$$$$ In Hiero, a synchronous context-free grammar (SCFG) is extracted from automatically wordaligned corpora.

For the detailed description, see Li and Eisner (2009) and its references. $$$$$ We are also given two functions of interest r, s : D → R, each of which decomposes additively over its component hyperedges e: that is, r(d) def = EeEd re, and s(d) def = EeEd se.
For the detailed description, see Li and Eisner (2009) and its references. $$$$$ For both n-best and hypergraph, MR+DA did obtain a better BLEU score than plain MR on the dev set.18 This shows that DA helps with the local minimum problem, as hoped.

We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Liand Eisner, 2009). $$$$$ We present details on how to compute many interesting quantities over the hypergraph using the expectation and variance semirings.
We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Liand Eisner, 2009). $$$$$ It suffices to define 4b(d) def = Ee∈d 4be, so that all features are local to individual hyperedges; the vector 4be indicates which features fire on hyperedge e. Then score(d) of (12) is additively decomposable: We can then set pe = exp(γ · scoree), and Vpe = γpe4b(e), and use the algorithms described in Section 6 to compute H(p) and R(p) and their gradients with respect to θ and γ.16 15Pauls et al. (2009) concurrently developed a method to maximize the expected n-gram counts on a hypergraph using gradient descent.
We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Liand Eisner, 2009). $$$$$ This enables efficient computation of many interesting quantities over the exponentially many derivations encoded in a hypergraph: second derivatives (Hessians), expectations of products (covariances), and expectations such as risk and entropy along with their derivatives.

We have implemented the hypergraph based minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. $$$$$ Our implementation will be released within the open-source MT toolkit Joshua (Li et al., 2009a).
We have implemented the hypergraph based minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. $$$$$ This table shows the operations between two values a = sa2ea and b = sb2eb, assuming `a ≥ `b.
We have implemented the hypergraph based minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. $$$$$ For example, in the real semiring (Il2, +, x, 0, 1), we define p* = (1 − p)−1 (= 1 + p + p2 + ...) for |p |< 1 and is undefined otherwise.
We have implemented the hypergraph based minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. $$$$$ Semiring-weighted logic programming is a general framework to specify these algorithms (Pereira and Warren, 1983; Shieber et al., 1994; Goodman, 1999; Eisner et al., 2005; Lopez, 2009).

The work of Smith and Eisner was extended by Li and Eisner (2009) who were able to obtain much better estimates of feature expectations by using a packed chart instead of an n-best list. $$$$$ Semiring-weighted logic programming is a general framework to specify these algorithms (Pereira and Warren, 1983; Shieber et al., 1994; Goodman, 1999; Eisner et al., 2005; Lopez, 2009).
The work of Smith and Eisner was extended by Li and Eisner (2009) who were able to obtain much better estimates of feature expectations by using a packed chart instead of an n-best list. $$$$$ In the simplest case, let K = (R, +, x, 0, 1), and define ke = pe for each hyperedge e. Then the algorithm of Figure 2 reduces to the classical inside algorithm (Baker, 1979) and computes Z.
The work of Smith and Eisner was extended by Li and Eisner (2009) who were able to obtain much better estimates of feature expectations by using a packed chart instead of an n-best list. $$$$$ The last row in Table 5 reports the BLEU scores.

Instead of n-best approximations, we may directly employ forests for a better conditional log-likelihood estimation (Li and Eisner,2009). $$$$$ Case 3: Our experiments will need to find expectations and their partial derivatives.
Instead of n-best approximations, we may directly employ forests for a better conditional log-likelihood estimation (Li and Eisner,2009). $$$$$ On the test set, MR or MR+DA on an n-best list is comparable to MERT.
Instead of n-best approximations, we may directly employ forests for a better conditional log-likelihood estimation (Li and Eisner,2009). $$$$$ The root node corresponds to the goal item.

Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). $$$$$ For example, in the real semiring (Il2, +, x, 0, 1), we define p* = (1 − p)−1 (= 1 + p + p2 + ...) for |p |< 1 and is undefined otherwise.
Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). $$$$$ In general, p may be an element of some other semiring, and r and s may be vectors or other algebraic objects.

This goal is different from the minimum risk training of Li and Eisner (2009) in a subtle but important way. $$$$$ While most of these semirings are used in “testing” (i.e., decoding), we are mainly interested in the semirings that are useful for “training” (i.e., parameter estimation).
This goal is different from the minimum risk training of Li and Eisner (2009) in a subtle but important way. $$$$$ The first five rows in Table 5 present the results by tuning the weights offive features (θ ∈ R5).
This goal is different from the minimum risk training of Li and Eisner (2009) in a subtle but important way. $$$$$ We use these semirings in an open-source machine translation toolkit, enabling minimum-risk training a benefit of up to 1.0
