Chinese word segmentation is done by the Stanford Chinese segmenter (Chang et al, 2008). $$$$$ The training data for the segmenter is two orders of magnitude smaller than for the MT system, it is not terribly well matched to it in terms of genre and variety, and the information an MT system learns about alignment of Chinese to English might be the basis for a task appropriate segmentation style for Chinese-English MT.
Chinese word segmentation is done by the Stanford Chinese segmenter (Chang et al, 2008). $$$$$ On the test set MT05, λ0 = 2 yields 31.47 BLEU, which represents a quite large improvement compared to the unbiased segmenter (30.95 BLEU).
Chinese word segmentation is done by the Stanford Chinese segmenter (Chang et al, 2008). $$$$$ In Section 3.2, we will look at how the MT training and test data are segmented by each segmenter, and provide statistics and analysis for why certain segmenters are better than others.
Chinese word segmentation is done by the Stanford Chinese segmenter (Chang et al, 2008). $$$$$ In order to contrast with the simple maximum matching lexicon-based model (MaxMatch), we built another segmenter with a CRF model.

The most obvious example of this lies in languages that do not separate words with white space such as Chinese, Japanese, or Thai, in which the choice of a segmentation standard has a large effect on translation accuracy (Chang et al., 2008). $$$$$ The second advantage of having a segmenter like the lexicon-based MaxMatch is that it helps the reordering model.
The most obvious example of this lies in languages that do not separate words with white space such as Chinese, Japanese, or Thai, in which the choice of a segmentation standard has a large effect on translation accuracy (Chang et al., 2008). $$$$$ MaxMatch achieved 32.09 BLEU and CharBased achieved 30.28 BLEU, which shows that on the sentences where all elements are in vocabulary, there MaxMatch is still significantly better than CharBased.

It has been recognized that varying segmentation granularities are needed for SMT (Chang et al, 2008). $$$$$ Therefore, Hypothesis 1 is refuted.
It has been recognized that varying segmentation granularities are needed for SMT (Chang et al, 2008). $$$$$ In this paper, we investigated what segmentation properties can improve machine translation performance.
It has been recognized that varying segmentation granularities are needed for SMT (Chang et al, 2008). $$$$$ One way to improve the consistency of the CRF model is to make use of external lexicons (which are not part of the segmentation training data) to add lexicon-based features.
It has been recognized that varying segmentation granularities are needed for SMT (Chang et al, 2008). $$$$$ Thus it is useful to have word segmentation for MT.

All Chinese data was re-segmented with the CRF-based Stanford Chinese segmenter (Chang et al, 2008) that is trained on the segmentation of the Chinese Treebank for consistency. $$$$$ We built a CRF segmenter with all the features listed in Table 6 (CRF-Lex).
All Chinese data was re-segmented with the CRF-based Stanford Chinese segmenter (Chang et al, 2008) that is trained on the segmentation of the Chinese Treebank for consistency. $$$$$ We report the MT performance using the original BLEU metric (Papineni et al., 2001).
All Chinese data was re-segmented with the CRF-based Stanford Chinese segmenter (Chang et al, 2008) that is trained on the segmentation of the Chinese Treebank for consistency. $$$$$ (iii) Conversely, splitting OOV words into noncompositional subparts can be very harmful to an MT system: it is better to produce such OOV items than to split them into unrelated character sequences that are known to the system.
All Chinese data was re-segmented with the CRF-based Stanford Chinese segmenter (Chang et al, 2008) that is trained on the segmentation of the Chinese Treebank for consistency. $$$$$ Previous work has shown that Chinese word segmentation is useful for machine translation to English, yet the way different segmentation strategies affect MT is still poorly understood.

The Chinese text was segmented with a CRF-based Chinesesegmenter optimized for MT (Chang et al, 2008). $$$$$ We also show that improving segmentation consistency using external lexicon and proper noun features yields a 0.32 BLEU increase.
The Chinese text was segmented with a CRF-based Chinesesegmenter optimized for MT (Chang et al, 2008). $$$$$ There are 423,224 distinct entries in all the external lexicons.
The Chinese text was segmented with a CRF-based Chinesesegmenter optimized for MT (Chang et al, 2008). $$$$$ In this paper, we investigated what segmentation properties can improve machine translation performance.
The Chinese text was segmented with a CRF-based Chinesesegmenter optimized for MT (Chang et al, 2008). $$$$$ Further reducing the average number of characters per token yields gradual drops of performance until character-level segmentation (λ0 > 32, 29.36 BLEU).

Chinese words were automatically segmented with a conditional random field (CRF) classifier (Chang et al, 2008) that conforms to the Chinese Treebank (CTB) standard. $$$$$ With the 4-label extension, the OOV recall rate improved by 3.29%; while the IV recall rate stays the same.
Chinese words were automatically segmented with a conditional random field (CRF) classifier (Chang et al, 2008) that conforms to the Chinese Treebank (CTB) standard. $$$$$ To improve a feature-based sequence model for MT, we propose 4 different approaches to deal with named entities, optimal length of word for MT and joint search for segmentation and MT decoding.
Chinese words were automatically segmented with a conditional random field (CRF) classifier (Chang et al, 2008) that conforms to the Chinese Treebank (CTB) standard. $$$$$ We report the MT performance using the original BLEU metric (Papineni et al., 2001).
Chinese words were automatically segmented with a conditional random field (CRF) classifier (Chang et al, 2008) that conforms to the Chinese Treebank (CTB) standard. $$$$$ People generally assume that improvements in a system’s word segmentation accuracy will be monotonically reflected in overall system performance.

These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al, 2008). But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the tree bank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task. $$$$$ The linguistic features help capturing words that were unseen to the segmenter; while the lexicon-based features constrain the segmenter with external knowledge of what sequences are likely to be words.
These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al, 2008). But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the tree bank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task. $$$$$ The external lexicons we used for the lexicon-based features come from various sources including named entities collected from Wikipedia and the Chinese section of the UN website, named entities collected by Harbin Institute of Technology, the ADSO dictionary, EMM News Explorer, Online Chinese Tools, Online Dictionary from Peking University and HowNet.
These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al, 2008). But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the tree bank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task. $$$$$ Using an already competitive CRF segmentation model, we directly optimize segmentation granularity for translation quality, and obtain an improvement of 0.73 BLEU point on MT05 over our lexicon-based segmentation baseline.
These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al, 2008). But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the tree bank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task. $$$$$ There are three categories of features: character identity n-grams, morphological and character reduplication features.

Chang et al (2008) enhanced a CRF s segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence. $$$$$ In this paper, we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better MT performance.
Chang et al (2008) enhanced a CRF s segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence. $$$$$ We build several different models to address these issues and to improve segmentation for the benefit of MT.
Chang et al (2008) enhanced a CRF s segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence. $$$$$ The authors would like to thank Menqgiu Wang and Huihsin Tseng for useful discussions.
Chang et al (2008) enhanced a CRF s segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence. $$$$$ Unless these issues are attended to, simple baseline segmenters can be more effective inside an MT system than more complex machine learning based models, with much lower word segmentation error rate.

Chang et al (2008) described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge. $$$$$ We find that other factors such as segmentation consistency and granularity of Chinese “words” can be more important for machine translation.
Chang et al (2008) described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge. $$$$$ The training data contains 509K words, and the test data has 155K words.
Chang et al (2008) described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge. $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM.
Chang et al (2008) described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge. $$$$$ Second, we augment our CRF model with lexicon and proper noun features in order to improve segmentation consistency, which provide a 0.32 BLEU point improvement.

The optimal set of the model parameter values was found on dev MT to be k= 3, t AC= 0.0 and t COOC= 15. The comparison candidates also involve two popular off-the-shelf segmentation models $$$$$ Since the MT training data is subsampled with character n-grams, it is not biased towards any particular word segmentation.
The optimal set of the model parameter values was found on dev MT to be k= 3, t AC= 0.0 and t COOC= 15. The comparison candidates also involve two popular off-the-shelf segmentation models $$$$$ In Section 4 and 5 we describe how we tune a CRF model to fit the “word” granularity and also how we incorporate external lexicon and information about named entities for better MT performance.
The optimal set of the model parameter values was found on dev MT to be k= 3, t AC= 0.0 and t COOC= 15. The comparison candidates also involve two popular off-the-shelf segmentation models $$$$$ Chinese information retrieval (IR) systems benefit from a segmentation that breaks compound words into shorter “words” (Peng et al., 2002), paralleling the IR gains from compound splitting in languages like German (Hollink et al., 2004), whereas automatic speech recognition (ASR) systems prefer having longer words in the speech lexicon (Gao et al., 2005).
The optimal set of the model parameter values was found on dev MT to be k= 3, t AC= 0.0 and t COOC= 15. The comparison candidates also involve two popular off-the-shelf segmentation models $$$$$ During decoding, we incorporate the standard eight feature functions of Moses as well as the lexicalized reordering model.

Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. $$$$$ In this paper, we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better MT performance.
Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. $$$$$ In this paper, we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better MT performance.
Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. $$$$$ For each λ0 value, we ran an entire MT training and testing cycle, i.e., we re-segmented the entire training data, ran GIZA++, acquired phrasal translations that abide to this new segmentation, and ran MERT and evaluations on segmented data using the same 4Note that character-per-token averages provided in the table consider each non-Chinese word (e.g., foreign names, numbers) as one character, since our segmentation post-processing prevents these tokens from being segmented. tive bias values (λ0 = −2) slightly improves segmentation performance.
Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. $$$$$ (iv) Since the optimal granularity of words for phrase-based MT is unknown, we can benefit from a model which provides a knob for adjusting average word size.

The third and fourth tokenizations come from the CRF-based Stanford Chinese segmenter described by Chang et al (2008). $$$$$ In Section 3.1 we showed that a statistical sequence model with rich features can generalize better than maximum matching segmenters.
The third and fourth tokenizations come from the CRF-based Stanford Chinese segmenter described by Chang et al (2008). $$$$$ We restricted the Gigaword corpus to a subsample of 25 million sentences, because of memory constraints.
The third and fourth tokenizations come from the CRF-based Stanford Chinese segmenter described by Chang et al (2008). $$$$$ The authors would like to thank Menqgiu Wang and Huihsin Tseng for useful discussions.

This affirms our be lief that consistency in tokenization is important for machine translation, which was also mentioned by Chang et al (2008). $$$$$ The paper is organized as follows: we describe the experimental settings for the segmentation task and the task in Section 2.
This affirms our be lief that consistency in tokenization is important for machine translation, which was also mentioned by Chang et al (2008). $$$$$ In Section 3.1 we demonstrate that it is helpful to have word segmenters for MT, but that segmentation performance does not directly correlate with MT performance.
This affirms our be lief that consistency in tokenization is important for machine translation, which was also mentioned by Chang et al (2008). $$$$$ However, we show that this assumption is false: aspects of segmenters other than error rate are more critical to their performance when embedded in an MT system.
This affirms our be lief that consistency in tokenization is important for machine translation, which was also mentioned by Chang et al (2008). $$$$$ (iii) Conversely, splitting OOV words into noncompositional subparts can be very harmful to an MT system: it is better to produce such OOV items than to split them into unrelated character sequences that are known to the system.

The use of monolingual probabilistic models does not necessarily yield a better MT performance (Chang et al, 2008). $$$$$ Previous work has shown that Chinese word segmentation is useful for machine translation to English, yet the way different segmentation strategies affect MT is still poorly understood.
The use of monolingual probabilistic models does not necessarily yield a better MT performance (Chang et al, 2008). $$$$$ First, we found that neither character-based nor a standard word segmentation standard are optimal for MT, and show that an intermediate granularity is much more effective.
The use of monolingual probabilistic models does not necessarily yield a better MT performance (Chang et al, 2008). $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM.
The use of monolingual probabilistic models does not necessarily yield a better MT performance (Chang et al, 2008). $$$$$ On the MT performance, CRF-Lex-NR has a 0.32 BLEU gain on the test set MT05.

 $$$$$ In Table 4, we list some statistics of each segmenter to explain this phenomenon.
 $$$$$ Thus it is useful to have word segmentation for MT.
 $$$$$ In this paper, we investigated what segmentation properties can improve machine translation performance.
 $$$$$ Based on these findings, we implement methods inside a conditional random field segmenter that directly optimize segmentation granularity with respect to the MT task, providing an improvement of 0.73 BLEU.

we first segmented the sentences using the Stanford Chinese Word Segmenter (Chang et al, 2008). $$$$$ We chose the λ0 = 2 on another dev set (MT02).
we first segmented the sentences using the Stanford Chinese Word Segmenter (Chang et al, 2008). $$$$$ The authors would like to thank Menqgiu Wang and Huihsin Tseng for useful discussions.
we first segmented the sentences using the Stanford Chinese Word Segmenter (Chang et al, 2008). $$$$$ This is because the role of a phrase table is to build domain and application appropriate larger chunks that are semantically coherent in the translation process.

We tokenized the English with packages from the Stan ford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al, 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al, 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al, 2008) according to the Penn Chinese Treebank standard (Xue et al, 2005). $$$$$ (iii) Conversely, splitting OOV words into noncompositional subparts can be very harmful to an MT system: it is better to produce such OOV items than to split them into unrelated character sequences that are known to the system.
We tokenized the English with packages from the Stan ford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al, 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al, 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al, 2008) according to the Penn Chinese Treebank standard (Xue et al, 2005). $$$$$ The authors would like to thank Menqgiu Wang and Huihsin Tseng for useful discussions.
We tokenized the English with packages from the Stan ford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al, 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al, 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al, 2008) according to the Penn Chinese Treebank standard (Xue et al, 2005). $$$$$ While the latter finding is not particularly surprising, it further confirms that segmentation and MT evaluations can yield rather different outcomes.
We tokenized the English with packages from the Stan ford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al, 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al, 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al, 2008) according to the Penn Chinese Treebank standard (Xue et al, 2005). $$$$$ The MT training data was subsampled from GALE Year 2 training data using a collection of character 5-grams and smaller n-grams drawn from all segmentations of the test data.

For English corpora, the pre-processing are the same as that in (Qiu et al, 2009), and for Chinese corpora, the Stanford Word Segmenter (Changet al, 2008) is used to perform word segmentation. $$$$$ Second, we augment our CRF model with lexicon and proper noun features in order to improve segmentation consistency, which provide a 0.32 BLEU point improvement.
For English corpora, the pre-processing are the same as that in (Qiu et al, 2009), and for Chinese corpora, the Stanford Word Segmenter (Changet al, 2008) is used to perform word segmentation. $$$$$ To understand how each segmenter learns about OOV words, we will report the F measure, the in-vocabulary (IV) recall rate as well as OOV recall rate of each segmenter.
For English corpora, the pre-processing are the same as that in (Qiu et al, 2009), and for Chinese corpora, the Stanford Word Segmenter (Changet al, 2008) is used to perform word segmentation. $$$$$ Our linguistic features are adopted from (Ng and Low, 2004) and (Tseng et al., 2005).
For English corpora, the pre-processing are the same as that in (Qiu et al, 2009), and for Chinese corpora, the Stanford Word Segmenter (Changet al, 2008) is used to perform word segmentation. $$$$$ One way to improve the consistency of the CRF model is to make use of external lexicons (which are not part of the segmentation training data) to add lexicon-based features.

The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al, 2008), and the English text was parsed using the Stanford parser (Klein and Manning, 2003). $$$$$ Based on these findings, we implement methods inside a conditional random field segmenter that directly optimize segmentation granularity with respect to the MT task, providing an improvement of 0.73 BLEU.
The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al, 2008), and the English text was parsed using the Stanford parser (Klein and Manning, 2003). $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM.
The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al, 2008), and the English text was parsed using the Stanford parser (Klein and Manning, 2003). $$$$$ There are 423,224 distinct entries in all the external lexicons.

A variety of segmentation granularities, or atomic units, exist, including segmentations at the morpheme (e.g., Sirts and Alumae 2012), word (e.g., Chang et al 2008), sentence (e.g., Reynar and Ratnaparkhi 1997), and paragraph (e.g., Hearst 1997) levels. $$$$$ Previous work has shown that Chinese word segmentation is useful for machine translation to English, yet the way different segmentation strategies affect MT is still poorly understood.
A variety of segmentation granularities, or atomic units, exist, including segmentations at the morpheme (e.g., Sirts and Alumae 2012), word (e.g., Chang et al 2008), sentence (e.g., Reynar and Ratnaparkhi 1997), and paragraph (e.g., Hearst 1997) levels. $$$$$ To make a fairer comparison, we set the linear distortion limit in Moses to unlimited, removed the lexicalized reordering model, and retested both systems.
A variety of segmentation granularities, or atomic units, exist, including segmentations at the morpheme (e.g., Sirts and Alumae 2012), word (e.g., Chang et al 2008), sentence (e.g., Reynar and Ratnaparkhi 1997), and paragraph (e.g., Hearst 1997) levels. $$$$$ Having lexicon-based features reduced the MT training lexicon by 29.5%, reduced the MT test data OOV rate by 34.1%, and led to a 0.38 BLEU point gain on the test data (MT05).
A variety of segmentation granularities, or atomic units, exist, including segmentations at the morpheme (e.g., Sirts and Alumae 2012), word (e.g., Chang et al 2008), sentence (e.g., Reynar and Ratnaparkhi 1997), and paragraph (e.g., Hearst 1997) levels. $$$$$ As for MT performance, in Table 1 we see that having a segmenter, even as sim2Different phrase extraction heuristics might affect the results.
