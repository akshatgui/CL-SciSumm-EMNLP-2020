LSA Match: v and M (v) are distributionally similar according to a freely available Latent Semantic Indexing package,2 or for verbs similar according to VerbOcean (Chklovski and Pantel, 2004). $$$$$ Hence, verb semantics could help in many natural language processing (NLP) tasks that deal with events or relations between entities.
LSA Match: v and M (v) are distributionally similar according to a freely available Latent Semantic Indexing package,2 or for verbs similar according to VerbOcean (Chklovski and Pantel, 2004). $$$$$ Our approach extends previously formulated ones that use surface patterns as indicators of semantic relations between nouns (Hearst 1992; Etzioni 2003; Ravichandran and Hovy 2002).
LSA Match: v and M (v) are distributionally similar according to a freely available Latent Semantic Indexing package,2 or for verbs similar according to VerbOcean (Chklovski and Pantel, 2004). $$$$$ Also known as semantic opposition, antonymy between verbs has several distinct subtypes.

We will consider to use a supervised learning approach, as well as the similar features employed for temporal relation classification task, in addition to lexical information (e.g. WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004)) and the existing causal signals. $$$$$ Finally, as described in Section 4.3, we compare the strengths of the individual semantic relations and, preferring the most specific and then strongest relations, output a consistent set as the final output.
We will consider to use a supervised learning approach, as well as the similar features employed for temporal relation classification task, in addition to lexical information (e.g. WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004)) and the existing causal signals. $$$$$ The authors wish to thank the reviewers for their helpful comments and Google Inc. for supporting high volume querying of their index.
We will consider to use a supervised learning approach, as well as the similar features employed for temporal relation classification task, in addition to lexical information (e.g. WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004)) and the existing causal signals. $$$$$ Our approach has two stages.
We will consider to use a supervised learning approach, as well as the similar features employed for temporal relation classification task, in addition to lexical information (e.g. WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004)) and the existing causal signals. $$$$$ The authors wish to thank the reviewers for their helpful comments and Google Inc. for supporting high volume querying of their index.

For event terms, we first find the root verbs of deverbal nouns and then measure verb similarity by using the fine-grained relations provided by VerbOcean (Chklovski and Pantel, 2004), which has proved useful in summarization (Liu et al, 2007). $$$$$ Finally, as described in Section 4.3, we compare the strengths of the individual semantic relations and, preferring the most specific and then strongest relations, output a consistent set as the final output.
For event terms, we first find the root verbs of deverbal nouns and then measure verb similarity by using the fine-grained relations provided by VerbOcean (Chklovski and Pantel, 2004), which has proved useful in summarization (Liu et al, 2007). $$$$$ The authors wish to thank the reviewers for their helpful comments and Google Inc. for supporting high volume querying of their index.
For event terms, we first find the root verbs of deverbal nouns and then measure verb similarity by using the fine-grained relations provided by VerbOcean (Chklovski and Pantel, 2004), which has proved useful in summarization (Liu et al, 2007). $$$$$ On the same 1GB corpus described in Section 5.1, DIRT extracted over 200K paths and 6M unique paraphrases.
For event terms, we first find the root verbs of deverbal nouns and then measure verb similarity by using the fine-grained relations provided by VerbOcean (Chklovski and Pantel, 2004), which has proved useful in summarization (Liu et al, 2007). $$$$$ It would be useful to enforce global consistency, e.g.

In particular, we could emulate the approach used in VerbOcean (Chklovski and Pantel 2004). $$$$$ This subtype is exemplified by damage :: repair, wrap :: unwrap.
In particular, we could emulate the approach used in VerbOcean (Chklovski and Pantel 2004). $$$$$ Some instances of strength sometimes map to WordNet's troponymy relation.
In particular, we could emulate the approach used in VerbOcean (Chklovski and Pantel 2004). $$$$$ Strength.
In particular, we could emulate the approach used in VerbOcean (Chklovski and Pantel 2004). $$$$$ Strength.

The work on VerbOcean is similar to our research in the use of the Web for acquiring relationships (Chklovski and Pantel, 2004). $$$$$ This research was partly supported by NSF grant #EIA-0205111.
The work on VerbOcean is similar to our research in the use of the Web for acquiring relationships (Chklovski and Pantel, 2004). $$$$$ This relation holds between two verbs V1 and V2 when the pair can be glossed as V1 is accomplished by V2.
The work on VerbOcean is similar to our research in the use of the Web for acquiring relationships (Chklovski and Pantel, 2004). $$$$$ Our algorithm identifies six semantic relations between verbs.
The work on VerbOcean is similar to our research in the use of the Web for acquiring relationships (Chklovski and Pantel, 2004). $$$$$ On a set of 29,165 associated verb pairs, experimental results show an accuracy of 65.5% in assigning similarity, strength, antonymy, enablement, and happens-before.

Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs. $$$$$ We have demonstrated that certain fine-grained semantic relations between verbs are present on the Web, and are extractable with a simple patternbased approach.
Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs. $$$$$ We relied on the strongly associated verb pairs, described in Section 4.4, for computational efficiency.
Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs. $$$$$ Analysis of types shows that on the relation achieved 75% accuracy.
Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs. $$$$$ The authors wish to thank the reviewers for their helpful comments and Google Inc. for supporting high volume querying of their index.

For example, Chklovski and Pantel (2004) loosely define ENABLEMENT as a relation that holds between two verbs V1 and V2 when the pair can be glossed as V1 is accomplished by V2 and gives two examples: assess: :review and accomplish: :complete. $$$$$ We provide the called for download at
For example, Chklovski and Pantel (2004) loosely define ENABLEMENT as a relation that holds between two verbs V1 and V2 when the pair can be glossed as V1 is accomplished by V2 and gives two examples: assess: :review and accomplish: :complete. $$$$$ These provide an opportunity to create a much larger corpus of semantic relations, or to construct smaller, in-depth resources for selected subdomains.
For example, Chklovski and Pantel (2004) loosely define ENABLEMENT as a relation that holds between two verbs V1 and V2 when the pair can be glossed as V1 is accomplished by V2 and gives two examples: assess: :review and accomplish: :complete. $$$$$ This relation holds between two verbs V1 and V2 when the pair can be glossed as V1 is accomplished by V2.

Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hy ponyms (Hearst, 1992) ,meronyms (Girju, 2003), synonyms (Lin et al, 2003), a variety of verb relations (Chklovski and Pantel, 2004), and general purpose analogy relations (Turney et al, 2003). $$$$$ Strength.
Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hy ponyms (Hearst, 1992) ,meronyms (Girju, 2003), synonyms (Lin et al, 2003), a variety of verb relations (Chklovski and Pantel, 2004), and general purpose analogy relations (Turney et al, 2003). $$$$$ It contains 3200 verbs classified into 191 classes.
Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hy ponyms (Hearst, 1992) ,meronyms (Girju, 2003), synonyms (Lin et al, 2003), a variety of verb relations (Chklovski and Pantel, 2004), and general purpose analogy relations (Turney et al, 2003). $$$$$ On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5% accuracy.
Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hy ponyms (Hearst, 1992) ,meronyms (Girju, 2003), synonyms (Lin et al, 2003), a variety of verb relations (Chklovski and Pantel, 2004), and general purpose analogy relations (Turney et al, 2003). $$$$$ WordNet's cause relation, between a causative and a resultative verb (as in buy :: own), would be tagged as instances of happens-before by our system.

Chklovski and Pantel (2004) used patterns like 'x-ed by y-ing' ('obtained by borrowing') to get co-occurrence data on candidate pairs from the Web. $$$$$ If the system does not identify any semantic relation for a verb pair, then the system tags the pair as having no relation.
Chklovski and Pantel (2004) used patterns like 'x-ed by y-ing' ('obtained by borrowing') to get co-occurrence data on candidate pairs from the Web. $$$$$ While relying on a search engine allows us to query a corpus of nearly a trillion words, some issues arise: (i) the number of instances has to be approximated by the number of hits (documents); (ii) the number of hits for the same query may fluctuate over time; and (iii) some needed counts are not directly available.
Chklovski and Pantel (2004) used patterns like 'x-ed by y-ing' ('obtained by borrowing') to get co-occurrence data on candidate pairs from the Web. $$$$$ We use a total of 35 patterns, which are listed in Table 2 along with the estimated frequency of hits.
Chklovski and Pantel (2004) used patterns like 'x-ed by y-ing' ('obtained by borrowing') to get co-occurrence data on candidate pairs from the Web. $$$$$ WordNet does provide relations between verbs, but at a coarser level.

Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. $$$$$ The authors wish to thank the reviewers for their helpful comments and Google Inc. for supporting high volume querying of their index.
Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. $$$$$ We present a semi-automatic method for extracting fine-grained semantic relations between verbs.
Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. $$$$$ It contains 3200 verbs classified into 191 classes.
Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. $$$$$ Analysis of types shows that on the relation achieved 75% accuracy.

This metric allows to match synonym predicates by using verb ontologies such as VerbNet (Schuler, 2006) and VerbOcean (Chklovski and Pantel, 2004) and distributional semantics similarity metrics, such as Dekang Lin's thesaurus (Lin, 1998), where previous semantic metrics only perform exact match of predicate structures and arguments. $$$$$ Our approach extends previously formulated ones that use surface patterns as indicators of semantic relations between nouns (Hearst 1992; Etzioni 2003; Ravichandran and Hovy 2002).
This metric allows to match synonym predicates by using verb ontologies such as VerbNet (Schuler, 2006) and VerbOcean (Chklovski and Pantel, 2004) and distributional semantics similarity metrics, such as Dekang Lin's thesaurus (Lin, 1998), where previous semantic metrics only perform exact match of predicate structures and arguments. $$$$$ The adjudicators were asked to judge whether or not the system classification was acceptable (i.e. whether or not the relations output by the system were correct).
This metric allows to match synonym predicates by using verb ontologies such as VerbNet (Schuler, 2006) and VerbOcean (Chklovski and Pantel, 2004) and distributional semantics similarity metrics, such as Dekang Lin's thesaurus (Lin, 1998), where previous semantic metrics only perform exact match of predicate structures and arguments. $$$$$ In addition to discovering relations identified in WordNet, such as opposition and enablement, we obtain strong results on strength relations (for which no wide-coverage resource is available).

Work was also done on relations be tween verbs (Chklovski and Pantel, 2004). $$$$$ V1 stronger-than V2, and V2 strongerthan V3 indicates that V1 stronger-than V3, which may be leveraged to identify additional relations or inconsistent relations (e.g.
Work was also done on relations be tween verbs (Chklovski and Pantel, 2004). $$$$$ Further work may refine extraction methods and further process the mined semantics to derive other relations such as entailment.
Work was also done on relations be tween verbs (Chklovski and Pantel, 2004). $$$$$ For example, it may be valuable to know that if someone has bought an item, they may sell it at a later time.

Hence, when exploring very specific relation ship types or very generic, but not widely accepted, types (like verb strength), many researchers resort to manual human-based evaluation (Chklovski and Pantel, 2004). $$$$$ Hence, verb semantics could help in many natural language processing (NLP) tasks that deal with events or relations between entities.
Hence, when exploring very specific relation ship types or very generic, but not widely accepted, types (like verb strength), many researchers resort to manual human-based evaluation (Chklovski and Pantel, 2004). $$$$$ WordNet's cause relation, between a causative and a resultative verb (as in buy :: own), would be tagged as instances of happens-before by our system.
Hence, when exploring very specific relation ship types or very generic, but not widely accepted, types (like verb strength), many researchers resort to manual human-based evaluation (Chklovski and Pantel, 2004). $$$$$ For example, we could extract that take a trip to is similar to travel to, and that board a plane happens before deplane.

30 relations are noun compound relationships as proposed in the (Nastase and Szpakowicz, 2003 ) classification scheme, and 5 relations are verb-verb relations proposed by (Chklovski and Pantel, 2004). $$$$$ For tasks which require canonicalization of natural language statements or derivation of plausible inferences from such statements, a particularly valuable resource is one which (i) relates verbs to one another and (ii) provides broad coverage of the verbs in the target language.
30 relations are noun compound relationships as proposed in the (Nastase and Szpakowicz, 2003 ) classification scheme, and 5 relations are verb-verb relations proposed by (Chklovski and Pantel, 2004). $$$$$ This research was partly supported by NSF grant #EIA-0205111.
30 relations are noun compound relationships as proposed in the (Nastase and Szpakowicz, 2003 ) classification scheme, and 5 relations are verb-verb relations proposed by (Chklovski and Pantel, 2004). $$$$$ On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5% accuracy.
30 relations are noun compound relationships as proposed in the (Nastase and Szpakowicz, 2003 ) classification scheme, and 5 relations are verb-verb relations proposed by (Chklovski and Pantel, 2004). $$$$$ In this paper, we present an algorithm that semiautomatically discovers fine-grained verb semantics by querying the Web using simple lexicosyntactic patterns.

Other types of relations that have been studied by pattern-based approaches include question answer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al, 2003), general purpose analogy (Turney et al, 2003), verb relations (including similarity, strength, antonym, enable ment and temporal) (Chklovski and Pantel, 2004), entailment (Szpektor et al, 2004), and more specific relations, such as purpose, creation (Cimiano and Wenderoth, 2007), LivesIn, and EmployedBy (Bunescu and Mooney, 2007). $$$$$ They were refined to decrease capturing wrong parts of speech or incorrect semantic relations.
Other types of relations that have been studied by pattern-based approaches include question answer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al, 2003), general purpose analogy (Turney et al, 2003), verb relations (including similarity, strength, antonym, enable ment and temporal) (Chklovski and Pantel, 2004), entailment (Szpektor et al, 2004), and more specific relations, such as purpose, creation (Cimiano and Wenderoth, 2007), LivesIn, and EmployedBy (Bunescu and Mooney, 2007). $$$$$ Identifying these relations over 29,165 verb pairs results in a broad-coverage resource we call VERBOCEAN.
Other types of relations that have been studied by pattern-based approaches include question answer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al, 2003), general purpose analogy (Turney et al, 2003), verb relations (including similarity, strength, antonym, enable ment and temporal) (Chklovski and Pantel, 2004), entailment (Szpektor et al, 2004), and more specific relations, such as purpose, creation (Cimiano and Wenderoth, 2007), LivesIn, and EmployedBy (Bunescu and Mooney, 2007). $$$$$ To evaluate the accuracy of the system, we randomly sampled 100 of these verb pairs, and presented the classifications to two human judges.

We introduce VerbOcean (Chklovski and Pantel, 2004), a broad-coverage repository of semantic verb relations, into event-based summarization. $$$$$ WordNet does not include the relation &quot;X buys Y&quot; happens-before &quot;X sells Y&quot; since it is possible to sell something without having bought it (e.g. having manufactured or stolen it).
We introduce VerbOcean (Chklovski and Pantel, 2004), a broad-coverage repository of semantic verb relations, into event-based summarization. $$$$$ We extend these approaches in two ways: (i) our patterns indicate verb conjugation to increase their expressiveness and specificity and (ii) we use a measure similar to mutual information to account for both the frequency of the verbs whose semantic relations are being discovered as well as for the frequency of the pattern.
We introduce VerbOcean (Chklovski and Pantel, 2004), a broad-coverage repository of semantic verb relations, into event-based summarization. $$$$$ The authors wish to thank the reviewers for their helpful comments and Google Inc. for supporting high volume querying of their index.
We introduce VerbOcean (Chklovski and Pantel, 2004), a broad-coverage repository of semantic verb relations, into event-based summarization. $$$$$ Verbs are the primary vehicle for describing events and expressing relations between entities.

Chklovski and Pantel (2004) address the automatic acquisition of verb-verb pairs and their relations from the web. $$$$$ Our approach addresses verbs and provides for a richer and finer-grained set of semantics.
Chklovski and Pantel (2004) address the automatic acquisition of verb-verb pairs and their relations from the web. $$$$$ V1 stronger-than V2, and V2 strongerthan V3 indicates that V1 stronger-than V3, which may be leveraged to identify additional relations or inconsistent relations (e.g.
Chklovski and Pantel (2004) address the automatic acquisition of verb-verb pairs and their relations from the web. $$$$$ Further work may refine extraction methods and further process the mined semantics to derive other relations such as entailment.
Chklovski and Pantel (2004) address the automatic acquisition of verb-verb pairs and their relations from the web. $$$$$ This research was partly supported by NSF grant #EIA-0205111.

An ablation study that formed part of the official RTE 5 evaluation attempted to evaluate the contribution of publicly available knowledge resources such as WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), and DIRT (Lin and Pantel, 2001) used by many of the systems. $$$$$ This research was partly supported by NSF grant #EIA-0205111.
An ablation study that formed part of the official RTE 5 evaluation attempted to evaluate the contribution of publicly available knowledge resources such as WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), and DIRT (Lin and Pantel, 2001) used by many of the systems. $$$$$ In work on translating verbs with many counterparts in the target language, Palmer and Wu (1995) discuss inherent limitations of approaches which do not examine a verb's class membership, and put forth an approach based on verb similarity.
An ablation study that formed part of the official RTE 5 evaluation attempted to evaluate the contribution of publicly available knowledge resources such as WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), and DIRT (Lin and Pantel, 2001) used by many of the systems. $$$$$ However, DIRT only outputs pairs of paths that have some semantic relation.
An ablation study that formed part of the official RTE 5 evaluation attempted to evaluate the contribution of publicly available knowledge resources such as WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), and DIRT (Lin and Pantel, 2001) used by many of the systems. $$$$$ MindNet (Richardson et al. 1998) extracts a collection of triples of the type &quot;ducks have wings&quot; and &quot;duck capable-of flying&quot;.

The latter utilized several resources for matching hypothesis terms with text terms: WordNet, VerbOcean (Chklovski and Pantel, 2004), utilizing two of its relations, as well as an acronym database ,number matching module, co-reference resolution and named entity recognition tools. $$$$$ For example, it may be valuable to know that if someone has bought an item, they may sell it at a later time.
The latter utilized several resources for matching hypothesis terms with text terms: WordNet, VerbOcean (Chklovski and Pantel, 2004), utilizing two of its relations, as well as an acronym database ,number matching module, co-reference resolution and named entity recognition tools. $$$$$ These provide an opportunity to create a much larger corpus of semantic relations, or to construct smaller, in-depth resources for selected subdomains.
The latter utilized several resources for matching hypothesis terms with text terms: WordNet, VerbOcean (Chklovski and Pantel, 2004), utilizing two of its relations, as well as an acronym database ,number matching module, co-reference resolution and named entity recognition tools. $$$$$ If the entire database is viewed as a graph, we currently leverage and enforce only local consistency.

Chklovski and Pantel (2004) used patterns to extract a set of relations between verbs, such as similarity, strength and antonymy. $$$$$ For verbs, we need to estimate the frequency of the verbs, but avoid counting other parts-of-speech (e.g. chair as a noun or painted as an adjective).
Chklovski and Pantel (2004) used patterns to extract a set of relations between verbs, such as similarity, strength and antonymy. $$$$$ Examples extracted by our system include maximize :: enhance, produce :: create, reduce :: restrict.
Chklovski and Pantel (2004) used patterns to extract a set of relations between verbs, such as similarity, strength and antonymy. $$$$$ In discussing future work on the system's logical form matching component, Rus (2002 p. 143) points to incorporating entailment and causation verb relations to improve the matcher's performance.
Chklovski and Pantel (2004) used patterns to extract a set of relations between verbs, such as similarity, strength and antonymy. $$$$$ Finally, as discussed in Section 5.3, entailment relations may be derivable by processing the complete graph of the identified semantic relation.
