Finally, the work that is the closest to ours in spirit is the idea of joint estimation (Smith and Smith, 2004). $$$$$ This work was supported under a National Science Foundation Graduate Research Fellowship and a Fannie and John Hertz Foundation Fellowship.
Finally, the work that is the closest to ours in spirit is the idea of joint estimation (Smith and Smith, 2004). $$$$$ We did not do this, largely out of concerns over computational expense (see the discussion of translation models in §3.4).
Finally, the work that is the closest to ours in spirit is the idea of joint estimation (Smith and Smith, 2004). $$$$$ How might we exploit English parsers to improve syntactic analysis tools for this language?

More recently, Smith and Smith (2004) proposed to merge an English parser, a word alignment model, and a Korean PCFG parser trained from a small number of Korean parse trees under a unified log linear model. $$$$$ To get an upper bound on performance, we used the true parses from the English side of the KTB.
More recently, Smith and Smith (2004) proposed to merge an English parser, a word alignment model, and a Korean PCFG parser trained from a small number of Korean parse trees under a unified log linear model. $$$$$ §4 presents Korean parsing results with various monolingual and bilingual algorithms, including our bilingual parsing algorithm.
More recently, Smith and Smith (2004) proposed to merge an English parser, a word alignment model, and a Korean PCFG parser trained from a small number of Korean parse trees under a unified log linear model. $$$$$ We describe how simple, commonly understood statistical models, such as statistical dependency parsers, probabilistic context-free grammars, and word-to-word translation models, can be effectively combined into a unified bilingual parser that jointly searches for the best English parse, Korean parse, and word alignment, where these hidden structures all constrain each other.
More recently, Smith and Smith (2004) proposed to merge an English parser, a word alignment model, and a Korean PCFG parser trained from a small number of Korean parse trees under a unified log linear model. $$$$$ We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very limited labeled data.

We know of only one study which evaluates these bilingual grammar formalisms on the task of grammar induction itself (Smith and Smith, 2004). $$$$$ We showed small but significant improvements for Korean parsers trained on small amounts of labeled data.
We know of only one study which evaluates these bilingual grammar formalisms on the task of grammar induction itself (Smith and Smith, 2004). $$$$$ Since unary productions do not translate well from language to language (Hwa et al., 2002), we collapse them to their lower nodes.
We know of only one study which evaluates these bilingual grammar formalisms on the task of grammar induction itself (Smith and Smith, 2004). $$$$$ We now describe parameter estimation for the four component models that combine to make our full system (Table 1).

(Smith and Smith, 2004) tried to build a Korean parser by bilingual approach with English, and achieved labeled precision/recall around 40% for Korean. $$$$$ We showed small but significant improvements for Korean parsers trained on small amounts of labeled data.
(Smith and Smith, 2004) tried to build a Korean parser by bilingual approach with English, and achieved labeled precision/recall around 40% for Korean. $$$$$ We have presented a novel technique for merging simple, separately trained models for Korean parsing, English dependency parsing, and word translation, and optimizing the joint result using dynamic programming.
(Smith and Smith, 2004) tried to build a Korean parser by bilingual approach with English, and achieved labeled precision/recall around 40% for Korean. $$$$$ How might we exploit English parsers to improve syntactic analysis tools for this language?
(Smith and Smith, 2004) tried to build a Korean parser by bilingual approach with English, and achieved labeled precision/recall around 40% for Korean. $$$$$ Bold-faced numbers in the bilingual parsers indicate significant improvements on the PCFG baseline using the paired-sample t-test at the 0.01 level. tences.

Smith and Smith (2004) explore syntactic projection further by proposing an English-Korean bilingual parser integrated with a word translation model. $$$$$ Similarly, translation models, which yield word alignments, can be used in principle to score competing alignments and offer alternatives to a single-best alignment.
Smith and Smith (2004) explore syntactic projection further by proposing an English-Korean bilingual parser integrated with a word translation model. $$$$$ On small training datasets, this effect is positive: although good constituents are lost so that recall is poor compared to the PCFG, precision and crossing brackets are improved.
Smith and Smith (2004) explore syntactic projection further by proposing an English-Korean bilingual parser integrated with a word translation model. $$$$$ We showed small but significant improvements for Korean parsers trained on small amounts of labeled data.
Smith and Smith (2004) explore syntactic projection further by proposing an English-Korean bilingual parser integrated with a word translation model. $$$$$ Crossing brackets for the flattened SITG parses are understandably lower.

Smith and Smith (2004 ) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they consider a fixed combination of two parsers and a word aligner. $$$$$ We showed small but significant improvements for Korean parsers trained on small amounts of labeled data.
Smith and Smith (2004 ) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they consider a fixed combination of two parsers and a word aligner. $$$$$ Working on the Penn Korean Treebank, Sarkar and Han (2002) made a single training/test split and used 91% of the sentences to train a morphological disambiguator and lexicalized tree adjoining grammar (LTAG) based parsing system.
Smith and Smith (2004 ) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they consider a fixed combination of two parsers and a word aligner. $$$$$ While both of these formalisms require bilingual grammar rules, Eisner (2003) describes an algorithm for learning tree substitution grammars from unaligned trees.

 $$$$$ This work was supported under a National Science Foundation Graduate Research Fellowship and a Fannie and John Hertz Foundation Fellowship.
 $$$$$ In more formal work, Melamed (2003) proposes multitext grammars and algorithms for parsing them.
 $$$$$ By positing a coupling of English syntax with L syntax, we can induce structure on the L side of the parallel text that is in some sense isomorphic to the English parse.

Another distinct body of work addresses the problem of parser bootstrapping based on syntactic dependency projection (e.g. Hwa et al 2002), often using approaches based in synchronous parsing (e.g. Smith and Smith, 2004). $$$$$ The KTB also analyzes Korean words into their component morphemes and morpheme tags, which allowed us to train a morphological disambiguation model.
Another distinct body of work addresses the problem of parser bootstrapping based on syntactic dependency projection (e.g. Hwa et al 2002), often using approaches based in synchronous parsing (e.g. Smith and Smith, 2004). $$$$$ One idea (Yarowsky and Ngai, 2001; Hwa et al., 2002) is to project English analysis onto L data, “through” word-aligned parallel text.
Another distinct body of work addresses the problem of parser bootstrapping based on syntactic dependency projection (e.g. Hwa et al 2002), often using approaches based in synchronous parsing (e.g. Smith and Smith, 2004). $$$$$ As discussed in §2, Klein and Manning (2002) guide their parser’s search using a combination of separate unlexicalized PCFG and lexical dependency models.

The lattice-conditional estimation approach was first used by Kudo et al (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. $$$$$ We have presented a novel technique for merging simple, separately trained models for Korean parsing, English dependency parsing, and word translation, and optimizing the joint result using dynamic programming.
The lattice-conditional estimation approach was first used by Kudo et al (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. $$$$$ We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very limited labeled data.
The lattice-conditional estimation approach was first used by Kudo et al (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. $$$$$ We have presented a novel technique for merging simple, separately trained models for Korean parsing, English dependency parsing, and word translation, and optimizing the joint result using dynamic programming.
The lattice-conditional estimation approach was first used by Kudo et al (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. $$$$$ The model event space for our stochastic “halfbilexicalized” 2-MTG consists of rewrite rules of the following two forms, with English above and L below: where upper-case symbols are nonterminals and lowercase symbols are words (potentially ∅).

Similar results were presented by Smith and Smith (2004), using a similar estimation strategy with a model that included far more feature templates. $$$$$ We showed small but significant improvements for Korean parsers trained on small amounts of labeled data.
Similar results were presented by Smith and Smith (2004), using a similar estimation strategy with a model that included far more feature templates. $$$$$ It can be used to find every parse admitted by a grammar, and also scores of those parses.
Similar results were presented by Smith and Smith (2004), using a similar estimation strategy with a model that included far more feature templates. $$$$$ To make the most of this small corpus, we performed all our evaluations using five-fold cross-validation.
Similar results were presented by Smith and Smith (2004), using a similar estimation strategy with a model that included far more feature templates. $$$$$ We treat this as a noisy channel: Korean morpheme-tag pairs are generated in sequence by some process, then passed through a channel that turns them into Korean words (with loss of information).

Smith and Smith (2004) applied factored estimation to a bilingual weighted grammar, driven by data limitations. $$$$$ This inference task is carried out by a bilingual parser.
Smith and Smith (2004) applied factored estimation to a bilingual weighted grammar, driven by data limitations. $$$$$ Consider the problem of parsing a language L for which annotated resources like treebanks are scarce.
Smith and Smith (2004) applied factored estimation to a bilingual weighted grammar, driven by data limitations. $$$$$ Experiments showed, however, that the SITG using words consistently outperformed the SITG using morphemes.
Smith and Smith (2004) applied factored estimation to a bilingual weighted grammar, driven by data limitations. $$$$$ The low dependency projection results (§3.4), in conjunction with our modest overall gains, indicate that the alignment/translation model should receive the most attention.

In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al (2010). $$$$$ Note that Hwa et al. used not only the true English trees, but also hand-produced alignments.
In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al (2010). $$$$$ In this paper we use a translation model to induce only a single best word matching, but in principle the translation model could be used to weight all possible wordword links, and the parser would solve the joint alignment/parsing problem.2 As a testbed for our experiments, the Penn Korean Treebank (KTB; Han et al., 2002) provides 5,083 Korean constituency trees along with English translations and their trees.
In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al (2010). $$$$$ We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very limited labeled data.
In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al (2010). $$$$$ The model used for parsing is completely factored into the two parsers and the TM, allowing separate parameter estimation.
