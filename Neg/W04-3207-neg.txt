Finally, the work that is the closest to ours in spirit is the idea of joint estimation (Smith and Smith, 2004). $$$$$ We showed small but significant improvements for Korean parsers trained on small amounts of labeled data.
Finally, the work that is the closest to ours in spirit is the idea of joint estimation (Smith and Smith, 2004). $$$$$ Performance on the English side of our KTB test set was 71.82% (averaged across 5 folds, u = 1.75).
Finally, the work that is the closest to ours in spirit is the idea of joint estimation (Smith and Smith, 2004). $$$$$ Just as CKY parsing starts with words in its chart, the dynamic program chart for the bilingual parser is seeded with the links given in the hypothesized word alignment.

More recently, Smith and Smith (2004) proposed to merge an English parser, a word alignment model, and a Korean PCFG parser trained from a small number of Korean parse trees under a unified log linear model. $$$$$ In more formal work, Melamed (2003) proposes multitext grammars and algorithms for parsing them.
More recently, Smith and Smith (2004) proposed to merge an English parser, a word alignment model, and a Korean PCFG parser trained from a small number of Korean parse trees under a unified log linear model. $$$$$ It can be used to find every parse admitted by a grammar, and also scores of those parses.
More recently, Smith and Smith (2004) proposed to merge an English parser, a word alignment model, and a Korean PCFG parser trained from a small number of Korean parse trees under a unified log linear model. $$$$$ The model used for parsing is completely factored into the two parsers and the TM, allowing separate parameter estimation.
More recently, Smith and Smith (2004) proposed to merge an English parser, a word alignment model, and a Korean PCFG parser trained from a small number of Korean parse trees under a unified log linear model. $$$$$ We describe how simple, commonly understood statistical models, such as statistical dependency parsers, probabilistic context-free grammars, and word-to-word translation models, can be effectively combined into a unified bilingual parser that jointly searches for the best English parse, Korean parse, and word alignment, where these hidden structures all constrain each other.

We know of only one study which evaluates these bilingual grammar formalisms on the task of grammar induction itself (Smith and Smith, 2004). $$$$$ This work was supported under a National Science Foundation Graduate Research Fellowship and a Fannie and John Hertz Foundation Fellowship.
We know of only one study which evaluates these bilingual grammar formalisms on the task of grammar induction itself (Smith and Smith, 2004). $$$$$ We would like to thank Elliott Dr´abek, Jason Eisner, Eric Goldlust, Philip Resnik, Charles Schafer, David Yarowsky, and the reviewers for their comments and assistance and Chung-hye Han, Na-Rae Han, and Anoop Sarkar for their help with the Korean resources.
We know of only one study which evaluates these bilingual grammar formalisms on the task of grammar induction itself (Smith and Smith, 2004). $$$$$ Because we use this model only for inference, it is not necessary to compute a partition function for the combined log-linear model.
We know of only one study which evaluates these bilingual grammar formalisms on the task of grammar induction itself (Smith and Smith, 2004). $$$$$ We have presented a novel technique for merging simple, separately trained models for Korean parsing, English dependency parsing, and word translation, and optimizing the joint result using dynamic programming.

(Smith and Smith, 2004) tried to build a Korean parser by bilingual approach with English, and achieved labeled precision/recall around 40% for Korean. $$$$$ Similarly, translation models, which yield word alignments, can be used in principle to score competing alignments and offer alternatives to a single-best alignment.
(Smith and Smith, 2004) tried to build a Korean parser by bilingual approach with English, and achieved labeled precision/recall around 40% for Korean. $$$$$ To handle out-of-vocabulary (OOV) words, we treat any word seen for the first time in the final 300 sentences of the training corpus as OOV.
(Smith and Smith, 2004) tried to build a Korean parser by bilingual approach with English, and achieved labeled precision/recall around 40% for Korean. $$$$$ By positing a coupling of English syntax with L syntax, we can induce structure on the L side of the parallel text that is in some sense isomorphic to the English parse.
(Smith and Smith, 2004) tried to build a Korean parser by bilingual approach with English, and achieved labeled precision/recall around 40% for Korean. $$$$$ We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very limited labeled data.

Smith and Smith (2004) explore syntactic projection further by proposing an English-Korean bilingual parser integrated with a word translation model. $$$$$ We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very limited labeled data.
Smith and Smith (2004) explore syntactic projection further by proposing an English-Korean bilingual parser integrated with a word translation model. $$$$$ 7
Smith and Smith (2004) explore syntactic projection further by proposing an English-Korean bilingual parser integrated with a word translation model. $$$$$ We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very limited labeled data.

Smith and Smith (2004 ) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they consider a fixed combination of two parsers and a word aligner. $$$$$ Even with over 20,000 sentence pairs of training data, the hypothesized alignments are relatively sparse.
Smith and Smith (2004 ) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they consider a fixed combination of two parsers and a word aligner. $$$$$ It can be used to find every parse admitted by a grammar, and also scores of those parses.
Smith and Smith (2004 ) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they consider a fixed combination of two parsers and a word aligner. $$$$$ This work was supported under a National Science Foundation Graduate Research Fellowship and a Fannie and John Hertz Foundation Fellowship.
Smith and Smith (2004 ) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they consider a fixed combination of two parsers and a word aligner. $$$$$ First, we discuss bilingual parsing (§2) and show how it can solve the problem of joint English-parse, L-parse, and word-alignment inference.

 $$$$$ Working on the Penn Korean Treebank, Sarkar and Han (2002) made a single training/test split and used 91% of the sentences to train a morphological disambiguator and lexicalized tree adjoining grammar (LTAG) based parsing system.
 $$$$$ This work was supported under a National Science Foundation Graduate Research Fellowship and a Fannie and John Hertz Foundation Fellowship.
 $$$$$ At present, the model used for parsing is completely factored into the two parsers and the TM, allowing separate parameter estimation.
 $$$$$ We also consider Wu’s (1997) stochastic inversion transduction grammar (SITG) as well as strictly left- and right-branching trees.

Another distinct body of work addresses the problem of parser bootstrapping based on syntactic dependency projection (e.g. Hwa et al 2002), often using approaches based in synchronous parsing (e.g. Smith and Smith, 2004). $$$$$ This work was supported under a National Science Foundation Graduate Research Fellowship and a Fannie and John Hertz Foundation Fellowship.
Another distinct body of work addresses the problem of parser bootstrapping based on syntactic dependency projection (e.g. Hwa et al 2002), often using approaches based in synchronous parsing (e.g. Smith and Smith, 2004). $$$$$ In the context of machine translation, Dorr (1994) investigated divergences between two languages’ structures.

The lattice-conditional estimation approach was first used by Kudo et al (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. $$$$$ Composition is even more of a staple in finitestate frameworks (Knight and Graehl, 1998).
The lattice-conditional estimation approach was first used by Kudo et al (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. $$$$$ The model used for parsing is completely factored into the two parsers and the TM, allowing separate parameter estimation.
The lattice-conditional estimation approach was first used by Kudo et al (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. $$$$$ D is the direction (left or right).
The lattice-conditional estimation approach was first used by Kudo et al (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. $$$$$ We would like to thank Elliott Dr´abek, Jason Eisner, Eric Goldlust, Philip Resnik, Charles Schafer, David Yarowsky, and the reviewers for their comments and assistance and Chung-hye Han, Na-Rae Han, and Anoop Sarkar for their help with the Korean resources.

Similar results were presented by Smith and Smith (2004), using a similar estimation strategy with a model that included far more feature templates. $$$$$ Our approach has this principal advantage: the various morphology, parsing, and alignment components can be improved or replaced easily without needing to retrain the other modules.
Similar results were presented by Smith and Smith (2004), using a similar estimation strategy with a model that included far more feature templates. $$$$$ A PCFG treated this way is a perfectly valid log-linear model; the exponentials of its weights just happen to satisfy certain sum-to-one constraints.
Similar results were presented by Smith and Smith (2004), using a similar estimation strategy with a model that included far more feature templates. $$$$$ Our bilingual parser implementation is on the cusp of practicality (in terms of memory requirements); when the grammar constant increased, we were unable to parse longer sentences.
Similar results were presented by Smith and Smith (2004), using a similar estimation strategy with a model that included far more feature templates. $$$$$ The underlying parsing algorithm remains the same, but the weights are no longer constrained to sum to one.

Smith and Smith (2004) applied factored estimation to a bilingual weighted grammar, driven by data limitations. $$$$$ To make the most of this small corpus, we performed all our evaluations using five-fold cross-validation.
Smith and Smith (2004) applied factored estimation to a bilingual weighted grammar, driven by data limitations. $$$$$ Wu (1997) and Alshawi et al. (2000) used unsupervised learning on parallel text to induce syntactic analysis that was useful for their respective applications in phrasal translation extraction and speech translation, though not necessarily similar to what a human annotator would select.
Smith and Smith (2004) applied factored estimation to a bilingual weighted grammar, driven by data limitations. $$$$$ This work was supported under a National Science Foundation Graduate Research Fellowship and a Fannie and John Hertz Foundation Fellowship.

In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al (2010). $$$$$ We model the sequence of morphemes and their tags as a log-linear trigram model.
In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al (2010). $$$$$ These “overlapping” features offer a kind of backoff, so that each child-generation event’s weight receives a contribution from several granularities of description.
In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al (2010). $$$$$ Combining separately trained systems and then searching for an (ideally) optimal solution is standard practice in statistical continuous speech recognition (Jelinek, 1998) and statistical machine translation (Brown et al., 1990).
