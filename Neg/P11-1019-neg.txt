A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al, 2003), and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones (Yannakoudakis et al, 2011). $$$$$ For this reason, we believe that an approach which directly measures linguistic competence will be better suited to ESOL text assessment, and will have the additional advantage that it may not require retraining for new prompts or tasks.
A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al, 2003), and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones (Yannakoudakis et al, 2011). $$$$$ In rank preference SVMs, the goal is to learn a ranking function which outputs a score for each data point, from which a global ordering of the data is constructed.
A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al, 2003), and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones (Yannakoudakis et al, 2011). $$$$$ Though many of the systems described in Section 6 have been shown to correlate well with examiners’ marks on test data in many experimental contexts, no cross-system comparisons are available because of the lack of a shared training and test dataset.
A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al, 2003), and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones (Yannakoudakis et al, 2011). $$$$$ A number of different features are extracted and ablation tests are used to investigate their contribution to overall performance.

Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in Yannakoudakis et al. (2011). $$$$$ However, our system is not measuring some properties of the scripts, such as discourse cohesion or relevance to the prompt eliciting the text, that examiners will take into account.
Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in Yannakoudakis et al. (2011). $$$$$ In order to estimate the error-rate, we build a trigram language model (LM) using ukWaC (ukWaC LM) (Ferraresi et al., 2008), a large corpus of English containing more than 2 billion tokens.
Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in Yannakoudakis et al. (2011). $$$$$ Experimental results on the first publically available dataset show that our system can achieve levels of performance close to the upper bound for the task, as defined by the agreement between human examiners on the same corpus.
Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in Yannakoudakis et al. (2011). $$$$$ Sentences are scored on a five-point scale based on the parser’s cost vector, which roughly measures the complexity and deviation of a sentence from the parser’s grammatical model.

Also, in Yannakoudakis et al (2011), experiments are presented that test the validity of the system using a number of automatically-created 'outlier' texts. $$$$$ Swap words that have the same PoS within a sentence Although the above modifications do not exhaust the potential challenges a deployed AA system might face, they represent a threat to the validity of our system since we are using a highly related feature set.
Also, in Yannakoudakis et al (2011), experiments are presented that test the validity of the system using a number of automatically-created 'outlier' texts. $$$$$ We plan to experiment with better error detection techniques, since the overall error-rate of a script is one of the most discriminant features.
Also, in Yannakoudakis et al (2011), experiments are presented that test the validity of the system using a number of automatically-created 'outlier' texts. $$$$$ Linear regression is used to assign optimal feature weights that maximise the correlation with the examiner’s scores.

We use the First Certificate in English (FCE) ESOL examinationscripts2 (upper-intermediate level assessment) described in detail in Yannakoudakis et al (2011), extracted from the Cambridge Learner Corpus3 (CLC). $$$$$ For the experiments reported here, we use complexity measures representing the sum of the longest distance in word tokens between a head and dependent in a grammatical relation (GR) from the RASP GR output, calculated for each GR graph from the top 10 parses per sentence.
We use the First Certificate in English (FCE) ESOL examinationscripts2 (upper-intermediate level assessment) described in detail in Yannakoudakis et al (2011), extracted from the Cambridge Learner Corpus3 (CLC). $$$$$ Finally, we would like to thank Marek Rei, Øistein Andersen and the anonymous reviewers for their useful comments.
We use the First Certificate in English (FCE) ESOL examinationscripts2 (upper-intermediate level assessment) described in detail in Yannakoudakis et al (2011), extracted from the Cambridge Learner Corpus3 (CLC). $$$$$ Finally, we would like to thank Marek Rei, Øistein Andersen and the anonymous reviewers for their useful comments.

As in Yannakoudakis et al (2011), we analyze all texts using the RASP toolkit (Briscoe et al, 2006). $$$$$ Preliminary experiments based on a set of ‘outlier’ texts have shown the types of texts for which the system’s scoring capability can be undermined.
As in Yannakoudakis et al (2011), we analyze all texts using the RASP toolkit (Briscoe et al, 2006). $$$$$ However, our system is not measuring some properties of the scripts, such as discourse cohesion or relevance to the prompt eliciting the text, that examiners will take into account.
As in Yannakoudakis et al (2011), we analyze all texts using the RASP toolkit (Briscoe et al, 2006). $$$$$ In this case, generalisation is achieved by maximising the differences between closely-ranked data pairs.

Among the features used in Yannakoudakis et al (2011), none explicitly captures coherence and none models inter sentential relationships. $$$$$ Finally, we would like to thank Marek Rei, Øistein Andersen and the anonymous reviewers for their useful comments.
Among the features used in Yannakoudakis et al (2011), none explicitly captures coherence and none models inter sentential relationships. $$$$$ We plan to experiment with better error detection techniques, since the overall error-rate of a script is one of the most discriminant features.
Among the features used in Yannakoudakis et al (2011), none explicitly captures coherence and none models inter sentential relationships. $$$$$ Based on the features used by our system and without bias towards any modification, we modified each script in one of the following ways: ii.

In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in Yannakoudakis et al (2011) to report results of the final best system. $$$$$ Several of these are now deployed in highstakes assessment of examination scripts.
In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in Yannakoudakis et al (2011) to report results of the final best system. $$$$$ The task of automated assessment of free text focuses on automatically analysing and assessing the quality of writing competence.
In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in Yannakoudakis et al (2011) to report results of the final best system. $$$$$ Finally, using a set of ‘outlier’ texts, we test the validity of our model and identify cases where the model’s scores diverge from that of a human examiner.
In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in Yannakoudakis et al (2011) to report results of the final best system. $$$$$ A number of different features are extracted and ablation tests are used to investigate their contribution to overall performance.

 $$$$$ Learning a ranking directly, rather than fitting a classifier score to a grade point scale after training, is both a more generic approach to the task and one which exploits the labelling information in the training data efficiently and directly.
 $$$$$ In order to obtain a ceiling for the performance of our system, we calculate the average correlation between the CLC and the examiners’ scores, and find an upper bound of 0.796 and 0.792 Pearson’s and Spearman’s correlation respectively.
 $$$$$ The FCE writing component consists of two tasks asking learners to write either a letter, a report, an article, a composition or a short story, between 200 and 400 words.
 $$$$$ Unlike our approach, e-Rater models discourse structure, semantic coherence and relevance to the prompt.

See Yannakoudakis et al (2011) for details. $$$$$ Our model predicted a high passing mark for this text, but not the highest one possible, that some journalists clearly feel it deserves.
See Yannakoudakis et al (2011) for details. $$$$$ Each script has been also manually tagged with information about the linguistic errors committed, using a taxonomy of approximately 80 error types (Nicholls, 2003).
See Yannakoudakis et al (2011) for details. $$$$$ Nevertheless, exploration of the trade-offs between degree of supervision required in training and grading accuracy is an important area for future research.
See Yannakoudakis et al (2011) for details. $$$$$ Examples include e-Rater (Attali and Burstein, 2006), Intelligent Essay Assessor (IEA) (Landauer et al., 2003), IntelliMetric (Elliot, 2003; Rudner et al., 2006) and Project Essay Grade (PEG) (Page, 2003).

Their correction detection algorithm relies on a set of heuristics developed from one single data collection (the FCE corpus (Yannakoudakis et al, 2011)). $$$$$ The Bayesian Essay Test Scoring sYstem (BETSY) (Rudner and Liang, 2002) uses multinomial or Bernoulli Naive Bayes models to classify texts into different classes (e.g. pass/fail, grades A– F) based on content and style features such as word unigrams and bigrams, sentence length, number of verbs, noun–verb pairs etc.
Their correction detection algorithm relies on a set of heuristics developed from one single data collection (the FCE corpus (Yannakoudakis et al, 2011)). $$$$$ Each script has been also manually tagged with information about the linguistic errors committed, using a taxonomy of approximately 80 error types (Nicholls, 2003).
Their correction detection algorithm relies on a set of heuristics developed from one single data collection (the FCE corpus (Yannakoudakis et al, 2011)). $$$$$ In this section we briefly discuss a number of the more influential and/or better described approaches.

We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al, 2011), NUCLE corpus (Dahlmeier et al, 2013), UIUCcor pus 2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). $$$$$ Finally, using a set of ‘outlier’ texts, we test the validity of our model and identify cases where the model’s scores diverge from that of a human examiner.
We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al, 2011), NUCLE corpus (Dahlmeier et al, 2013), UIUCcor pus 2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). $$$$$ The following is an example errorcoded sentence: In the morning, you are <NS type = “TV”> waken|woken</NS> up by a singing puppy.
We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al, 2011), NUCLE corpus (Dahlmeier et al, 2013), UIUCcor pus 2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). $$$$$ A comparison between regression and rank preference models further supports our method.
We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al, 2011), NUCLE corpus (Dahlmeier et al, 2013), UIUCcor pus 2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). $$$$$ In e-Rater (Attali and Burstein, 2006), texts are represented using vectors of weighted features.

The CLC-FCE sub corpus was extracted, anonymized, and made available as a set of XML files by Yannakoudakis et al (2011). $$$$$ We demonstrate how supervised discriminative machine learning techniques can be used to automate the assessment of ‘English as a Second or Other Language’ (ESOL) examination scripts.
The CLC-FCE sub corpus was extracted, anonymized, and made available as a set of XML files by Yannakoudakis et al (2011). $$$$$ Nevertheless, exploration of the trade-offs between degree of supervision required in training and grading accuracy is an important area for future research.
The CLC-FCE sub corpus was extracted, anonymized, and made available as a set of XML files by Yannakoudakis et al (2011). $$$$$ Intelligent Essay Assessor (IEA) (Landauer et al., 2003) uses Latent Semantic Analysis (LSA) (Landauer and Foltz, 1998) to compute the semantic similarity between texts, at a specific grade point, and a test text.
The CLC-FCE sub corpus was extracted, anonymized, and made available as a set of XML files by Yannakoudakis et al (2011). $$$$$ The rank preference model improves Pearson’s and Spearman’s correlation by 0.044 and 0.067 respectively, and these differences are significant, suggesting that rank preference is a more appropriate model for the AA task.

Errors have been shown to have a significant impact on predicting learner level (Yannakoudakis et al, 2011). $$$$$ T/txt-frag, ‘a text unit consisting of 2 or more subanalyses that cannot be combined using any rule in the grammar’).
Errors have been shown to have a significant impact on predicting learner level (Yannakoudakis et al, 2011). $$$$$ Analysis of the results showed that our system predicted higher scores than the ones assigned by the examiner.
Errors have been shown to have a significant impact on predicting learner level (Yannakoudakis et al, 2011). $$$$$ In order to test the significance of the improved correlations, we ran one-tailed t-tests with a = 0.05 for the difference between dependent correlations (Williams, 1959; Steiger, 1980).
Errors have been shown to have a significant impact on predicting learner level (Yannakoudakis et al, 2011). $$$$$ Our focus is on developing an accurate AA system for ESOL text that does not require prompt-specific or topic-specific training.

By running ablation studies - i.e., removing one or more sets of features (cf. e.g. (Yannakoudakis et al., 2011)) - we can determine their relative importance and usefulness. $$$$$ Ablation tests highlight the contribution of each feature type to the overall performance, while significance of the resulting improvements in correlation with human scores has been calculated.
By running ablation studies - i.e., removing one or more sets of features (cf. e.g. (Yannakoudakis et al., 2011)) - we can determine their relative importance and usefulness. $$$$$ Singular Value Decomposition (SVD) is used to obtain a reduced dimension matrix clustering words and contexts.
By running ablation studies - i.e., removing one or more sets of features (cf. e.g. (Yannakoudakis et al., 2011)) - we can determine their relative importance and usefulness. $$$$$ The final case, where correct sentences are randomly ordered, receives the lowest correlation.
By running ablation studies - i.e., removing one or more sets of features (cf. e.g. (Yannakoudakis et al., 2011)) - we can determine their relative importance and usefulness. $$$$$ A comparison between regression and rank preference models further supports our method.

We extend our n-gram-based data-driven prediction approach from the Helping Our Own (HOO) 2011 Shared Task (Boyd and Meurers, 2011) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset (Yannakoudakis et al, 2011) as part of the HOO 2012 Shared Task. $$$$$ Learning a ranking directly, rather than fitting a classifier score to a grade point scale after training, is both a more generic approach to the task and one which exploits the labelling information in the training data efficiently and directly.
We extend our n-gram-based data-driven prediction approach from the Helping Our Own (HOO) 2011 Shared Task (Boyd and Meurers, 2011) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset (Yannakoudakis et al, 2011) as part of the HOO 2012 Shared Task. $$$$$ Deployment of automated assessment systems gives a number of advantages, such as the reduced workload in marking texts, especially when applied to large-scale assessments.
We extend our n-gram-based data-driven prediction approach from the Helping Our Own (HOO) 2011 Shared Task (Boyd and Meurers, 2011) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset (Yannakoudakis et al, 2011) as part of the HOO 2012 Shared Task. $$$$$ The addition of an incoherence metric to the feature set of an AA system has been shown to improve performance significantly (Miltsakaki and Kukich, 2000; Miltsakaki and Kukich, 2004).
We extend our n-gram-based data-driven prediction approach from the Helping Our Own (HOO) 2011 Shared Task (Boyd and Meurers, 2011) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset (Yannakoudakis et al, 2011) as part of the HOO 2012 Shared Task. $$$$$ There is no overlap between the prompts used in 2000 and in 2001.

The documents are a subset of the 1,244 document sin the Cambridge Learner Corpus FCE (First Certificate in English) data set (Yannakoudakis et al, 2011). $$$$$ Experimental results on the first publically available dataset show that our system can achieve levels of performance close to the upper bound for the task, as defined by the agreement between human examiners on the same corpus.
The documents are a subset of the 1,244 document sin the Cambridge Learner Corpus FCE (First Certificate in English) data set (Yannakoudakis et al, 2011). $$$$$ Our data consists of 1141 scripts from the year 2000 for training written by 1141 distinct learners, and 97 scripts from the year 2001 for testing written by 97 distinct learners.
The documents are a subset of the 1,244 document sin the Cambridge Learner Corpus FCE (First Certificate in English) data set (Yannakoudakis et al, 2011). $$$$$ Although there are many published analyses of the performance of individual systems, as yet there is no publically available shared dataset for training and testing such systems and comparing their performance.

A suitable corpus for developing this program is the Cambridge Learner Corpus (CLC) (Yannakoudakis et al, 2011). $$$$$ Finally, using a set of ‘outlier’ texts, we test the validity of our model and identify cases where the model’s scores diverge from that of a human examiner.
A suitable corpus for developing this program is the Cambridge Learner Corpus (CLC) (Yannakoudakis et al, 2011). $$$$$ The FCE writing component consists of two tasks asking learners to write either a letter, a report, an article, a composition or a short story, between 200 and 400 words.
A suitable corpus for developing this program is the Cambridge Learner Corpus (CLC) (Yannakoudakis et al, 2011). $$$$$ As mentioned in Section 2, the marking criteria for FCE scripts are primarily based on the accurate use of a range of different grammatical constructions relevant to specific communicative goals, but our system assesses this indirectly.
A suitable corpus for developing this program is the Cambridge Learner Corpus (CLC) (Yannakoudakis et al, 2011). $$$$$ In e-Rater (Attali and Burstein, 2006), texts are represented using vectors of weighted features.

The HOO development dataset consists of 1000 exam scripts drawn from a subset of the CLC FCE Dataset (Yannakoudakis et al, 2011). $$$$$ Sentences are scored on a five-point scale based on the parser’s cost vector, which roughly measures the complexity and deviation of a sentence from the parser’s grammatical model.
The HOO development dataset consists of 1000 exam scripts drawn from a subset of the CLC FCE Dataset (Yannakoudakis et al, 2011). $$$$$ There is no overlap between the prompts used in 2000 and in 2001.
The HOO development dataset consists of 1000 exam scripts drawn from a subset of the CLC FCE Dataset (Yannakoudakis et al, 2011). $$$$$ Finally, we hope that the release of the training and test dataset described here will facilitate further research on the AA task for ESOL free text and, in particular, precise comparison of different systems, feature types, and grade fitting methods.
The HOO development dataset consists of 1000 exam scripts drawn from a subset of the CLC FCE Dataset (Yannakoudakis et al, 2011). $$$$$ In order to evaluate our AA system, we use two correlation measures, Pearson’s product-moment correlation coefficient and Spearman’s rank correlation coefficient (hereafter Pearson’s and Spearman’s correlation respectively).

For the shared task, we made use of data drawn from the CLC FCE Dataset, a set of 1,244 exam scripts written by candidates sitting the Cambridge ESOL First Certificate in English (FCE) examination in 2000 and 2001, and made available by Cambridge Universiy Press; see (Yannakoudakis et al, 2011). $$$$$ (2003) provide a more detailed overview of existing AA systems.
For the shared task, we made use of data drawn from the CLC FCE Dataset, a set of 1,244 exam scripts written by candidates sitting the Cambridge ESOL First Certificate in English (FCE) examination in 2000 and 2001, and made available by Cambridge Universiy Press; see (Yannakoudakis et al, 2011). $$$$$ A comparison between regression and rank preference models further supports our method.

We also apply our visualiser to a set of 1,244 publically available FCE ESOL texts (Yannakoudakis et al, 2011) and make it available as a web service to other researchers. $$$$$ Finally, we would like to thank Marek Rei, Øistein Andersen and the anonymous reviewers for their useful comments.
We also apply our visualiser to a set of 1,244 publically available FCE ESOL texts (Yannakoudakis et al, 2011) and make it available as a web service to other researchers. $$$$$ Although the CLC contains information about the linguistic errors committed (see Section 2), we try to extract an error-rate in a way that doesn’t require manually tagged data.
We also apply our visualiser to a set of 1,244 publically available FCE ESOL texts (Yannakoudakis et al, 2011) and make it available as a web service to other researchers. $$$$$ It is likely that a larger training set and/or more consistent grading of the existing training data would help to close this gap.
We also apply our visualiser to a set of 1,244 publically available FCE ESOL texts (Yannakoudakis et al, 2011) and make it available as a web service to other researchers. $$$$$ The earliest systems used superficial features, such as word and sentence length, as proxies for understanding the text.
