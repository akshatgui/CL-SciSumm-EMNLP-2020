A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al, 2003), and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones (Yannakoudakis et al, 2011). $$$$$ Answers to each of these tasks are annotated with marks (in the range 1–40), which have been fitted to a RASCH model (Fischer and Molenaar, 1995) to correct for inter-examiner inconsistency and comparability.
A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al, 2003), and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones (Yannakoudakis et al, 2011). $$$$$ The learners’ ages follow a bimodal distribution with peaks at approximately 16–20 and 26–30 years of age.
A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al, 2003), and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones (Yannakoudakis et al, 2011). $$$$$ Experimental results on the first publically available dataset show that our system can achieve levels of performance close to the upper bound for the task, as defined by the agreement between human examiners on the same corpus.
A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al, 2003), and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones (Yannakoudakis et al, 2011). $$$$$ In addition, an overall mark is assigned to both tasks, which is the one we use in our experiments.

Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in Yannakoudakis et al. (2011). $$$$$ We are also grateful to Cambridge Assessment for arranging for the test scripts to be remarked by four of their senior examiners.
Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in Yannakoudakis et al. (2011). $$$$$ We are also grateful to Cambridge Assessment for arranging for the test scripts to be remarked by four of their senior examiners.
Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in Yannakoudakis et al. (2011). $$$$$ A word trigram in test data is counted as an error if it is not found in the language model.

Also, in Yannakoudakis et al (2011), experiments are presented that test the validity of the system using a number of automatically-created 'outlier' texts. $$$$$ There is no overlap between the prompts used in 2000 and in 2001.
Also, in Yannakoudakis et al (2011), experiments are presented that test the validity of the system using a number of automatically-created 'outlier' texts. $$$$$ As far as we know, this is the first application of a rank preference model to automated assessment (hereafter AA).
Also, in Yannakoudakis et al (2011), experiments are presented that test the validity of the system using a number of automatically-created 'outlier' texts. $$$$$ In addition, an overall mark is assigned to both tasks, which is the one we use in our experiments.

We use the First Certificate in English (FCE) ESOL examinationscripts2 (upper-intermediate level assessment) described in detail in Yannakoudakis et al (2011), extracted from the Cambridge Learner Corpus3 (CLC). $$$$$ We have shown experimentally how rank preference models can be effectively deployed for automated assessment of ESOL free-text answers.
We use the First Certificate in English (FCE) ESOL examinationscripts2 (upper-intermediate level assessment) described in detail in Yannakoudakis et al (2011), extracted from the Cambridge Learner Corpus3 (CLC). $$$$$ We are also grateful to Cambridge Assessment for arranging for the test scripts to be remarked by four of their senior examiners.
We use the First Certificate in English (FCE) ESOL examinationscripts2 (upper-intermediate level assessment) described in detail in Yannakoudakis et al (2011), extracted from the Cambridge Learner Corpus3 (CLC). $$$$$ The latter is a nonparametric robust measure of association which is sensitive only to the ordinal arrangement of values.
We use the First Certificate in English (FCE) ESOL examinationscripts2 (upper-intermediate level assessment) described in detail in Yannakoudakis et al (2011), extracted from the Cambridge Learner Corpus3 (CLC). $$$$$ However, as i(c) has a lower correlation compared to i(a) and i(b), it is likely that a random ordering of ngrams with N > 3 will further decrease performance.

As in Yannakoudakis et al (2011), we analyze all texts using the RASP toolkit (Briscoe et al, 2006). $$$$$ The final case, where correct sentences are randomly ordered, receives the lowest correlation.
As in Yannakoudakis et al (2011), we analyze all texts using the RASP toolkit (Briscoe et al, 2006). $$$$$ Next, we extend our language model with trigrams extracted from a subset of the texts contained in the CLC (CLC LM).
As in Yannakoudakis et al (2011), we analyze all texts using the RASP toolkit (Briscoe et al, 2006). $$$$$ Recent comments in the British media have focussed on this issue, reporting that, for example, one deployed essay marking system assigned Winston Churchill’s speech ‘We Shall Fight on the Beaches’ a low score because of excessive repetition5.
As in Yannakoudakis et al (2011), we analyze all texts using the RASP toolkit (Briscoe et al, 2006). $$$$$ We are also grateful to Cambridge Assessment for arranging for the test scripts to be remarked by four of their senior examiners.

Among the features used in Yannakoudakis et al (2011), none explicitly captures coherence and none models inter sentential relationships. $$$$$ The latter is a nonparametric robust measure of association which is sensitive only to the ordinal arrangement of values.
Among the features used in Yannakoudakis et al (2011), none explicitly captures coherence and none models inter sentential relationships. $$$$$ Finally, we would like to thank Marek Rei, Øistein Andersen and the anonymous reviewers for their useful comments.
Among the features used in Yannakoudakis et al (2011), none explicitly captures coherence and none models inter sentential relationships. $$$$$ The following is an example errorcoded sentence: In the morning, you are <NS type = “TV”> waken|woken</NS> up by a singing puppy.

In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in Yannakoudakis et al (2011) to report results of the final best system. $$$$$ Finally, we would like to thank Marek Rei, Øistein Andersen and the anonymous reviewers for their useful comments.
In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in Yannakoudakis et al (2011) to report results of the final best system. $$$$$ In the following sections we describe in more detail the dataset used for training and testing, the system developed, the evaluation methodology, as well as ablation experiments aimed at studying the contribution of different feature types to the AA task.
In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in Yannakoudakis et al (2011) to report results of the final best system. $$$$$ A comparison between regression and rank preference models further supports our method.
In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in Yannakoudakis et al (2011) to report results of the final best system. $$$$$ We would like to thank Cambridge ESOL, a division of Cambridge Assessment, for permission to use and distribute the examination scripts.

 $$$$$ We make such a dataset of ESOL examination scripts available1 (see Section 2 for more details), describe our novel approach to the task, and provide results for our system on this dataset.
 $$$$$ Nevertheless, exploration of the trade-offs between degree of supervision required in training and grading accuracy is an important area for future research.

See Yannakoudakis et al (2011) for details. $$$$$ A modification of type ii, where words with the same PoS within a sentence are swapped, results in a Pearson and Spearman correlation of 0.634 and 0.761 respectively.
See Yannakoudakis et al (2011) for details. $$$$$ This suggests that there is room for improvement in the language models we developed to estimate the error-rate.
See Yannakoudakis et al (2011) for details. $$$$$ We are also grateful to Cambridge Assessment for arranging for the test scripts to be remarked by four of their senior examiners.

Their correction detection algorithm relies on a set of heuristics developed from one single data collection (the FCE corpus (Yannakoudakis et al, 2011)). $$$$$ Finally, we would like to thank Marek Rei, Øistein Andersen and the anonymous reviewers for their useful comments.
Their correction detection algorithm relies on a set of heuristics developed from one single data collection (the FCE corpus (Yannakoudakis et al, 2011)). $$$$$ Furthermore, none of the published work of which we are aware has systematically compared the contribution of different feature types to the AA task, and only one (Powers et al., 2002) assesses the ease with which the system can be subverted given some knowledge of the features deployed.
Their correction detection algorithm relies on a set of heuristics developed from one single data collection (the FCE corpus (Yannakoudakis et al, 2011)). $$$$$ Singular Value Decomposition (SVD) is used to obtain a reduced dimension matrix clustering words and contexts.

We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al, 2011), NUCLE corpus (Dahlmeier et al, 2013), UIUCcor pus 2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). $$$$$ Based on a range of feature types automatically extracted using generic text processing techniques, our system achieves performance close to the upper bound for the task.
We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al, 2011), NUCLE corpus (Dahlmeier et al, 2013), UIUCcor pus 2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). $$$$$ We demonstrate how supervised discriminative machine learning techniques can be used to automate the assessment of ‘English as a Second or Other Language’ (ESOL) examination scripts.
We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al, 2011), NUCLE corpus (Dahlmeier et al, 2013), UIUCcor pus 2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). $$$$$ A comparison between regression and rank preference models further supports our method.
We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al, 2011), NUCLE corpus (Dahlmeier et al, 2013), UIUCcor pus 2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). $$$$$ The latter is a nonparametric robust measure of association which is sensitive only to the ordinal arrangement of values.

The CLC-FCE sub corpus was extracted, anonymized, and made available as a set of XML files by Yannakoudakis et al (2011). $$$$$ Surprisingly, there is very little published data on the robustness of existing systems.
The CLC-FCE sub corpus was extracted, anonymized, and made available as a set of XML files by Yannakoudakis et al (2011). $$$$$ The average correlation of the AA system with the CLC and the examiner scores shows that it is close dicted values with the CLC and the examiners’ scores, where E1 refers to the first examiner, E2 to the second etc. to the upper bound for the task.
The CLC-FCE sub corpus was extracted, anonymized, and made available as a set of XML files by Yannakoudakis et al (2011). $$$$$ This system shows that treating AA as a text classification problem is viable, but the feature types are all fairly shallow, and the approach doesn’t make efficient use of the training data as a separate classifier is trained for each grade point.

Errors have been shown to have a significant impact on predicting learner level (Yannakoudakis et al, 2011). $$$$$ An evaluation of our best error detection method shows a Pearson correlation of 0.611 between the estimated and the true CLC error counts.
Errors have been shown to have a significant impact on predicting learner level (Yannakoudakis et al, 2011). $$$$$ We are also grateful to Cambridge Assessment for arranging for the test scripts to be remarked by four of their senior examiners.
Errors have been shown to have a significant impact on predicting learner level (Yannakoudakis et al, 2011). $$$$$ Intuitively, this feature captures information about the grammatical sophistication of the writer.
Errors have been shown to have a significant impact on predicting learner level (Yannakoudakis et al, 2011). $$$$$ The rank preference model improves Pearson’s and Spearman’s correlation by 0.044 and 0.067 respectively, and these differences are significant, suggesting that rank preference is a more appropriate model for the AA task.

By running ablation studies - i.e., removing one or more sets of features (cf. e.g. (Yannakoudakis et al., 2011)) - we can determine their relative importance and usefulness. $$$$$ A typical prompt taken from the 2000 training dataset is shown below: Your teacher has asked you to write a story for the school’s English language magazine.
By running ablation studies - i.e., removing one or more sets of features (cf. e.g. (Yannakoudakis et al., 2011)) - we can determine their relative importance and usefulness. $$$$$ We have shown experimentally how rank preference models can be effectively deployed for automated assessment of ESOL free-text answers.
By running ablation studies - i.e., removing one or more sets of features (cf. e.g. (Yannakoudakis et al., 2011)) - we can determine their relative importance and usefulness. $$$$$ However, although our corpus of manually-marked texts was produced by learners of English in response to prompts eliciting free-text answers, the marking criteria are primarily based on the accurate use of a range of different linguistic constructions.
By running ablation studies - i.e., removing one or more sets of features (cf. e.g. (Yannakoudakis et al., 2011)) - we can determine their relative importance and usefulness. $$$$$ Each script has been also manually tagged with information about the linguistic errors committed, using a taxonomy of approximately 80 error types (Nicholls, 2003).

We extend our n-gram-based data-driven prediction approach from the Helping Our Own (HOO) 2011 Shared Task (Boyd and Meurers, 2011) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset (Yannakoudakis et al, 2011) as part of the HOO 2012 Shared Task. $$$$$ One of the main approaches adopted by previous systems involves the identification of features that measure writing skill, and then the application of linear or stepwise regression to find optimal feature weights so that the correlation with manually assigned scores is maximised.
We extend our n-gram-based data-driven prediction approach from the Helping Our Own (HOO) 2011 Shared Task (Boyd and Meurers, 2011) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset (Yannakoudakis et al, 2011) as part of the HOO 2012 Shared Task. $$$$$ The earliest systems used superficial features, such as word and sentence length, as proxies for understanding the text.
We extend our n-gram-based data-driven prediction approach from the Helping Our Own (HOO) 2011 Shared Task (Boyd and Meurers, 2011) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset (Yannakoudakis et al, 2011) as part of the HOO 2012 Shared Task. $$$$$ A number of different features are extracted and ablation tests are used to investigate their contribution to overall performance.

The documents are a subset of the 1,244 document sin the Cambridge Learner Corpus FCE (First Certificate in English) data set (Yannakoudakis et al, 2011). $$$$$ The results are given in Table 3.
The documents are a subset of the 1,244 document sin the Cambridge Learner Corpus FCE (First Certificate in English) data set (Yannakoudakis et al, 2011). $$$$$ Our goal here is to determine the extent to which knowledge of the feature types deployed poses a threat to the validity of our system, where certain text generation strategies may give rise to large positive discrepancies.
The documents are a subset of the 1,244 document sin the Cambridge Learner Corpus FCE (First Certificate in English) data set (Yannakoudakis et al, 2011). $$$$$ We would like to thank Cambridge ESOL, a division of Cambridge Assessment, for permission to use and distribute the examination scripts.
The documents are a subset of the 1,244 document sin the Cambridge Learner Corpus FCE (First Certificate in English) data set (Yannakoudakis et al, 2011). $$$$$ Each script has been also manually tagged with information about the linguistic errors committed, using a taxonomy of approximately 80 error types (Nicholls, 2003).

A suitable corpus for developing this program is the Cambridge Learner Corpus (CLC) (Yannakoudakis et al, 2011). $$$$$ Experimental results on the first publically available dataset show that our system can achieve levels of performance close to the upper bound for the task, as defined by the agreement between human examiners on the same corpus.
A suitable corpus for developing this program is the Cambridge Learner Corpus (CLC) (Yannakoudakis et al, 2011). $$$$$ A modification of type ii, where words with the same PoS within a sentence are swapped, results in a Pearson and Spearman correlation of 0.634 and 0.761 respectively.
A suitable corpus for developing this program is the Cambridge Learner Corpus (CLC) (Yannakoudakis et al, 2011). $$$$$ Our reasons are twofold: Discriminative classification techniques often outperform non-discriminative ones in the context of text classification (Joachims, 1998).
A suitable corpus for developing this program is the Cambridge Learner Corpus (CLC) (Yannakoudakis et al, 2011). $$$$$ Ablation tests highlight the contribution of each feature type to the overall performance, while significance of the resulting improvements in correlation with human scores has been calculated.

The HOO development dataset consists of 1000 exam scripts drawn from a subset of the CLC FCE Dataset (Yannakoudakis et al, 2011). $$$$$ The prompts eliciting the free text are provided with the dataset.
The HOO development dataset consists of 1000 exam scripts drawn from a subset of the CLC FCE Dataset (Yannakoudakis et al, 2011). $$$$$ In the experiments reported here we use Support Vector Machines (SVMs) (Vapnik, 1995) through the SVMlight package (Joachims, 1999).
The HOO development dataset consists of 1000 exam scripts drawn from a subset of the CLC FCE Dataset (Yannakoudakis et al, 2011). $$$$$ More recent systems have used more sophisticated automated text processing techniques to measure grammaticality, textual coherence, prespecified errors, and so forth.
The HOO development dataset consists of 1000 exam scripts drawn from a subset of the CLC FCE Dataset (Yannakoudakis et al, 2011). $$$$$ However, although our corpus of manually-marked texts was produced by learners of English in response to prompts eliciting free-text answers, the marking criteria are primarily based on the accurate use of a range of different linguistic constructions.

For the shared task, we made use of data drawn from the CLC FCE Dataset, a set of 1,244 exam scripts written by candidates sitting the Cambridge ESOL First Certificate in English (FCE) examination in 2000 and 2001, and made available by Cambridge Universiy Press; see (Yannakoudakis et al, 2011). $$$$$ Several of these are now deployed in highstakes assessment of examination scripts.
For the shared task, we made use of data drawn from the CLC FCE Dataset, a set of 1,244 exam scripts written by candidates sitting the Cambridge ESOL First Certificate in English (FCE) examination in 2000 and 2001, and made available by Cambridge Universiy Press; see (Yannakoudakis et al, 2011). $$$$$ In order to evaluate our AA system, we use two correlation measures, Pearson’s product-moment correlation coefficient and Spearman’s rank correlation coefficient (hereafter Pearson’s and Spearman’s correlation respectively).
For the shared task, we made use of data drawn from the CLC FCE Dataset, a set of 1,244 exam scripts written by candidates sitting the Cambridge ESOL First Certificate in English (FCE) examination in 2000 and 2001, and made available by Cambridge Universiy Press; see (Yannakoudakis et al, 2011). $$$$$ In rank preference SVMs, the goal is to learn a ranking function which outputs a score for each data point, from which a global ordering of the data is constructed.
For the shared task, we made use of data drawn from the CLC FCE Dataset, a set of 1,244 exam scripts written by candidates sitting the Cambridge ESOL First Certificate in English (FCE) examination in 2000 and 2001, and made available by Cambridge Universiy Press; see (Yannakoudakis et al, 2011). $$$$$ For the purpose of this work, we extracted scripts produced by learners taking the First Certificate in English (FCE) exam, which assesses English at an upper-intermediate level.

We also apply our visualiser to a set of 1,244 publically available FCE ESOL texts (Yannakoudakis et al, 2011) and make it available as a web service to other researchers. $$$$$ In this case, generalisation is achieved by maximising the differences between closely-ranked data pairs.
We also apply our visualiser to a set of 1,244 publically available FCE ESOL texts (Yannakoudakis et al, 2011) and make it available as a web service to other researchers. $$$$$ In addition, an overall mark is assigned to both tasks, which is the one we use in our experiments.
We also apply our visualiser to a set of 1,244 publically available FCE ESOL texts (Yannakoudakis et al, 2011) and make it available as a web service to other researchers. $$$$$ We address automated assessment as a supervised discriminative machine learning problem and particularly as a rank preference problem (Joachims, 2002).
