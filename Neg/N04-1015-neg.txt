This methodology is very similar to the way [Barzilay and Lee, 2004] evaluate their probabilistic TS model in comparison to the approach of [Lapata, 2003]. $$$$$ We first present an effective knowledge-leanmethod for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models.
This methodology is very similar to the way [Barzilay and Lee, 2004] evaluate their probabilistic TS model in comparison to the approach of [Lapata, 2003]. $$$$$ This idea dates back at least to Harris (1982), who claimed that ?various types of [word] recurrence patterns seem to characterize various types ofdiscourse?.

Thus, there might exist more than one equally good solution for TS, a view shared by almost all TS researchers, but which has not been accounted for in the evaluation methodologies of [Karamanis et al, 2004] and [Barzilay and Lee, 2004]. $$$$$ Cognitive psychologists have long posited that this similarity is not accidental, arguing that formulaic text structure facilitates readers?
Thus, there might exist more than one equally good solution for TS, a view shared by almost all TS researchers, but which has not been accounted for in the evaluation methodologies of [Karamanis et al, 2004] and [Barzilay and Lee, 2004]. $$$$$ Cognitive psychologists have long posited that this similarity is not accidental, arguing that formulaic text structure facilitates readers?
Thus, there might exist more than one equally good solution for TS, a view shared by almost all TS researchers, but which has not been accounted for in the evaluation methodologies of [Karamanis et al, 2004] and [Barzilay and Lee, 2004]. $$$$$ Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.

The simplest formulation we consider is an HMM where each state contains a unigram language model (LM), proposed by Chotimongkol (2008) for task-oriented dialogue and originally developed for discourse analysis by Barzilay and Lee (2004). $$$$$ Advantages of a distributional perspective include both drastic reduction in human effort and recogni tion of ?topics?
The simplest formulation we consider is an HMM where each state contains a unigram language model (LM), proposed by Chotimongkol (2008) for task-oriented dialogue and originally developed for discourse analysis by Barzilay and Lee (2004). $$$$$ corresponds roughly to the notions of topic and topic change.

In this work, we also assume a content model, which we fix to be the document-level HMM as used in Barzilay and Lee (2004). $$$$$ We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear.
In this work, we also assume a content model, which we fix to be the document-level HMM as used in Barzilay and Lee (2004). $$$$$ is not necessarily equivalent to ?simple?, so automated approaches still offer advantages over manual techniques, especially if one needs to model several domains.
In this work, we also assume a content model, which we fix to be the document-level HMM as used in Barzilay and Lee (2004). $$$$$ comprehension and recall (Bartlett, 1932).1In this paper, we investigate the utility of domain specific content models for representing topics and topic shifts.

Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.
Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.
Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.
Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. $$$$$ Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.


Like Barzilay and Lee (2004), this model was used to order extracted sentences in summaries. $$$$$ The resulting summaries yield 88%match with human-written output, which compares fa vorably to the 69% achieved by the standard ?leading 
Like Barzilay and Lee (2004), this model was used to order extracted sentences in summaries. $$$$$ Second, we consider extractive summa 1But ?formulaic?

Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. $$$$$ Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.
Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. $$$$$ Advantages of a distributional perspective include both drastic reduction in human effort and recogni tion of ?topics?
Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. $$$$$ comprehension and recall (Bartlett, 1932).1In this paper, we investigate the utility of domain specific content models for representing topics and topic shifts.
Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. $$$$$ Cognitive psychologists have long posited that this similarity is not accidental, arguing that formulaic text structure facilitates readers?

Barzilay and Lee (2004) have proposed content models to deal with topic transition in domain specific text. $$$$$ Content models are Hidden Markov Models (HMMs) wherein states correspond to typesof information characteristic to the domain of interest (e.g., earthquake magnitude or previous earth quake occurrences), and state transitions capture possible information-presentation orderings within that domain.
Barzilay and Lee (2004) have proposed content models to deal with topic transition in domain specific text. $$$$$ We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear.
Barzilay and Lee (2004) have proposed content models to deal with topic transition in domain specific text. $$$$$ We first present an effective knowledge-leanmethod for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models.

These methods identify regularities in words (Barzilay and Lee, 2004). $$$$$ This idea dates back at least to Harris (1982), who claimed that ?various types of [word] recurrence patterns seem to characterize various types ofdiscourse?.

To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance as signing each sentence a single topic and then topic specific word models can be used $$$$$ Our technique incorporates a novel adaptation of the standard HMM induction algorithm that is tailored to the task of modeling content.
To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance as signing each sentence a single topic and then topic specific word models can be used $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.
To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance as signing each sentence a single topic and then topic specific word models can be used $$$$$ The focus of ourwork, however, is on an equally fundamental but domain dependent dimension of the structure of text: content.

Barzilay and Lee (2004)'s knowledge-lean approach attempts to automate the inference of knowledge-rich information using a distributional view of content. $$$$$ We first present an effective knowledge-leanmethod for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models.
Barzilay and Lee (2004)'s knowledge-lean approach attempts to automate the inference of knowledge-rich information using a distributional view of content. $$$$$ First, we consider in formation ordering, that is, choosing a sequence in whichto present a pre-selected set of items; this is an essen tial step in concept-to-text generation, multi-document summarization, and other text-synthesis problems.
Barzilay and Lee (2004)'s knowledge-lean approach attempts to automate the inference of knowledge-rich information using a distributional view of content. $$$$$ Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.
Barzilay and Lee (2004)'s knowledge-lean approach attempts to automate the inference of knowledge-rich information using a distributional view of content. $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.

This is specific to their approach as both Lapata (2003)'s and Barzilay and Lee (2004)'s approaches are not tailored to summarization and therefore do not experience the topic bias problem. $$$$$ Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.
This is specific to their approach as both Lapata (2003)'s and Barzilay and Lee (2004)'s approaches are not tailored to summarization and therefore do not experience the topic bias problem. $$$$$ We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear.
This is specific to their approach as both Lapata (2003)'s and Barzilay and Lee (2004)'s approaches are not tailored to summarization and therefore do not experience the topic bias problem. $$$$$ Advantages of a distributional perspective include both drastic reduction in human effort and recogni tion of ?topics?
This is specific to their approach as both Lapata (2003)'s and Barzilay and Lee (2004)'s approaches are not tailored to summarization and therefore do not experience the topic bias problem. $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.

The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from Barzilay and Lapata (2008). $$$$$ that might not occur to a human expert and yet, when explicitly modeled, aid in applications.
The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from Barzilay and Lapata (2008). $$$$$ comprehension and recall (Bartlett, 1932).1In this paper, we investigate the utility of domain specific content models for representing topics and topic shifts.
The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from Barzilay and Lapata (2008). $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.
The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from Barzilay and Lapata (2008). $$$$$ Second, we consider extractive summa 1But ?formulaic?

When compared to the results obtained by Barzilay and Lapata (2008) and Barzilay and Lee (2004), it would appear that direct sentence to-sentence similarity (as suggested by the Barzilay and Lapata baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset. $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.
When compared to the results obtained by Barzilay and Lapata (2008) and Barzilay and Lee (2004), it would appear that direct sentence to-sentence similarity (as suggested by the Barzilay and Lapata baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset. $$$$$ Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.

This assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004). $$$$$ Second, we consider extractive summa 1But ?formulaic?
This assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004). $$$$$ Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.

However, as Barzilay and Lee (2004) observe, the content of document collections is highly structured, consisting of several topical themes, each with its own vocabulary and ordering preferences. $$$$$ We desire models that can specify, for example, that articles about earthquakes typically contain information about quake strength, location, and casualties, and that descriptions of casualties usually precede those of rescue efforts.
However, as Barzilay and Lee (2004) observe, the content of document collections is highly structured, consisting of several topical themes, each with its own vocabulary and ordering preferences. $$$$$ We first present an effective knowledge-leanmethod for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models.

An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004). $$$$$ Previous research has sought to characterize texts in terms of domain-independent rhetorical elements, such as schema items (McKeown, 1985) or rhetorical relations (Mann and Thompson, 1988; Marcu, 1997).
An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004). $$$$$ We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear.
An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004). $$$$$ We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear.
An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004). $$$$$ The focus of ourwork, however, is on an equally fundamental but domain dependent dimension of the structure of text: content.

Following Ruch et al (2003) and Barzilay and Lee (2004), we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts. $$$$$ Our technique incorporates a novel adaptation of the standard HMM induction algorithm that is tailored to the task of modeling content.
Following Ruch et al (2003) and Barzilay and Lee (2004), we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts. $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.
Following Ruch et al (2003) and Barzilay and Lee (2004), we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts. $$$$$ Previous research has sought to characterize texts in terms of domain-independent rhetorical elements, such as schema items (McKeown, 1985) or rhetorical relations (Mann and Thompson, 1988; Marcu, 1997).
Following Ruch et al (2003) and Barzilay and Lee (2004), we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts. $$$$$ Content models are Hidden Markov Models (HMMs) wherein states correspond to typesof information characteristic to the domain of interest (e.g., earthquake magnitude or previous earth quake occurrences), and state transitions capture possible information-presentation orderings within that domain.

An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee,2004). $$$$$ We first present an effective knowledge-leanmethod for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models.
An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee,2004). $$$$$ We first present an effective knowledge-leanmethod for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models.
An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee,2004). $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.
