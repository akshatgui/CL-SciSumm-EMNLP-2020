This methodology is very similar to the way [Barzilay and Lee, 2004] evaluate their probabilistic TS model in comparison to the approach of [Lapata, 2003]. $$$$$ Our use of the term ?content?
This methodology is very similar to the way [Barzilay and Lee, 2004] evaluate their probabilistic TS model in comparison to the approach of [Lapata, 2003]. $$$$$ In arbi trary document collections, such patterns might be toovariable to be easily detected by statistical means.

Thus, there might exist more than one equally good solution for TS, a view shared by almost all TS researchers, but which has not been accounted for in the evaluation methodologies of [Karamanis et al, 2004] and [Barzilay and Lee, 2004]. $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.
Thus, there might exist more than one equally good solution for TS, a view shared by almost all TS researchers, but which has not been accounted for in the evaluation methodologies of [Karamanis et al, 2004] and [Barzilay and Lee, 2004]. $$$$$ This idea dates back at least to Harris (1982), who claimed that ?various types of [word] recurrence patterns seem to characterize various types ofdiscourse?.

The simplest formulation we consider is an HMM where each state contains a unigram language model (LM), proposed by Chotimongkol (2008) for task-oriented dialogue and originally developed for discourse analysis by Barzilay and Lee (2004). $$$$$ Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.
The simplest formulation we consider is an HMM where each state contains a unigram language model (LM), proposed by Chotimongkol (2008) for task-oriented dialogue and originally developed for discourse analysis by Barzilay and Lee (2004). $$$$$ We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear.

In this work, we also assume a content model, which we fix to be the document-level HMM as used in Barzilay and Lee (2004). $$$$$ We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear.

Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.
Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. $$$$$ The development and application of computational models of text structure is a central concern in natural language processing.

Barzilay and Lee (2004) showed that it is possible to obtain schema-like knowledge automatically from a corpus for the purposes of extracting sentences and ordering them. However, their work represents patterns at the sentence level, and is thus not directly comparable to our work, given our focus on sentence generation. $$$$$ We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear.
Barzilay and Lee (2004) showed that it is possible to obtain schema-like knowledge automatically from a corpus for the purposes of extracting sentences and ordering them. However, their work represents patterns at the sentence level, and is thus not directly comparable to our work, given our focus on sentence generation. $$$$$ The development and application of computational models of text structure is a central concern in natural language processing.

Like Barzilay and Lee (2004), this model was used to order extracted sentences in summaries. $$$$$ But rather than manually determine the topics for a given domain, we take a distributional view, learning them directly from un-annotated texts via analysis of word distribution patterns.
Like Barzilay and Lee (2004), this model was used to order extracted sentences in summaries. $$$$$ But rather than manually determine the topics for a given domain, we take a distributional view, learning them directly from un-annotated texts via analysis of word distribution patterns.
Like Barzilay and Lee (2004), this model was used to order extracted sentences in summaries. $$$$$ Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.

Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. $$$$$ We desire models that can specify, for example, that articles about earthquakes typically contain information about quake strength, location, and casualties, and that descriptions of casualties usually precede those of rescue efforts.
Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. $$$$$ First, we consider in formation ordering, that is, choosing a sequence in whichto present a pre-selected set of items; this is an essen tial step in concept-to-text generation, multi-document summarization, and other text-synthesis problems.
Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. $$$$$ We first present an effective knowledge-leanmethod for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models.
Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. $$$$$ The focus of ourwork, however, is on an equally fundamental but domain dependent dimension of the structure of text: content.

Barzilay and Lee (2004) have proposed content models to deal with topic transition in domain specific text. $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.
Barzilay and Lee (2004) have proposed content models to deal with topic transition in domain specific text. $$$$$ Then, we apply techniques based on content models totwo complex text-processing tasks.
Barzilay and Lee (2004) have proposed content models to deal with topic transition in domain specific text. $$$$$ comprehension and recall (Bartlett, 1932).1In this paper, we investigate the utility of domain specific content models for representing topics and topic shifts.
Barzilay and Lee (2004) have proposed content models to deal with topic transition in domain specific text. $$$$$ Previous research has sought to characterize texts in terms of domain-independent rhetorical elements, such as schema items (McKeown, 1985) or rhetorical relations (Mann and Thompson, 1988; Marcu, 1997).

These methods identify regularities in words (Barzilay and Lee, 2004). $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.

To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance as signing each sentence a single topic and then topic specific word models can be used: the hidden topic Markov model (Gruber et al2007) that models the transitional topic structure; a global model based on the generalised Mallows model (Chen et al2009), and a HMM based content model (Barzilay and Lee, 2004). $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.
To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance as signing each sentence a single topic and then topic specific word models can be used: the hidden topic Markov model (Gruber et al2007) that models the transitional topic structure; a global model based on the generalised Mallows model (Chen et al2009), and a HMM based content model (Barzilay and Lee, 2004). $$$$$ First, we consider in formation ordering, that is, choosing a sequence in whichto present a pre-selected set of items; this is an essen tial step in concept-to-text generation, multi-document summarization, and other text-synthesis problems.
To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance as signing each sentence a single topic and then topic specific word models can be used: the hidden topic Markov model (Gruber et al2007) that models the transitional topic structure; a global model based on the generalised Mallows model (Chen et al2009), and a HMM based content model (Barzilay and Lee, 2004). $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.

Barzilay and Lee (2004)'s knowledge-lean approach attempts to automate the inference of knowledge-rich information using a distributional view of content. $$$$$ that might not occur to a human expert and yet, when explicitly modeled, aid in applications.
Barzilay and Lee (2004)'s knowledge-lean approach attempts to automate the inference of knowledge-rich information using a distributional view of content. $$$$$ rization: the compression of a document by choosinga subsequence of its sentences.
Barzilay and Lee (2004)'s knowledge-lean approach attempts to automate the inference of knowledge-rich information using a distributional view of content. $$$$$ Advantages of a distributional perspective include both drastic reduction in human effort and recogni tion of ?topics?
Barzilay and Lee (2004)'s knowledge-lean approach attempts to automate the inference of knowledge-rich information using a distributional view of content. $$$$$ rization: the compression of a document by choosinga subsequence of its sentences.

This is specific to their approach as both Lapata (2003)'s and Barzilay and Lee (2004)'s approaches are not tailored to summarization and therefore do not experience the topic bias problem. $$$$$ rization: the compression of a document by choosinga subsequence of its sentences.

The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from Barzilay and Lapata (2008). $$$$$ Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.
The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from Barzilay and Lapata (2008). $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.

When compared to the results obtained by Barzilay and Lapata (2008) and Barzilay and Lee (2004), it would appear that direct sentence to-sentence similarity (as suggested by the Barzilay and Lapata baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset. $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.
When compared to the results obtained by Barzilay and Lapata (2008) and Barzilay and Lee (2004), it would appear that direct sentence to-sentence similarity (as suggested by the Barzilay and Lapata baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset. $$$$$ corresponds roughly to the notions of topic and topic change.
When compared to the results obtained by Barzilay and Lapata (2008) and Barzilay and Lee (2004), it would appear that direct sentence to-sentence similarity (as suggested by the Barzilay and Lapata baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset. $$$$$ We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear.

This assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004). $$$$$ Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.
This assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004). $$$$$ We first present an effective knowledge-leanmethod for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models.

However, as Barzilay and Lee (2004) observe, the content of document collections is highly structured, consisting of several topical themes, each with its own vocabulary and ordering preferences. $$$$$ Of course, the success of the distributional approachdepends on the existence of recurrent patterns.
However, as Barzilay and Lee (2004) observe, the content of document collections is highly structured, consisting of several topical themes, each with its own vocabulary and ordering preferences. $$$$$ The focus of ourwork, however, is on an equally fundamental but domain dependent dimension of the structure of text: content.
However, as Barzilay and Lee (2004) observe, the content of document collections is highly structured, consisting of several topical themes, each with its own vocabulary and ordering preferences. $$$$$ Our use of the term ?content?
However, as Barzilay and Lee (2004) observe, the content of document collections is highly structured, consisting of several topical themes, each with its own vocabulary and ordering preferences. $$$$$ Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.

An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004). $$$$$ is not necessarily equivalent to ?simple?, so automated approaches still offer advantages over manual techniques, especially if one needs to model several domains.
An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004). $$$$$ We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear.
An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004). $$$$$ We first present an effective knowledge-leanmethod for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models.

Following Ruch et al (2003) and Barzilay and Lee (2004), we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts. $$$$$ Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.
Following Ruch et al (2003) and Barzilay and Lee (2004), we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts. $$$$$ This idea dates back at least to Harris (1982), who claimed that ?various types of [word] recurrence patterns seem to characterize various types ofdiscourse?.
Following Ruch et al (2003) and Barzilay and Lee (2004), we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts. $$$$$ Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.

An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee,2004). $$$$$ First, we consider in formation ordering, that is, choosing a sequence in whichto present a pre-selected set of items; this is an essen tial step in concept-to-text generation, multi-document summarization, and other text-synthesis problems.
An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee,2004). $$$$$ We then apply our method to two complementary tasks: information ordering and ex tractive summarization.
An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee,2004). $$$$$ Previous research has sought to characterize texts in terms of domain-independent rhetorical elements, such as schema items (McKeown, 1985) or rhetorical relations (Mann and Thompson, 1988; Marcu, 1997).
An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee,2004). $$$$$ We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear.
