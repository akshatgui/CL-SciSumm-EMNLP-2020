In comparison, (Yarowsky, 1995) achieved 91.4% correct performance, using 1380 contexts and the dictionary definitions in training. $$$$$ The algorithm is based on two powerful constraints — that words tend to have one sense per discourse and one sense per collocation — exploited in an iterative bootstrapping procedure.
In comparison, (Yarowsky, 1995) achieved 91.4% correct performance, using 1380 contexts and the dictionary definitions in training. $$$$$ Although apparently small in absolute terms, on average this represents a 27% reduction in error rate.11 When applied at each iteration, this process reduces the training noise, yielding the optimal observed accuracy in column 10.
In comparison, (Yarowsky, 1995) achieved 91.4% correct performance, using 1380 contexts and the dictionary definitions in training. $$$$$ SENSE-A seeds plus newly added examples) will tend to grow, while the residual will tend to shrink.
In comparison, (Yarowsky, 1995) achieved 91.4% correct performance, using 1380 contexts and the dictionary definitions in training. $$$$$ Although several algorithms can accomplish similar ends,6 the following approach has the advantages of simplicity and the ability to build on an existing supervised classification algorithm without modification.'

Recently, Yarowsky (1995) combined a MIlD and a corpus in a bootstrapping process. $$$$$ By not combining probabilities, this decision-list approach avoids the problematic complex modeling of statistical dependencies 'It is interesting to speculate on the reasons for this phenomenon.
Recently, Yarowsky (1995) combined a MIlD and a corpus in a bootstrapping process. $$$$$ At the end of Step 4, this property is used for error correction.
Recently, Yarowsky (1995) combined a MIlD and a corpus in a bootstrapping process. $$$$$ Column 11 shows the performance of Schiitze's unsupervised algorithm applied to some of these words, trained on a New York Times News Service corpus.
Recently, Yarowsky (1995) combined a MIlD and a corpus in a bootstrapping process. $$$$$ This differs from sense induction: using distributional similarity to partition word instances into clusters that may have no relation to standard sense partitions.

Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). $$$$$ This approach is least successful for senses with a complex concept space, which cannot be adequately represented by single words.
Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). $$$$$ This provides a mechanism for bootstrapping a sense tagger.
Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). $$$$$ Regrettably, this algorithm was only described in two sentences and was not developed further.
Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). $$$$$ There is additional hope for these cases, however, as such isolated tokens tend to strongly favor a particular sense (the less &quot;bursty&quot; one).

Yarowsky (1995) successfully used this observation as an approximate annotation technique in an unsupervised WSD model. $$$$$ The algorithm will be illustrated by the disambiguation of 7538 instances of the polysemous word plant in a previously untagged corpus.
Yarowsky (1995) successfully used this observation as an approximate annotation technique in an unsupervised WSD model. $$$$$ This differs from sense induction: using distributional similarity to partition word instances into clusters that may have no relation to standard sense partitions.
Yarowsky (1995) successfully used this observation as an approximate annotation technique in an unsupervised WSD model. $$$$$ Using the decision-list algorithm, these additions will contain newly-learned collocations that are reliably indicative of the previously-trained seed sets.
Yarowsky (1995) successfully used this observation as an approximate annotation technique in an unsupervised WSD model. $$$$$ ... Nissan car and truck plant in Japan is ... ?

This heuristic naturally reflects the broadly known assumption about lexical ambiguity presented in (Yarowsky, 1995), namely the one-sense-per-discourse heuristic. $$$$$ Stop.
This heuristic naturally reflects the broadly known assumption about lexical ambiguity presented in (Yarowsky, 1995), namely the one-sense-per-discourse heuristic. $$$$$ Column 11 shows the performance of Schiitze's unsupervised algorithm applied to some of these words, trained on a New York Times News Service corpus.
This heuristic naturally reflects the broadly known assumption about lexical ambiguity presented in (Yarowsky, 1995), namely the one-sense-per-discourse heuristic. $$$$$ For example: TIndeed, any supervised classification algorithm that returns probabilities with its classifications may potentially be used here.
This heuristic naturally reflects the broadly known assumption about lexical ambiguity presented in (Yarowsky, 1995), namely the one-sense-per-discourse heuristic. $$$$$ If cumulative evidence for the majority sense exceeds that of the minority by a threshold (conditional on n), the minority cases are relabeled.

This heuristic mimes the one-sense-per collocation heuristic presented in (Yarowsky, 1995). $$$$$ Co-occurrence analysis selects collocates that span the space with minimal overlap, optimizing the efforts of the human assistant.
This heuristic mimes the one-sense-per collocation heuristic presented in (Yarowsky, 1995). $$$$$ We have yet to use this additional information.

These include the bootstrapping approach [Yarowsky 1995] and the context clustering approach [Schutze 1998]. $$$$$ Words not only tend to occur in collocations that reliably indicate their sense, they tend to occur in multiple such collocations.
These include the bootstrapping approach [Yarowsky 1995] and the context clustering approach [Schutze 1998]. $$$$$ Repeat Step 3 iteratively.
These include the bootstrapping approach [Yarowsky 1995] and the context clustering approach [Schutze 1998]. $$$$$ Although several algorithms can accomplish similar ends,6 the following approach has the advantages of simplicity and the ability to build on an existing supervised classification algorithm without modification.'
These include the bootstrapping approach [Yarowsky 1995] and the context clustering approach [Schutze 1998]. $$$$$ Such a bridge to the SENSE-A collocate &quot;cell&quot; is illustrated graphically in the upper half of Figure 2.

Yarowsky (1995) presented an approach that significantly reduces the amount of labeled data needed forword sense disambiguation. $$$$$ They have been displaced by more broadly applicable collocations that better partition the newly learned classes.
Yarowsky (1995) presented an approach that significantly reduces the amount of labeled data needed forword sense disambiguation. $$$$$ Thus contexts that are added to the wrong seed set because of a misleading word in a dictionary definition may be (and typically are) correctly reclassified as iterative training proceeds.
Yarowsky (1995) presented an approach that significantly reduces the amount of labeled data needed forword sense disambiguation. $$$$$ Repeat Step 3 iteratively.
Yarowsky (1995) presented an approach that significantly reduces the amount of labeled data needed forword sense disambiguation. $$$$$ IIf their classification begins to waver because new examples have discredited the crucial collocate, they are returned to the residual and may later be classified differently.

Many of these tasks have been addressed in other fields, for example, hypothesis verification in the field of machine translation (Tran et al, 1996), sense disambiguation in speech synthesis (Yarowsky, 1995), and relation tagging in information retrieval (Marsh and Perzanowski, 1999). $$$$$ Thus contexts that are added to the wrong seed set because of a misleading word in a dictionary definition may be (and typically are) correctly reclassified as iterative training proceeds.
Many of these tasks have been addressed in other fields, for example, hypothesis verification in the field of machine translation (Tran et al, 1996), sense disambiguation in speech synthesis (Yarowsky, 1995), and relation tagging in information retrieval (Marsh and Perzanowski, 1999). $$$$$ In this current work, the one-sense-per-discourse hypothesis was tested on a set of 37,232 examples (hand-tagged over a period of 3 years), the same data studied in the disambiguation experiments.

Yarowsky (1995) used both supervised and unsupervised WSD for correct phonetizitation of words in speech synthesis. $$$$$ ... Nissan car and truck plant in Japan is ... ?
Yarowsky (1995) used both supervised and unsupervised WSD for correct phonetizitation of words in speech synthesis. $$$$$ The observation that words strongly tend to exhibit only one sense in a given discourse or document was stated and quantified in Gale, Church and Yarowsky (1992).
Yarowsky (1995) used both supervised and unsupervised WSD for correct phonetizitation of words in speech synthesis. $$$$$ In this current work, the one-sense-per-discourse hypothesis was tested on a set of 37,232 examples (hand-tagged over a period of 3 years), the same data studied in the disambiguation experiments.
Yarowsky (1995) used both supervised and unsupervised WSD for correct phonetizitation of words in speech synthesis. $$$$$ This partitions the training set into 82 examples of living plants (1%), 106 examples of manufacturing plants (1%), and 7350 residual examples (98%).

The idea of sense consistency was first introduced and extended to operate across related documents by (Yarowsky, 1995). $$$$$ WordNet (Miller, 1990) is an automatic source for such defining terms.
The idea of sense consistency was first introduced and extended to operate across related documents by (Yarowsky, 1995). $$$$$ The probability differentials necessary for such a reclassification were determined empirically in an early pilot study.

This method, initially proposed by (Yarowsky, 1995), was successfully evaluated in the context of the SENSEVAL framework (Mihalcea, 2002). $$$$$ This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations.
This method, initially proposed by (Yarowsky, 1995), was successfully evaluated in the context of the SENSEVAL framework (Mihalcea, 2002). $$$$$ In cases where there are multiple seeds, it is even possible for an original seed for SENSE-A to become an indicator for SENSE-B if the collocate is more compatible with this second class.
This method, initially proposed by (Yarowsky, 1995), was successfully evaluated in the context of the SENSEVAL framework (Mihalcea, 2002). $$$$$ This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations.
This method, initially proposed by (Yarowsky, 1995), was successfully evaluated in the context of the SENSEVAL framework (Mihalcea, 2002). $$$$$ This differs from sense induction: using distributional similarity to partition word instances into clusters that may have no relation to standard sense partitions.

See Yarowsky (1995) for details. $$$$$ 4This latter effect is actually a continuous function conditional on the burstiness of the word (the tendency of a word to deviate from a constant Poisson distribution in a corpus).
See Yarowsky (1995) for details. $$$$$ Tested accuracy exceeds 96%.
See Yarowsky (1995) for details. $$$$$ It may be corrected if the majority of the seeds forms a coherent collocation space.
See Yarowsky (1995) for details. $$$$$ The algorithm is based on two powerful constraints — that words tend to have one sense per discourse and one sense per collocation — exploited in an iterative bootstrapping procedure.

The well-known observation that words rarely exhibit more than one sense per discourse (Yarowsky, 1995) implies that features closely associated with a particular sense have a low probability of appearing in the same document as features associated with another sense. $$$$$ Although several algorithms can accomplish similar ends,6 the following approach has the advantages of simplicity and the ability to build on an existing supervised classification algorithm without modification.'
The well-known observation that words rarely exhibit more than one sense per discourse (Yarowsky, 1995) implies that features closely associated with a particular sense have a low probability of appearing in the same document as features associated with another sense. $$$$$ Repeat Step 3 iteratively.
The well-known observation that words rarely exhibit more than one sense per discourse (Yarowsky, 1995) implies that features closely associated with a particular sense have a low probability of appearing in the same document as features associated with another sense. $$$$$ Yet to date, the full power of this property has not been exploited for sense disambiguation.

Yarowsky (1995) first recognized that it is possible to use a small number of features for different senses to bootstrap an unsupervised word sense disambiguation system. $$$$$ STEP 3c: Optionally, the one-sense-per-discourse constraint is then used both to filter and augment this addition.
Yarowsky (1995) first recognized that it is possible to use a small number of features for different senses to bootstrap an unsupervised word sense disambiguation system. $$$$$ This can be done automatically, using words that occur with significantly greater frequency in the entry relative to the entire dictionary.
Yarowsky (1995) first recognized that it is possible to use a small number of features for different senses to bootstrap an unsupervised word sense disambiguation system. $$$$$ Tested accuracy exceeds 96%.
Yarowsky (1995) first recognized that it is possible to use a small number of features for different senses to bootstrap an unsupervised word sense disambiguation system. $$$$$ Clearly, the claim holds with very high reliability for these words, and may be confidently exploited as another source of evidence in sense tagging.'

Self-training (Yarowsky, 1995) is a semi supervised algorithm which has been well studied in the NLP area and gained promising result. $$$$$ In this current work, the one-sense-per-discourse hypothesis was tested on a set of 37,232 examples (hand-tagged over a period of 3 years), the same data studied in the disambiguation experiments.
Self-training (Yarowsky, 1995) is a semi supervised algorithm which has been well studied in the NLP area and gained promising result. $$$$$ It is very much stronger for collocations with content words than those with function words.'
Self-training (Yarowsky, 1995) is a semi supervised algorithm which has been well studied in the NLP area and gained promising result. $$$$$ Note that the original seed words are no longer at the top of the list.

The algorithm proposed by Yarowsky (1995) for the problem of word sense disambiguation has been cited as the origination of self-training. $$$$$ Repeat Step 3 iteratively.
The algorithm proposed by Yarowsky (1995) for the problem of word sense disambiguation has been cited as the origination of self-training. $$$$$ The use of this property after each iteration is similar to the final post-hoc application, but helps prevent initially mistagged collocates from gaining a foothold.
The algorithm proposed by Yarowsky (1995) for the problem of word sense disambiguation has been cited as the origination of self-training. $$$$$ This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations.
The algorithm proposed by Yarowsky (1995) for the problem of word sense disambiguation has been cited as the origination of self-training. $$$$$ This differs from sense induction: using distributional similarity to partition word instances into clusters that may have no relation to standard sense partitions.

For the fine-grained track, it achieves 2nd place after that of Tugwell and Kilgarriff (2001), which used a decision list (Yarowsky, 1995) on manually selected corpora evidence for each inventory sense, and thus is not subject to loss of distinguishability in the glosses as Lesk variants are. $$$$$ This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations.
For the fine-grained track, it achieves 2nd place after that of Tugwell and Kilgarriff (2001), which used a decision list (Yarowsky, 1995) on manually selected corpora evidence for each inventory sense, and thus is not subject to loss of distinguishability in the glosses as Lesk variants are. $$$$$ The work reported here is the first to take advantage of this regularity in conjunction with separate models of local context for each word.
For the fine-grained track, it achieves 2nd place after that of Tugwell and Kilgarriff (2001), which used a decision list (Yarowsky, 1995) on manually selected corpora evidence for each inventory sense, and thus is not subject to loss of distinguishability in the glosses as Lesk variants are. $$$$$ This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations.
For the fine-grained track, it achieves 2nd place after that of Tugwell and Kilgarriff (2001), which used a decision list (Yarowsky, 1995) on manually selected corpora evidence for each inventory sense, and thus is not subject to loss of distinguishability in the glosses as Lesk variants are. $$$$$ It attempts to derive maximal leverage from these properties by modeling a rich diversity of collocational relationships.

Two more recent investigations are by Yarowsky, (Yarowsky, 1995), and later, Mihalcea, (Mihalcea,2002). $$$$$ Words in the entry appearing in the most reliable collocational relationships with the target word are given the most weight, based on the criteria given in Yarowsky (1993).
Two more recent investigations are by Yarowsky, (Yarowsky, 1995), and later, Mihalcea, (Mihalcea,2002). $$$$$ ... Nissan car and truck plant in Japan is ... ?
Two more recent investigations are by Yarowsky, (Yarowsky, 1995), and later, Mihalcea, (Mihalcea,2002). $$$$$ In brief, if several instances of the polysemous word in a discourse have already been assigned SENSE-A, this sense tag may be extended to all examples in the discourse, conditional on the relative numbers and the probabilities associated with the tagged examples.
Two more recent investigations are by Yarowsky, (Yarowsky, 1995), and later, Mihalcea, (Mihalcea,2002). $$$$$ ... computer disk drive plant located in ... polystyrene manufacturing plant at its Dow ... company manufacturing plant is in Orlando ...

Self-training (Yarowsky, 1995) is a form of semi-supervised learning. $$$$$ While not fully automatic, this approach yields rich and highly reliable seed sets with minimal work.
Self-training (Yarowsky, 1995) is a form of semi-supervised learning. $$$$$ Regrettably, this algorithm was only described in two sentences and was not developed further.
Self-training (Yarowsky, 1995) is a form of semi-supervised learning. $$$$$ At the end of Step 4, this property is used for error correction.
