Also, several models are proposed to address the problem of improving generative models with small amount of manual data, including Model 6 (Och and Ney, 2003) and the model proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ The training corpora are publicly available: both the Arabic/English data and the French/English Hansards were released by LDC.
Also, several models are proposed to address the problem of improving generative models with small amount of manual data, including Model 6 (Och and Ney, 2003) and the model proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ We do not have enough labels for this approach.
Also, several models are proposed to address the problem of improving generative models with small amount of manual data, including Model 6 (Och and Ney, 2003) and the model proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality.

Fraser and Marcu (2006) propose an EMD algorithm, where labeled data is used for discriminative reranking. $$$$$ We use the “union”, “refined” and “intersection” heuristics defined in (Och and Ney, 2003) which are used in conjunction with IBM Model 4 as the baseline in virtually all recent work on word alignment.
Fraser and Marcu (2006) propose an EMD algorithm, where labeled data is used for discriminative reranking. $$$$$ Given a vector of these weights a, the alignment search problem, i.e. the search to return the best alignment a� of the sentences e and f according to the model, is specified by Equation 2.
Fraser and Marcu (2006) propose an EMD algorithm, where labeled data is used for discriminative reranking. $$$$$ 4.
Fraser and Marcu (2006) propose an EMD algorithm, where labeled data is used for discriminative reranking. $$$$$ It is standard practice to improve the final alignments by combining the “F to E” and “E to F” directions using symmetrization heuristics.

With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. $$$$$ Callison-Burch et al. (2004) showed in their work on combining alignments of lower and higher quality that the alignments of higher quality should be given a much higher weight than the lower quality alignments.
With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. $$$$$ This work was partially supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No.
With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. $$$$$ Both sets have training data and two gold standard word alignments for small samples of the training data, which we use as the alignment discriminative training set and alignment test set.
With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. $$$$$ Nigam et al. (2000) addressed a text classification task.

Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998). $$$$$ We created the manual word alignments ourselves, following the Blinker guidelines (Melamed, 1998).
Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998). $$$$$ We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus.
Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998). $$$$$ We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality.

 $$$$$ For the “F to E” direction the models assign non-zero probability to alignments consisting of links from one Foreign word to zero or more English words, while for “E to F” the models assign non-zero probability to alignments consisting of links from one English word to zero or more Foreign words.
 $$$$$ HR001106-C-0022.
 $$$$$ (2000).

EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements. $$$$$ If these techniques are applied when there are a small number of labels in relation to the number of parameters used, they will suffer from the “overconfident pseudo-labeling problem” (Seeger, 2000), where the initial labels of poor quality assigned to the unlabeled data will dominate the model estimated in the M-step.
EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements. $$$$$ We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus.
EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements. $$$$$ They had access to 5000 gold standard word alignments, considerably more than the 100 or 110 gold standard word alignments used here.
EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements. $$$$$ We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality.

 $$$$$ Cherry and Lin (2003) and Taskar et al. (2005) compared their results with Model 4 using “intersection” by looking at AER (with the “Sure” versus “Possible” link distinction), and restricted themselves to considering 1-to-1 alignments.
 $$$$$ They had access to 5000 gold standard word alignments, considerably more than the 100 or 110 gold standard word alignments used here.
 $$$$$ We also show the F-measure after heuristic symmetrization of the alignment test sets.
 $$$$$ We created the manual word alignments ourselves, following the Blinker guidelines (Melamed, 1998).

For an alignment model, most of these use the Aachen HMM approach (Vogel et al, 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). $$$$$ The intuition behind this approach to semi-supervised training is that we wish to obtain the advantages of both discriminative training (error minimization) and approximate EM (which allows us to estimate a large numbers of parameters even though we have very few gold standard word alignments).
For an alignment model, most of these use the Aachen HMM approach (Vogel et al, 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). $$$$$ This approximation is called Viterbi training.
For an alignment model, most of these use the Aachen HMM approach (Vogel et al, 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). $$$$$ Using these alignments in our phrasal SMT system, we produced a statistically significant BLEU improvement (at a 95% confidence interval a gain of 0.78 is necessary) on the French/English task and a statistically significant BLEU improvement on the Arabic/English task (at a 95% confidence interval a gain of 1.2 is necessary).
For an alignment model, most of these use the Aachen HMM approach (Vogel et al, 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). $$$$$ The semi-supervised learning literature generally addresses augmenting supervised learning tasks with unlabeled data (Seeger, 2000).

 $$$$$ For each corpus we sampled 1000 sentence pairs randomly, with no sentence length restriction.
 $$$$$ 3.
 $$$$$ Table 8 shows our results.
 $$$$$ In Section 3, we present iteratively our contributions that eventually lead to absolute increases in alignment quality of 4.8% for French/English and 4.8% for Arabic/English, as measured using F-measure for large word alignment tasks.

If human-aligned data is available, the EMD algorithm provides higher baseline alignments than GIZA++ that have led to better MT performance (Fraser and Marcu, 2006). $$$$$ Intuitively, in approximate EM training for Model 4 (Brown et al., 1993), the E-step corresponds to calculating the probability of all alignments according to the current model estimate, while the M-step is the creation of a new model estimate given a probability distribution over alignments (calculated in the E-step).
If human-aligned data is available, the EMD algorithm provides higher baseline alignments than GIZA++ that have led to better MT performance (Fraser and Marcu, 2006). $$$$$ In the process, we also show that these improvements are explained not only by the power of the new models, but also by a novel search procedure for the alignment of highest probability.
If human-aligned data is available, the EMD algorithm provides higher baseline alignments than GIZA++ that have led to better MT performance (Fraser and Marcu, 2006). $$$$$ (Fraser and Marcu, 2006) established that tuning the trade-off between Precision and Recall in the F-Measure formula will lead to the best BLEU results.
If human-aligned data is available, the EMD algorithm provides higher baseline alignments than GIZA++ that have led to better MT performance (Fraser and Marcu, 2006). $$$$$ Ivanov et al. (2001) used discriminative training in a reinforcement learning context in a similar way to our adding of a discriminative training step to an unsupervised context.

We use the semi-supervised EMD algorithm (Fraser and Marcu, 2006b) to train the model. $$$$$ HR001106-C-0022.
We use the semi-supervised EMD algorithm (Fraser and Marcu, 2006b) to train the model. $$$$$ However, “union” and “refined” alignments, which are many-to-many, are what are used to build competitive phrasal SMT systems, because “intersection” performs poorly, despite having been shown to have the best AER scores for the French/English corpus we are using (Och and Ney, 2003).
We use the semi-supervised EMD algorithm (Fraser and Marcu, 2006b) to train the model. $$$$$ Another important difference with previous work is that we are concerned with generating many-to-many word alignments.
We use the semi-supervised EMD algorithm (Fraser and Marcu, 2006b) to train the model. $$$$$ In contrast, we augmented an unsupervised learning task with labeled data.

We compare semi-supervised LEAF with a previous state of the art semi-supervised system (Fraser and Marcu, 2006b). $$$$$ These contributions pertain to the casting of the training procedure in the discriminative framework (Section 3.1); the IBM model extensions and modified search procedure for the Viterbi alignments (Section 3.2); and the interleaved, minimum error/maximum likelihood, training algorithm (Section 4).
We compare semi-supervised LEAF with a previous state of the art semi-supervised system (Fraser and Marcu, 2006b). $$$$$ Table 8 shows our results.
We compare semi-supervised LEAF with a previous state of the art semi-supervised system (Fraser and Marcu, 2006b). $$$$$ The first line is the starting point, which is the Viterbi alignment of the 4th iteration of HMM training.

We ran the baseline semi-supervised system for two iterations (line 2), and in contrast with (Fraser and Marcu, 2006b) we found that the best symmetrization heuristic for this system was  union, which is most likely due to our use of fully linked alignments which was discussed at the end of Section 3. $$$$$ The best settings were α = 0.2 for Arabic/English and α = 0.7 for French/English, and these settings of α were used for every result reported in this paper.
We ran the baseline semi-supervised system for two iterations (line 2), and in contrast with (Fraser and Marcu, 2006b) we found that the best symmetrization heuristic for this system was  union, which is most likely due to our use of fully linked alignments which was discussed at the end of Section 3. $$$$$ In line 3, we run discriminative training using the algorithm from Section 3.1.
We ran the baseline semi-supervised system for two iterations (line 2), and in contrast with (Fraser and Marcu, 2006b) we found that the best symmetrization heuristic for this system was  union, which is most likely due to our use of fully linked alignments which was discussed at the end of Section 3. $$$$$ We tuned α by building a collection of alignments using our baseline system, measuring Precision and Recall against the alignment discriminative training set, building SMT systems and measuring resulting BLEU scores, and then searching for an appropriate α setting.

(Fraser and Marcu, 2006b) described symmetrized training of a 1-to-N log-linear model and a M-to-1 log-linear model. $$$$$ We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus.
(Fraser and Marcu, 2006b) described symmetrized training of a 1-to-N log-linear model and a M-to-1 log-linear model. $$$$$ See (Fraser and Marcu, 2006) for further details.
(Fraser and Marcu, 2006b) described symmetrized training of a 1-to-N log-linear model and a M-to-1 log-linear model. $$$$$ We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality.
(Fraser and Marcu, 2006b) described symmetrized training of a 1-to-N log-linear model and a M-to-1 log-linear model. $$$$$ Parameters have a superscript t representing their value at iteration t. We initialize the algorithm with the gold standard word alignments (labels) of the word alignment discriminative training set, an initial A, N, and the starting alignments (the iteration 4 HMM Viterbi alignment).

Examples of this line of research include Model 6 (OchandNey, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ We also show the F-measure after heuristic symmetrization of the alignment test sets.
Examples of this line of research include Model 6 (OchandNey, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ We tuned α by building a collection of alignments using our baseline system, measuring Precision and Recall against the alignment discriminative training set, building SMT systems and measuring resulting BLEU scores, and then searching for an appropriate α setting.
Examples of this line of research include Model 6 (OchandNey, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ The approximative stemming sub-model (sub-model 9) uses the first 4 letters of each vocabulary item as the stem for English and French while for Arabic we use the full word as the stem.
Examples of this line of research include Model 6 (OchandNey, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ The training corpora are publicly available: both the Arabic/English data and the French/English Hansards were released by LDC.

Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. $$$$$ Each sub-model hm has an associated weight am.
Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. $$$$$ (Fraser and Marcu, 2006) recently found serious problems with AER both empirically and analytically, which explains why optimizing AER frequently results in poor machine translation performance.
Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. $$$$$ We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus.
Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. $$$$$ We would like to thank the USC Center for High Performance Computing and Communications.

A super set of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). $$$$$ In the main loop in line 7 we align the full training set (similar to the E-step of EM), while in line 8 we estimate the iteration-dependent sub-models (similar to the M-step of EM).
A super set of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). $$$$$ Then we perform discriminative reranking in line 9 and check for convergence in lines 10 and 11 (convergence means that error was not decreased from the previous iteration).
A super set of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). $$$$$ The symmetrized alignments from the last iteration of EMD were used to build phrasal SMT systems, as were the symmetrized Model 4 alignments (the baseline).
A super set of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). $$$$$ We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality.

Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ We interleave discriminative training with EM and are therefore performing semi-supervised training.
Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ In Section 5, we assess the impact that our improved alignments have on MT quality.
Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ Using these alignments in our phrasal SMT system, we produced a statistically significant BLEU improvement (at a 95% confidence interval a gain of 0.78 is necessary) on the French/English task and a statistically significant BLEU improvement on the Arabic/English task (at a 95% confidence interval a gain of 1.2 is necessary).

Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ It is standard practice to improve the final alignments by combining the “F to E” and “E to F” directions using symmetrization heuristics.
Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ Then we perform discriminative reranking in line 9 and check for convergence in lines 10 and 11 (convergence means that error was not decreased from the previous iteration).
Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ We hope that Minimum Error / Maximum Likelihood training using the EMD algorithm can be used for a wide diversity of tasks where there is not enough labeled data to allow supervised estimation of an initial model of reasonable quality.
Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ Table 4 shows an evaluation of discriminative reranking.

(Fraser and Marcu, 2006) have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models. $$$$$ The symmetrized alignments from the last iteration of EMD were used to build phrasal SMT systems, as were the symmetrized Model 4 alignments (the baseline).
(Fraser and Marcu, 2006) have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models. $$$$$ To train our baseline systems we follow a standard procedure.
(Fraser and Marcu, 2006) have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models. $$$$$ The semi-supervised learning literature generally addresses augmenting supervised learning tasks with unlabeled data (Seeger, 2000).
