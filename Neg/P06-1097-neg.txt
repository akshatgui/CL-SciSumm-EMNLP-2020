Also, several models are proposed to address the problem of improving generative models with small amount of manual data, including Model 6 (Och and Ney, 2003) and the model proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ We observe: 1.
Also, several models are proposed to address the problem of improving generative models with small amount of manual data, including Model 6 (Och and Ney, 2003) and the model proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ We also performed an additional experiment for French/English aimed at understanding the potential contribution of the word aligned data without the new algorithm4.
Also, several models are proposed to address the problem of improving generative models with small amount of manual data, including Model 6 (Och and Ney, 2003) and the model proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ We use 1−F-measure as our error function, comparing hypothesized word alignments for the discriminative training set with the gold standard.

Fraser and Marcu (2006) propose an EMD algorithm, where labeled data is used for discriminative reranking. $$$$$ When compared with gold standard parallel data which was manually aligned using a high-recall/precision methodology (Melamed, 1998), the word-level alignments produced automatically have an F-measure accuracy of 64.6 and 76.4% (see Section 2 for details).
Fraser and Marcu (2006) propose an EMD algorithm, where labeled data is used for discriminative reranking. $$$$$ This work was partially supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No.
Fraser and Marcu (2006) propose an EMD algorithm, where labeled data is used for discriminative reranking. $$$$$ The semi-supervised learning literature generally addresses augmenting supervised learning tasks with unlabeled data (Seeger, 2000).
Fraser and Marcu (2006) propose an EMD algorithm, where labeled data is used for discriminative reranking. $$$$$ Using these alignments in our phrasal SMT system, we produced a statistically significant BLEU improvement (at a 95% confidence interval a gain of 0.78 is necessary) on the French/English task and a statistically significant BLEU improvement on the Arabic/English task (at a 95% confidence interval a gain of 1.2 is necessary).

With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. $$$$$ We interleave discriminative training with EM and are therefore performing semi-supervised training.
With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. $$$$$ Intuitively, in approximate EM training for Model 4 (Brown et al., 1993), the E-step corresponds to calculating the probability of all alignments according to the current model estimate, while the M-step is the creation of a new model estimate given a probability distribution over alignments (calculated in the E-step).
With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. $$$$$ They had access to 5000 gold standard word alignments, considerably more than the 100 or 110 gold standard word alignments used here.
With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. $$$$$ We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality.

Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998). $$$$$ For instance, in (Basu et al., 2004), the clustering system was supplied with pairs of instances labeled as belonging to the same or different clusters.
Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998). $$$$$ We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus.
Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998). $$$$$ They had access to 5000 gold standard word alignments, considerably more than the 100 or 110 gold standard word alignments used here.
Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998). $$$$$ The first evaluates the number of search errors.

 $$$$$ We also performed an additional experiment for French/English aimed at understanding the potential contribution of the word aligned data without the new algorithm4.
 $$$$$ Nigam et al. (2000) addressed a text classification task.
 $$$$$ In Section 5, we assess the impact that our improved alignments have on MT quality.
 $$$$$ We conduct experiments on alignment and translation tasks using Arabic/English and French/English data sets (see Table 1 for details).

EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements. $$$$$ However, there are tasks with large numbers of parameters where there are sufficient labels.
EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements. $$$$$ Like Ittycheriah and Roukos (2005), we converted the alignment discriminative training corpus links into a special corpus consisting of parallel sentences where each sentence consists only of a single word involved in the link.
EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements. $$$$$ The semi-supervised learning literature generally addresses augmenting supervised learning tasks with unlabeled data (Seeger, 2000).
EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements. $$$$$ We first recast the problem of estimating the IBM models (Brown et al., 1993) in a discriminative framework, which leads to an initial increase in word-alignment accuracy.

 $$$$$ We searched α = 0.1, 0.2, ..., 0.9 and set α so that the resulting F-measure tracks BLEU to the best extent possible.
 $$$$$ We presented a semi-supervised algorithm based on IBM Model 4, with modeling and search extensions, which produces alignments of improved F-measure over unsupervised Model 4 training.
 $$$$$ This work was partially supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No.
 $$$$$ We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality.

For an alignment model, most of these use the Aachen HMM approach (Vogel et al, 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). $$$$$ An additional translation set called the Maximum BLEU set is employed by the SMT system to train the weights associated with the components of its log-linear model (Och, 2003).
For an alignment model, most of these use the Aachen HMM approach (Vogel et al, 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). $$$$$ HR001106-C-0022.
For an alignment model, most of these use the Aachen HMM approach (Vogel et al, 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). $$$$$ In contrast, we augmented an unsupervised learning task with labeled data.
For an alignment model, most of these use the Aachen HMM approach (Vogel et al, 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). $$$$$ Previous work on discriminative training for wordalignment differed most strongly from our approach in that it generally views word-alignment as a supervised task.

 $$$$$ For instance, in (Basu et al., 2004), the clustering system was supplied with pairs of instances labeled as belonging to the same or different clusters.

If human-aligned data is available, the EMD algorithm provides higher baseline alignments than GIZA++ that have led to better MT performance (Fraser and Marcu, 2006). $$$$$ We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality.
If human-aligned data is available, the EMD algorithm provides higher baseline alignments than GIZA++ that have led to better MT performance (Fraser and Marcu, 2006). $$$$$ HR001106-C-0022.
If human-aligned data is available, the EMD algorithm provides higher baseline alignments than GIZA++ that have led to better MT performance (Fraser and Marcu, 2006). $$$$$ We would like to thank the USC Center for High Performance Computing and Communications.
If human-aligned data is available, the EMD algorithm provides higher baseline alignments than GIZA++ that have led to better MT performance (Fraser and Marcu, 2006). $$$$$ In contrast, we augmented an unsupervised learning task with labeled data.

We use the semi-supervised EMD algorithm (Fraser and Marcu, 2006b) to train the model. $$$$$ We observe: improvement for the first, second and third iterations.
We use the semi-supervised EMD algorithm (Fraser and Marcu, 2006b) to train the model. $$$$$ Nigam et al. (2000) addressed a text classification task.

We compare semi-supervised LEAF with a previous state of the art semi-supervised system (Fraser and Marcu, 2006b). $$$$$ The low F-measure scores of the baselines motivate our work.
We compare semi-supervised LEAF with a previous state of the art semi-supervised system (Fraser and Marcu, 2006b). $$$$$ The low F-measure scores of the baselines motivate our work.
We compare semi-supervised LEAF with a previous state of the art semi-supervised system (Fraser and Marcu, 2006b). $$$$$ We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality.
We compare semi-supervised LEAF with a previous state of the art semi-supervised system (Fraser and Marcu, 2006b). $$$$$ We extend the IBM models with new (sub)models, which leads to additional increases in word-alignment accuracy.

We ran the baseline semi-supervised system for two iterations (line 2), and in contrast with (Fraser and Marcu, 2006b) we found that the best symmetrization heuristic for this system was  union, which is most likely due to our use of fully linked alignments which was discussed at the end of Section 3. $$$$$ See (Fraser and Marcu, 2006) for further details.
We ran the baseline semi-supervised system for two iterations (line 2), and in contrast with (Fraser and Marcu, 2006b) we found that the best symmetrization heuristic for this system was  union, which is most likely due to our use of fully linked alignments which was discussed at the end of Section 3. $$$$$ Neal and Hinton (1998) analyze approximate EM training and motivate this type of variant.
We ran the baseline semi-supervised system for two iterations (line 2), and in contrast with (Fraser and Marcu, 2006b) we found that the best symmetrization heuristic for this system was  union, which is most likely due to our use of fully linked alignments which was discussed at the end of Section 3. $$$$$ Och (2003) has described an efficient exact one-dimensional error minimization technique for a similar search problem in machine translation.
We ran the baseline semi-supervised system for two iterations (line 2), and in contrast with (Fraser and Marcu, 2006b) we found that the best symmetrization heuristic for this system was  union, which is most likely due to our use of fully linked alignments which was discussed at the end of Section 3. $$$$$ The best settings were α = 0.2 for Arabic/English and α = 0.7 for French/English, and these settings of α were used for every result reported in this paper.

(Fraser and Marcu, 2006b) described symmetrized training of a 1-to-N log-linear model and a M-to-1 log-linear model. $$$$$ HR001106-C-0022.
(Fraser and Marcu, 2006b) described symmetrized training of a 1-to-N log-linear model and a M-to-1 log-linear model. $$$$$ The table shows that 4We would like to thank an anonymous reviewer for suggesting that this experiment would be useful even when using a small discriminative training corpus. our algorithm produces heuristically symmetrized final alignments of improved F-measure.
(Fraser and Marcu, 2006b) described symmetrized training of a 1-to-N log-linear model and a M-to-1 log-linear model. $$$$$ We also performed an additional experiment for French/English aimed at understanding the potential contribution of the word aligned data without the new algorithm4.

Examples of this line of research include Model 6 (OchandNey, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ We presented a semi-supervised algorithm based on IBM Model 4, with modeling and search extensions, which produces alignments of improved F-measure over unsupervised Model 4 training.
Examples of this line of research include Model 6 (OchandNey, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ Each sub-model hm has an associated weight am.
Examples of this line of research include Model 6 (OchandNey, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ We also show the F-measure after heuristic symmetrization of the alignment test sets.
Examples of this line of research include Model 6 (OchandNey, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ The semi-supervised learning literature generally addresses augmenting supervised learning tasks with unlabeled data (Seeger, 2000).

Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. $$$$$ We would like to thank the USC Center for High Performance Computing and Communications.
Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. $$$$$ This work was partially supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No.
Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. $$$$$ For instance, in (Basu et al., 2004), the clustering system was supplied with pairs of instances labeled as belonging to the same or different clusters.
Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. $$$$$ In line 2, we make iteration 0 estimates of the 5 sub-models of Model 4 and the 6 heuristic sub-models which are iteration dependent.

A super set of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). $$$$$ We used these alignments to produce translations of higher quality.
A super set of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). $$$$$ We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus.
A super set of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). $$$$$ We hope that Minimum Error / Maximum Likelihood training using the EMD algorithm can be used for a wide diversity of tasks where there is not enough labeled data to allow supervised estimation of an initial model of reasonable quality.

Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ We run Maximum BLEU (Och, 2003) for 25 iterations individually for each system.
Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality.
Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus.
Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ This work was partially supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No.

Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ In contrast, we augmented an unsupervised learning task with labeled data.
Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality.
Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ The technique involves calculating a piecewise constant function f,,,(x) which evaluates the error of the hypotheses which would be picked by equation 2 from a set of hypotheses if we hold all weights constant, except for the weight An (which is set to x).
Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ We use 1−F-measure as our error function, comparing hypothesized word alignments for the discriminative training set with the gold standard.

(Fraser and Marcu, 2006) have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models. $$$$$ In line 3, we run discriminative training using the algorithm from Section 3.1.
(Fraser and Marcu, 2006) have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models. $$$$$ To train our baseline systems we follow a standard procedure.
(Fraser and Marcu, 2006) have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models. $$$$$ When compared with gold standard parallel data which was manually aligned using a high-recall/precision methodology (Melamed, 1998), the word-level alignments produced automatically have an F-measure accuracy of 64.6 and 76.4% (see Section 2 for details).
