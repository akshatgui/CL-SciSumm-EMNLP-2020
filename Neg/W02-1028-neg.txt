
Given the endless amount of data we have at our disposal, many efforts have focused on mining knowledge from structured or unstructured text, including ground facts (Etzioni et al, 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). $$$$$ The subject of “robbed” identifies the perpetrator, while the direct object of “robbed” identifies the victim or target.
Given the endless amount of data we have at our disposal, many efforts have focused on mining knowledge from structured or unstructured text, including ground facts (Etzioni et al, 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). $$$$$ We evaluate Basilisk on six semantic categories.
Given the endless amount of data we have at our disposal, many efforts have focused on mining knowledge from structured or unstructured text, including ground facts (Etzioni et al, 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). $$$$$ The accuracy achieved by Basilisk is substantially higher than that of previous techniques for semantic lexicon induction on the MUC-4 corpus, and empirical results show that both of Basilisk’s ideas contribute to its performance.
Given the endless amount of data we have at our disposal, many efforts have focused on mining knowledge from structured or unstructured text, including ground facts (Etzioni et al, 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). $$$$$ In Figure 4, category C has claimed a significant number of words that belong to categories B and E. When generating a lexicon for one category at a time, these confusion errors are impossible to detect because the learner has no knowledge of the other categories.

(Thelen and Riloff, 2002) address this problem by learning multiple semantic categories simultaneously, relying on the often unrealistic assumption that a word cannot belong to more than one semantic category. $$$$$ After some number of iterations, all of the valid category members extracted by the top 20 patterns will have been added to the lexicon, leaving only non-category words left to consider.
(Thelen and Riloff, 2002) address this problem by learning multiple semantic categories simultaneously, relying on the often unrealistic assumption that a word cannot belong to more than one semantic category. $$$$$ Before we ran these experiments, one of the authors manually labeled every head noun in the corpus that was found by an extraction pattern.

Previous approaches to context pattern induction were described by Riloff and Jones (1999), Agichtein and Gravano (2000), Thelen and Riloff (2002), Lin et al (2003), and Etzioni et al (2005), among others. $$$$$ Even for relatively general texts, such as the Wall Street Journal (Marcus et al., 1993) or terrorism articles (MUC4 Proceedings, 1992), Roark and Charniak (Roark and Charniak, 1998) reported that 3 of every 5 terms generated by their semantic lexicon learner were not present in WordNet.
Previous approaches to context pattern induction were described by Riloff and Jones (1999), Agichtein and Gravano (2000), Thelen and Riloff (2002), Lin et al (2003), and Etzioni et al (2005), among others. $$$$$ Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts.
Previous approaches to context pattern induction were described by Riloff and Jones (1999), Agichtein and Gravano (2000), Thelen and Riloff (2002), Lin et al (2003), and Etzioni et al (2005), among others. $$$$$ For example, the score for each candidate LOCATION word will be its AvgLog score for the LOCATION category minus its maximum AvgLog score for all other categories.

Over the years, researchers have successfully shown how to build ground facts (Etzioni et al., 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). $$$$$ Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category.
Over the years, researchers have successfully shown how to build ground facts (Etzioni et al., 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). $$$$$ The accuracy achieved by Basilisk is substantially higher than that of previous techniques for semantic lexicon induction on the MUC-4 corpus, and empirical results show that both of Basilisk’s ideas contribute to its performance.

Next, we applied the Basilisk bootstrapping algorithm (Thelen and Riloff, 2002) to learn PPVs. $$$$$ The pattern pool and the candidate word pool are then emptied, and the bootstrapping process starts over again.
Next, we applied the Basilisk bootstrapping algorithm (Thelen and Riloff, 2002) to learn PPVs. $$$$$ To evaluate Basilisk’s performance, we ran experiments with the MUC-4 corpus (MUC-4 Proceedings, 1992), which contains 1700 texts associated with terrorism.

Similar approaches are used among others in (Thelen and Riloff, 2002) for learning semantic lexicons, in (Collins and Singer, 1999) for named entity recognition, and in (Fagni and Sebastiani, 2007) for hierarchical text categorization. $$$$$ We evaluate Basilisk on six semantic categories.
Similar approaches are used among others in (Thelen and Riloff, 2002) for learning semantic lexicons, in (Collins and Singer, 1999) for named entity recognition, and in (Fagni and Sebastiani, 2007) for hierarchical text categorization. $$$$$ The goal of the bootstrapping algorithm is to expand the area of hypothesized words so that it exactly matches the category’s true territory.

Thelen and Riloff (2002)'s bootstrapping method iteratively performs feature selection and word selection for each class. $$$$$ Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts.
Thelen and Riloff (2002)'s bootstrapping method iteratively performs feature selection and word selection for each class. $$$$$ If the shaded area expands beyond the category’s true territory, then incorrect words have been added to the lexicon.
Thelen and Riloff (2002)'s bootstrapping method iteratively performs feature selection and word selection for each class. $$$$$ Basilisk finds all patterns that extract “Peru” and computes the average number of known locations extracted by those patterns.

In Section 5, we report experiments using syntactic features shown to be useful by the above studies, and compare performance with Thelen and Riloff (2002)'s bootstrapping method. $$$$$ Riloff and Shepherd (Riloff and Shepherd, 1997) developed a bootstrapping algorithm that exploits lexical co-occurrence statistics, and Roark and Charniak (Roark and Charniak, 1998) refined this algorithm to focus more explicitly on certain syntactic structures.
In Section 5, we report experiments using syntactic features shown to be useful by the above studies, and compare performance with Thelen and Riloff (2002)'s bootstrapping method. $$$$$ The accuracy achieved by Basilisk is substantially higher than that of previous techniques for semantic lexicon induction on the MUC-4 corpus, and empirical results show that both of Basilisk’s ideas contribute to its performance.
In Section 5, we report experiments using syntactic features shown to be useful by the above studies, and compare performance with Thelen and Riloff (2002)'s bootstrapping method. $$$$$ The Y axes have different ranges because some categories are more prolific than others.

We use the following algorithms as baseline: EM, co training, and co-EM, as established techniques for learning from unlabeled data in general; the bootstrapping method proposed by Thelen and Riloff (2002) (hereafter, TRB and TR) as a state-of-the-art bootstrapping method designed for semantic lexicon construction. $$$$$ The semantic lexicons produced by Basilisk have higher precision than those produced by previous techniques, with several categories showing substantial improvement.
We use the following algorithms as baseline: EM, co training, and co-EM, as established techniques for learning from unlabeled data in general; the bootstrapping method proposed by Thelen and Riloff (2002) (hereafter, TRB and TR) as a state-of-the-art bootstrapping method designed for semantic lexicon construction. $$$$$ Although some semantic dictionaries do exist (e.g., WordNet (Miller, 1990)), these resources often do not contain the specialized vocabulary and jargon that is needed for specific domains.
We use the following algorithms as baseline: EM, co training, and co-EM, as established techniques for learning from unlabeled data in general; the bootstrapping method proposed by Thelen and Riloff (2002) (hereafter, TRB and TR) as a state-of-the-art bootstrapping method designed for semantic lexicon construction. $$$$$ The semantic lexicons produced by Basilisk have higher precision than those produced by previous techniques, with several categories showing substantial improvement.

The extraction of large sets of candidate facts opens the possibility of fast-growth iterative extraction, as opposed to the de-facto strategy of conservatively growing the seed set by as few as five items (Thelen and Riloff, 2002) after each iteration. $$$$$ The accuracy achieved by Basilisk is substantially higher than that of previous techniques for semantic lexicon induction on the MUC-4 corpus, and empirical results show that both of Basilisk’s ideas contribute to its performance.
The extraction of large sets of candidate facts opens the possibility of fast-growth iterative extraction, as opposed to the de-facto strategy of conservatively growing the seed set by as few as five items (Thelen and Riloff, 2002) after each iteration. $$$$$ Riloff and Shepherd (Riloff and Shepherd, 1997) developed a bootstrapping algorithm that exploits lexical co-occurrence statistics, and Roark and Charniak (Roark and Charniak, 1998) refined this algorithm to focus more explicitly on certain syntactic structures.
The extraction of large sets of candidate facts opens the possibility of fast-growth iterative extraction, as opposed to the de-facto strategy of conservatively growing the seed set by as few as five items (Thelen and Riloff, 2002) after each iteration. $$$$$ The accuracy achieved by Basilisk is substantially higher than that of previous techniques for semantic lexicon induction on the MUC-4 corpus, and empirical results show that both of Basilisk’s ideas contribute to its performance.

Pattern generalization is disabled, and the ranking of patterns and facts follows strictly the criteria and scoring functions from (Thelen and Riloff, 2002), which are also used in slightly different form in (Lita and Carbonell, 2004) and (Agichtein and Gravano,2000). $$$$$ These numbers represent a baseline: an algorithm that randomly selects words would be expected to get accuracies consistent with these numbers.
Pattern generalization is disabled, and the ranking of patterns and facts follows strictly the criteria and scoring functions from (Thelen and Riloff, 2002), which are also used in slightly different form in (Lita and Carbonell, 2004) and (Agichtein and Gravano,2000). $$$$$ Basilisk’s bootstrapping algorithm exploits two ideas: (1) collective evidence from extraction patterns can be used to infer semantic category associations, and (2) learning multiple semantic categories simultaneously can help constrain the bootstrapping process.
Pattern generalization is disabled, and the ranking of patterns and facts follows strictly the criteria and scoring functions from (Thelen and Riloff, 2002), which are also used in slightly different form in (Lita and Carbonell, 2004) and (Agichtein and Gravano,2000). $$$$$ For example, three patterns that would extract people are: “<subject> was arrested”, “murdered <direct object>”, and “collaborated with <pp object>”.

As a more realistic compromise over overly-cautious acquisition, the baseline run retains as many of the top candidate facts as the size of the current seed, whereas (Thelen and Riloff, 2002) only add the top five candidate facts to the seed set after each iteration. $$$$$ Caraballo (Caraballo, 1999) and Hearst (Hearst, 1992) created techniques to learn hypernym/hyponym relationships.
As a more realistic compromise over overly-cautious acquisition, the baseline run retains as many of the top candidate facts as the size of the current seed, whereas (Thelen and Riloff, 2002) only add the top five candidate facts to the seed set after each iteration. $$$$$ We also demonBuilding: theatre store cathedral temple palace penitentiary academy houses school mansions Event: ambush assassination uprisings sabotage takeover incursion kidnappings clash shoot-out Human: boys snipers detainees commandoes extremists deserter narcoterrorists demonstrators cronies missionaries Location: suburb Soyapango capital Oslo regions cities neighborhoods Quito corregimiento Time: afternoon evening decade hour March weeks Saturday eve anniversary Wednesday Weapon: cannon grenade launchers firebomb car-bomb rifle pistol machineguns firearms strated that learning multiple semantic categories simultaneously improves the meta-bootstrapping algorithm, which suggests that this is a general observation which may improve other bootstrapping algorithms as well.

Multi-category algorithms out perform MLB (Thelen and Riloff, 2002), and we focus on these algorithms in our experiments. $$$$$ Basilisk’s bootstrapping algorithm exploits two ideas: (1) collective evidence from extraction patterns can be used to infer semantic category associations, and (2) learning multiple semantic categories simultaneously can help constrain the bootstrapping process.
Multi-category algorithms out perform MLB (Thelen and Riloff, 2002), and we focus on these algorithms in our experiments. $$$$$ We evaluate Basilisk on six semantic categories.

In BASILISK (Thelen and Riloff, 2002), candidate terms are ranked highly if they have strong evidence for a category and little or no evidence for other categories. $$$$$ The five best candidate words are added to the lexicon, and the process starts over again.
In BASILISK (Thelen and Riloff, 2002), candidate terms are ranked highly if they have strong evidence for a category and little or no evidence for other categories. $$$$$ We evaluate Basilisk on six semantic categories.
In BASILISK (Thelen and Riloff, 2002), candidate terms are ranked highly if they have strong evidence for a category and little or no evidence for other categories. $$$$$ We evaluate Basilisk on six semantic categories.
In BASILISK (Thelen and Riloff, 2002), candidate terms are ranked highly if they have strong evidence for a category and little or no evidence for other categories. $$$$$ The accuracy achieved by Basilisk is substantially higher than that of previous techniques for semantic lexicon induction on the MUC-4 corpus, and empirical results show that both of Basilisk’s ideas contribute to its performance.

To improve the seeds, the frequency of the potential seeds in the corpora is often considered, on the assumption that highly frequent seeds are better (Thelen and Riloff, 2002). $$$$$ We believe that learning individual nouns is a more conservative approach because noun phrases often overlap (e.g., “high-power bombs” and “incendiary bombs” would count as two different lexicon entries in the original meta-bootstrapping algorithm).
To improve the seeds, the frequency of the potential seeds in the corpora is often considered, on the assumption that highly frequent seeds are better (Thelen and Riloff, 2002). $$$$$ Basilisk’s bootstrapping algorithm exploits two ideas: (1) collective evidence from extraction patterns can be used to infer semantic category associations, and (2) learning multiple semantic categories simultaneously can help constrain the bootstrapping process.
To improve the seeds, the frequency of the potential seeds in the corpora is often considered, on the assumption that highly frequent seeds are better (Thelen and Riloff, 2002). $$$$$ We will use the abbreviation 1CAT to indicate that only one semantic category was bootstrapped, and MCAT to indicate that multiple semantic categories were simultaneously bootstrapped.
To improve the seeds, the frequency of the potential seeds in the corpora is often considered, on the assumption that highly frequent seeds are better (Thelen and Riloff, 2002). $$$$$ Three semantic lexicon learners have previously been evaluated on the MUC-4 corpus (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Riloff and Jones, 1999), and of these meta-bootstrapping achieved the best results.

This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). $$$$$ The five best candidate words are added to the lexicon, and the process starts over again.
This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). $$$$$ Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts.
This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). $$$$$ The five best candidate words are added to the lexicon, and the process starts over again.
This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). $$$$$ Basilisk hypothesizes the semantic class of a word by gathering collective evidence about semantic associations from extraction pattern contexts.

Seed-based supervision is closely related to the idea of seeding in the bootstrapping literature for learning semantic lexicons (Thelen and Riloff, 2002). $$$$$ Basilisk uses a value of N=20 for the first iteration, which allows a variety of patterns to be considered, yet is small enough that all of the patterns are strongly associated with the category.1 The purpose of the pattern pool is to narrow down the field of candidates for the lexicon.
Seed-based supervision is closely related to the idea of seeding in the bootstrapping literature for learning semantic lexicons (Thelen and Riloff, 2002). $$$$$ These manual annotations were the gold standard.
Seed-based supervision is closely related to the idea of seeding in the bootstrapping literature for learning semantic lexicons (Thelen and Riloff, 2002). $$$$$ We also demonBuilding: theatre store cathedral temple palace penitentiary academy houses school mansions Event: ambush assassination uprisings sabotage takeover incursion kidnappings clash shoot-out Human: boys snipers detainees commandoes extremists deserter narcoterrorists demonstrators cronies missionaries Location: suburb Soyapango capital Oslo regions cities neighborhoods Quito corregimiento Time: afternoon evening decade hour March weeks Saturday eve anniversary Wednesday Weapon: cannon grenade launchers firebomb car-bomb rifle pistol machineguns firearms strated that learning multiple semantic categories simultaneously improves the meta-bootstrapping algorithm, which suggests that this is a general observation which may improve other bootstrapping algorithms as well.
Seed-based supervision is closely related to the idea of seeding in the bootstrapping literature for learning semantic lexicons (Thelen and Riloff, 2002). $$$$$ The algorithm most closely related to Basilisk is meta-bootstrapping (Riloff and Jones, 1999), which also uses extraction pattern contexts for semantic lexicon induction.

Thelen and Riloff (2002) use a bootstrapping algorithm to learn semantic lexicons of nouns for six semantic categories, one of which is EVENTS. $$$$$ We evaluate Basilisk on six semantic categories.
Thelen and Riloff (2002) use a bootstrapping algorithm to learn semantic lexicons of nouns for six semantic categories, one of which is EVENTS. $$$$$ Several learning algorithms have also been developed for named entity recognition (e.g., (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999)).
Thelen and Riloff (2002) use a bootstrapping algorithm to learn semantic lexicons of nouns for six semantic categories, one of which is EVENTS. $$$$$ Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category.

Multi-category bootstrapping algorithms, such as Basilisk (Thelen and Riloff, 2002), NOMEN (Yangarber et al, 2002), and WMEB (McIntosh and Curran, 2008), aim to reduce semantic drift by extracting multiple semantic categories simultaneously. $$$$$ In recent years, several algorithms have been developed to acquire semantic lexicons automatically or semi-automatically using corpus-based techniques.
Multi-category bootstrapping algorithms, such as Basilisk (Thelen and Riloff, 2002), NOMEN (Yangarber et al, 2002), and WMEB (McIntosh and Curran, 2008), aim to reduce semantic drift by extracting multiple semantic categories simultaneously. $$$$$ Finally, we present results showing that learning multiple semantic categories simultaneously improves performance.
Multi-category bootstrapping algorithms, such as Basilisk (Thelen and Riloff, 2002), NOMEN (Yangarber et al, 2002), and WMEB (McIntosh and Curran, 2008), aim to reduce semantic drift by extracting multiple semantic categories simultaneously. $$$$$ If more than one category tries to claim a word, then we use conflict resolution to decide which category should win.
Multi-category bootstrapping algorithms, such as Basilisk (Thelen and Riloff, 2002), NOMEN (Yangarber et al, 2002), and WMEB (McIntosh and Curran, 2008), aim to reduce semantic drift by extracting multiple semantic categories simultaneously. $$$$$ Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category.
