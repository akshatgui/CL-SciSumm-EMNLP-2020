However, (Thelen and Riloff, 2002) did not focus on the issue of convergence, and on leveraging negative categories to achieve or improve convergence. $$$$$ Although some semantic dictionaries do exist (e.g., WordNet (Miller, 1990)), these resources often do not contain the specialized vocabulary and jargon that is needed for specific domains.
However, (Thelen and Riloff, 2002) did not focus on the issue of convergence, and on leveraging negative categories to achieve or improve convergence. $$$$$ If the shaded area expands beyond the category’s true territory, then incorrect words have been added to the lexicon.
However, (Thelen and Riloff, 2002) did not focus on the issue of convergence, and on leveraging negative categories to achieve or improve convergence. $$$$$ If the lexicons cannot overlap, then we constrain the ability of a category to overstep its bounds.
However, (Thelen and Riloff, 2002) did not focus on the issue of convergence, and on leveraging negative categories to achieve or improve convergence. $$$$$ Finally, we present results showing that learning multiple semantic categories simultaneously improves performance.

Given the endless amount of data we have at our disposal, many efforts have focused on mining knowledge from structured or unstructured text, including ground facts (Etzioni et al, 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). $$$$$ We incorporated a simple conflict resolution procedure into Basilisk, as well as the metabootstrapping algorithm.
Given the endless amount of data we have at our disposal, many efforts have focused on mining knowledge from structured or unstructured text, including ground facts (Etzioni et al, 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). $$$$$ We were surprised that the improvement for meta-bootstrapping was much We also measured the recall of Basilisk’s lexicons after 1000 words had been learned, based on the gold standard data shown in Table 1.

(Thelen and Riloff, 2002) address this problem by learning multiple semantic categories simultaneously, relying on the often unrealistic assumption that a word cannot belong to more than one semantic category. $$$$$ The accuracy achieved by Basilisk is substantially higher than that of previous techniques for semantic lexicon induction on the MUC-4 corpus, and empirical results show that both of Basilisk’s ideas contribute to its performance.
(Thelen and Riloff, 2002) address this problem by learning multiple semantic categories simultaneously, relying on the often unrealistic assumption that a word cannot belong to more than one semantic category. $$$$$ As the bootstrapping progresses, using the same value N=20 causes the candidate pool to become stagnant.
(Thelen and Riloff, 2002) address this problem by learning multiple semantic categories simultaneously, relying on the often unrealistic assumption that a word cannot belong to more than one semantic category. $$$$$ Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category.

Previous approaches to context pattern induction were described by Riloff and Jones (1999), Agichtein and Gravano (2000), Thelen and Riloff (2002), Lin et al (2003), and Etzioni et al (2005), among others. $$$$$ It is worth noting that Basilisk’s performance held up well on the human and location categories even at the end, achieving 79.5% (795/1000) accuracy for humans and 53.2% (532/1000) accuracy for locations.
Previous approaches to context pattern induction were described by Riloff and Jones (1999), Agichtein and Gravano (2000), Thelen and Riloff (2002), Lin et al (2003), and Etzioni et al (2005), among others. $$$$$ We modified Basilisk’s scoring function to prefer words that have strong evidence for one category but little or no evidence for competing categories.
Previous approaches to context pattern induction were described by Riloff and Jones (1999), Agichtein and Gravano (2000), Thelen and Riloff (2002), Lin et al (2003), and Etzioni et al (2005), among others. $$$$$ However, meta-bootstrapping produces category-specific extraction patterns in addition to a semantic lexicon, while Basilisk focuses exclusively on semantic lexicon induction.
Previous approaches to context pattern induction were described by Riloff and Jones (1999), Agichtein and Gravano (2000), Thelen and Riloff (2002), Lin et al (2003), and Etzioni et al (2005), among others. $$$$$ Basilisk hypothesizes the semantic class of a word by gathering collective evidence about semantic associations from extraction pattern contexts.

Over the years, researchers have successfully shown how to build ground facts (Etzioni et al., 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). $$$$$ However, meta-bootstrapping produces category-specific extraction patterns in addition to a semantic lexicon, while Basilisk focuses exclusively on semantic lexicon induction.
Over the years, researchers have successfully shown how to build ground facts (Etzioni et al., 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). $$$$$ The accuracy achieved by Basilisk is substantially higher than that of previous techniques for semantic lexicon induction on the MUC-4 corpus, and empirical results show that both of Basilisk’s ideas contribute to its performance.
Over the years, researchers have successfully shown how to build ground facts (Etzioni et al., 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). $$$$$ The hypothesized words in the growing lexicon are represented by a shaded area.

Next, we applied the Basilisk bootstrapping algorithm (Thelen and Riloff, 2002) to learn PPVs. $$$$$ We believe that learning individual nouns is a more conservative approach because noun phrases often overlap (e.g., “high-power bombs” and “incendiary bombs” would count as two different lexicon entries in the original meta-bootstrapping algorithm).
Next, we applied the Basilisk bootstrapping algorithm (Thelen and Riloff, 2002) to learn PPVs. $$$$$ The semantic lexicons produced by Basilisk have higher precision than those produced by previous techniques, with several categories showing substantial improvement.
Next, we applied the Basilisk bootstrapping algorithm (Thelen and Riloff, 2002) to learn PPVs. $$$$$ The goal of the bootstrapping algorithm is to expand the area of hypothesized words so that it exactly matches the category’s true territory.
Next, we applied the Basilisk bootstrapping algorithm (Thelen and Riloff, 2002) to learn PPVs. $$$$$ In recent years, several algorithms have been developed to acquire semantic lexicons automatically or semi-automatically using corpus-based techniques.

Similar approaches are used among others in (Thelen and Riloff, 2002) for learning semantic lexicons, in (Collins and Singer, 1999) for named entity recognition, and in (Fagni and Sebastiani, 2007) for hierarchical text categorization. $$$$$ The algorithm most closely related to Basilisk is meta-bootstrapping (Riloff and Jones, 1999), which also uses extraction pattern contexts for semantic lexicon induction.
Similar approaches are used among others in (Thelen and Riloff, 2002) for learning semantic lexicons, in (Collins and Singer, 1999) for named entity recognition, and in (Fagni and Sebastiani, 2007) for hierarchical text categorization. $$$$$ Although some semantic dictionaries do exist (e.g., WordNet (Miller, 1990)), these resources often do not contain the specialized vocabulary and jargon that is needed for specific domains.
Similar approaches are used among others in (Thelen and Riloff, 2002) for learning semantic lexicons, in (Collins and Singer, 1999) for named entity recognition, and in (Fagni and Sebastiani, 2007) for hierarchical text categorization. $$$$$ The accuracy achieved by Basilisk is substantially higher than that of previous techniques for semantic lexicon induction on the MUC-4 corpus, and empirical results show that both of Basilisk’s ideas contribute to its performance.

Thelen and Riloff (2002)'s bootstrapping method iteratively performs feature selection and word selection for each class. $$$$$ In general, this is not true because words are often polysemous.
Thelen and Riloff (2002)'s bootstrapping method iteratively performs feature selection and word selection for each class. $$$$$ Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category.
Thelen and Riloff (2002)'s bootstrapping method iteratively performs feature selection and word selection for each class. $$$$$ For example, consider the verb “robbed” when it occurs in the active voice.

In Section 5, we report experiments using syntactic features shown to be useful by the above studies, and compare performance with Thelen and Riloff (2002)'s bootstrapping method. $$$$$ Although some semantic dictionaries do exist (e.g., WordNet (Miller, 1990)), these resources often do not contain the specialized vocabulary and jargon that is needed for specific domains.
In Section 5, we report experiments using syntactic features shown to be useful by the above studies, and compare performance with Thelen and Riloff (2002)'s bootstrapping method. $$$$$ The semantic lexicons produced by Basilisk have higher precision than those produced by previous techniques, with several categories showing substantial improvement.
In Section 5, we report experiments using syntactic features shown to be useful by the above studies, and compare performance with Thelen and Riloff (2002)'s bootstrapping method. $$$$$ Each category owns a certain territory within the space (demarcated with a dashed line), representing the words that are true members of that category.

We use the following algorithms as baseline $$$$$ Third, we explore the idea of learning multiple semantic categories simultaneously by adding this capability to Basilisk as well as another bootstrapping algorithm.
We use the following algorithms as baseline $$$$$ The accuracy achieved by Basilisk is substantially higher than that of previous techniques for semantic lexicon induction on the MUC-4 corpus, and empirical results show that both of Basilisk’s ideas contribute to its performance.
We use the following algorithms as baseline $$$$$ Second, we present empirical results showing that Basilisk outperforms a previous algorithm.
We use the following algorithms as baseline $$$$$ Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts.

The extraction of large sets of candidate facts opens the possibility of fast-growth iterative extraction, as opposed to the de-facto strategy of conservatively growing the seed set by as few as five items (Thelen and Riloff, 2002) after each iteration. $$$$$ Before we ran these experiments, one of the authors manually labeled every head noun in the corpus that was found by an extraction pattern.
The extraction of large sets of candidate facts opens the possibility of fast-growth iterative extraction, as opposed to the de-facto strategy of conservatively growing the seed set by as few as five items (Thelen and Riloff, 2002) after each iteration. $$$$$ The easiest way to take advantage of multiple categories is to add simple conflict resolution that enforces the “one sense per domain” constraint.
The extraction of large sets of candidate facts opens the possibility of fast-growth iterative extraction, as opposed to the de-facto strategy of conservatively growing the seed set by as few as five items (Thelen and Riloff, 2002) after each iteration. $$$$$ Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category.

Pattern generalization is disabled, and the ranking of patterns and facts follows strictly the criteria and scoring functions from (Thelen and Riloff, 2002), which are also used in slightly different form in (Lita and Carbonell, 2004) and (Agichtein and Gravano,2000). $$$$$ As we will demonstrate in Section 2.2, Basilisk’s approach produces better results than meta-bootstrapping and is also considerably more efficient because it uses only a single bootstrapping loop (meta-bootstrapping uses nested bootstrapping).

As a more realistic compromise over overly-cautious acquisition, the baseline run retains as many of the top candidate facts as the size of the current seed, whereas (Thelen and Riloff, 2002) only add the top five candidate facts to the seed set after each iteration. $$$$$ For the human and location categories, Basilisk learned hundreds of words, with accuracies in the 80-89% range through much of the bootstrapping.
As a more realistic compromise over overly-cautious acquisition, the baseline run retains as many of the top candidate facts as the size of the current seed, whereas (Thelen and Riloff, 2002) only add the top five candidate facts to the seed set after each iteration. $$$$$ Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts.
As a more realistic compromise over overly-cautious acquisition, the baseline run retains as many of the top candidate facts as the size of the current seed, whereas (Thelen and Riloff, 2002) only add the top five candidate facts to the seed set after each iteration. $$$$$ The goal of the bootstrapping algorithm is to expand the area of hypothesized words so that it exactly matches the category’s true territory.

Multi-category algorithms out perform MLB (Thelen and Riloff, 2002), and we focus on these algorithms in our experiments. $$$$$ It is worth noting that Basilisk’s performance held up well on the human and location categories even at the end, achieving 79.5% (795/1000) accuracy for humans and 53.2% (532/1000) accuracy for locations.
Multi-category algorithms out perform MLB (Thelen and Riloff, 2002), and we focus on these algorithms in our experiments. $$$$$ Most categories show small performance gains, with the BUILDING, LOCATION, and WEAPON categories benefitting the most.
Multi-category algorithms out perform MLB (Thelen and Riloff, 2002), and we focus on these algorithms in our experiments. $$$$$ We also demonBuilding: theatre store cathedral temple palace penitentiary academy houses school mansions Event: ambush assassination uprisings sabotage takeover incursion kidnappings clash shoot-out Human: boys snipers detainees commandoes extremists deserter narcoterrorists demonstrators cronies missionaries Location: suburb Soyapango capital Oslo regions cities neighborhoods Quito corregimiento Time: afternoon evening decade hour March weeks Saturday eve anniversary Wednesday Weapon: cannon grenade launchers firebomb car-bomb rifle pistol machineguns firearms strated that learning multiple semantic categories simultaneously improves the meta-bootstrapping algorithm, which suggests that this is a general observation which may improve other bootstrapping algorithms as well.

In BASILISK (Thelen and Riloff, 2002), candidate terms are ranked highly if they have strong evidence for a category and little or no evidence for other categories. $$$$$ The average number of category members extracted by these patterns will be 5.
In BASILISK (Thelen and Riloff, 2002), candidate terms are ranked highly if they have strong evidence for a category and little or no evidence for other categories. $$$$$ A word receives a high score if it is extracted by patterns that also have a tendency to extract known category members.
In BASILISK (Thelen and Riloff, 2002), candidate terms are ranked highly if they have strong evidence for a category and little or no evidence for other categories. $$$$$ We will use the abbreviation 1CAT to indicate that only one semantic category was bootstrapped, and MCAT to indicate that multiple semantic categories were simultaneously bootstrapped.

To improve the seeds, the frequency of the potential seeds in the corpora is often considered, on the assumption that highly frequent seeds are better (Thelen and Riloff, 2002). $$$$$ This paper describes a bootstrapping algorithm called Basilisk that learns highquality semantic lexicons for multiple categories.
To improve the seeds, the frequency of the potential seeds in the corpora is often considered, on the assumption that highly frequent seeds are better (Thelen and Riloff, 2002). $$$$$ Several learning algorithms have also been developed for named entity recognition (e.g., (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999)).
To improve the seeds, the frequency of the potential seeds in the corpora is often considered, on the assumption that highly frequent seeds are better (Thelen and Riloff, 2002). $$$$$ Even for relatively general texts, such as the Wall Street Journal (Marcus et al., 1993) or terrorism articles (MUC4 Proceedings, 1992), Roark and Charniak (Roark and Charniak, 1998) reported that 3 of every 5 terms generated by their semantic lexicon learner were not present in WordNet.
To improve the seeds, the frequency of the potential seeds in the corpora is often considered, on the assumption that highly frequent seeds are better (Thelen and Riloff, 2002). $$$$$ We evaluate Basilisk on six semantic categories.

This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). $$$$$ Basilisk’s bootstrapping algorithm exploits two ideas: (1) collective evidence from extraction patterns can be used to infer semantic category associations, and (2) learning multiple semantic categories simultaneously can help constrain the bootstrapping process.
This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). $$$$$ A word is ranked highly only if it has a high score for the targeted category and there is little evidence that it belongs to a different category.
This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). $$$$$ The five best candidate words are added to the lexicon, and the process starts over again.
This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). $$$$$ All extraction patterns are used during this step, not just the patterns in the pattern pool.

Seed-based supervision is closely related to the idea of seeding in the bootstrapping literature for learning semantic lexicons (Thelen and Riloff, 2002). $$$$$ We have developed a weakly supervised bootstrapping algorithm called Basilisk that automatically generates semantic lexicons.
Seed-based supervision is closely related to the idea of seeding in the bootstrapping literature for learning semantic lexicons (Thelen and Riloff, 2002). $$$$$ Semantic class information has proven to be useful for many natural language processing tasks, including information extraction (Riloff and Schmelzenbach, 1998; Soderland et al., 1995), anaphora resolution (Aone and Bennett, 1996), question answering (Moldovan et al., 1999; Hirschman et al., 1999), and prepositional phrase attachment (Brill and Resnik, 1994).

Thelen and Riloff (2002) use a bootstrapping algorithm to learn semantic lexicons of nouns for six semantic categories, one of which is EVENTS. $$$$$ Second, we present empirical results showing that Basilisk outperforms a previous algorithm.
Thelen and Riloff (2002) use a bootstrapping algorithm to learn semantic lexicons of nouns for six semantic categories, one of which is EVENTS. $$$$$ We evaluate Basilisk on six semantic categories.

Multi-category bootstrapping algorithms, such as Basilisk (Thelen and Riloff, 2002), NOMEN (Yangarber et al, 2002), and WMEB (McIntosh and Curran, 2008), aim to reduce semantic drift by extracting multiple semantic categories simultaneously. $$$$$ We evaluate Basilisk on six semantic categories.
Multi-category bootstrapping algorithms, such as Basilisk (Thelen and Riloff, 2002), NOMEN (Yangarber et al, 2002), and WMEB (McIntosh and Curran, 2008), aim to reduce semantic drift by extracting multiple semantic categories simultaneously. $$$$$ Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts.
Multi-category bootstrapping algorithms, such as Basilisk (Thelen and Riloff, 2002), NOMEN (Yangarber et al, 2002), and WMEB (McIntosh and Curran, 2008), aim to reduce semantic drift by extracting multiple semantic categories simultaneously. $$$$$ Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts.
Multi-category bootstrapping algorithms, such as Basilisk (Thelen and Riloff, 2002), NOMEN (Yangarber et al, 2002), and WMEB (McIntosh and Curran, 2008), aim to reduce semantic drift by extracting multiple semantic categories simultaneously. $$$$$ For example, let’s assume that Basilisk performs perfectly, adding only valid category words to the lexicon.
