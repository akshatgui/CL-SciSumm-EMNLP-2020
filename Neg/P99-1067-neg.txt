On the other hand, the alternative approach using comparable or unrelated text corpora were studied by Rapp (1999) and Fung et al (1998). $$$$$ In these vectors, all entries belonging to words not found in the English part of the base lexicon were deleted.
On the other hand, the alternative approach using comparable or unrelated text corpora were studied by Rapp (1999) and Fung et al (1998). $$$$$ By simply assuming an initial lexicon the large number of permutations to be considered is reduced to a much smaller number of vector comparisons.
On the other hand, the alternative approach using comparable or unrelated text corpora were studied by Rapp (1999) and Fung et al (1998). $$$$$ Instead of computing a single co-occurrence vector for a word A, we compute several, one for each position within the window.

For instance, good results are obtained from large corpora several million words for which the accuracy of the proposed translation is between 76% (Fung, 1998) and 89% (Rapp, 1999) for the first 20 candidates. $$$$$ We translate all known words in this vector to the target language.
For instance, good results are obtained from large corpora several million words for which the accuracy of the proposed translation is between 76% (Fung, 1998) and 89% (Rapp, 1999) for the first 20 candidates. $$$$$ Results based on mutual information or co-occurrence counts were significantly worse.

This can be accomplished as in Rapp (1999) and Schafer and Yarowsky (2002) by creating bag-of-words context vectors around both the source and target language words and then projecting the source vectors into the (English) target space via the current small translation dictionary. $$$$$ 3 The limitation to words with frequencies above 99 was introduced for computational reasons to reduce the number of vector comparisons and thus speed up the program.
This can be accomplished as in Rapp (1999) and Schafer and Yarowsky (2002) by creating bag-of-words context vectors around both the source and target language words and then projecting the source vectors into the (English) target space via the current small translation dictionary. $$$$$ Whereas for parallel texts in some studies up to 99% of the word alignments have been shown to be correct, the accuracy for non-parallel texts has been around 30% up to now.
This can be accomplished as in Rapp (1999) and Schafer and Yarowsky (2002) by creating bag-of-words context vectors around both the source and target language words and then projecting the source vectors into the (English) target space via the current small translation dictionary. $$$$$ However, it must be emphasized that their result has been achieved under very different circumstances.

Here, a standard technique of estimating bilingual term correspondences from com parable corpora (e.g., Fung and Yee (1998) and Rapp (1999)) is employed. $$$$$ In this situation, Rapp (1995) proposed using a clue different from the three mentioned above: His co-occurrence clue is based on the assumption that there is a correlation between cooccurrence patterns in different languages.
Here, a standard technique of estimating bilingual term correspondences from com parable corpora (e.g., Fung and Yee (1998) and Rapp (1999)) is employed. $$$$$ In all three cases, the problem of robustness — as observed when applying the word-order clue to parallel corpora — is not severe.
Here, a standard technique of estimating bilingual term correspondences from com parable corpora (e.g., Fung and Yee (1998) and Rapp (1999)) is employed. $$$$$ Others reduce the size of the co-occurrence matrices by performing a singular value decomposition.
Here, a standard technique of estimating bilingual term correspondences from com parable corpora (e.g., Fung and Yee (1998) and Rapp (1999)) is employed. $$$$$ A much more severe problem is that our current approach cannot properly handle ambiguities: For the German word weifi it does not predict white, but instead know.

For example, Rapp (1999) filtered out bilingual term pairs with low monolingual frequencies (those below 100 times), while Fung and Yee (1998) restricted candidate bilingual term pairs to be pairs of the most frequent 118 unknown words. $$$$$ Since our base lexicon is small, only some of the translations are known.
For example, Rapp (1999) filtered out bilingual term pairs with low monolingual frequencies (those below 100 times), while Fung and Yee (1998) restricted candidate bilingual term pairs to be pairs of the most frequent 118 unknown words. $$$$$ Whereas for parallel texts in some studies up to 99% of the word alignments have been shown to be correct, the accuracy for non-parallel texts has been around 30% up to now.
For example, Rapp (1999) filtered out bilingual term pairs with low monolingual frequencies (those below 100 times), while Fung and Yee (1998) restricted candidate bilingual term pairs to be pairs of the most frequent 118 unknown words. $$$$$ This task is more difficult, because most statistical clues useful in the processing of parallel texts cannot be applied to non-parallel texts.

The approach we investigate for identifying term translations in comparable corpora is similar to (Rapp, 1999) and many others. $$$$$ The validity of the co-occurrence clue is obvious for parallel corpora, but — as empirically shown by Rapp — it also holds for non-parallel corpora.
The approach we investigate for identifying term translations in comparable corpora is similar to (Rapp, 1999) and many others. $$$$$ One possibility would be to semantically disambiguate the words in the corpora beforehand, another to look at co-occurrences between significant word sequences instead of co-occurrences between single words.
The approach we investigate for identifying term translations in comparable corpora is similar to (Rapp, 1999) and many others. $$$$$ However, as Rapp (1995) proposed, this correlation may be strengthened by not using the co-occurrence counts directly, but association strengths between words instead.
The approach we investigate for identifying term translations in comparable corpora is similar to (Rapp, 1999) and many others. $$$$$ However, only recently new approaches have been proposed to identify word translations from non-parallel or even unrelated texts.

Complex linguistic tools such as terminological extractors (Daille and Morin,2005), parsers (Yu and Tsujii, 2009) or lemma tizers (Rapp, 1999) are sometimes used. $$$$$ Future work has to approach the difficult problem of ambiguity resolution, which has not been dealt with here.
Complex linguistic tools such as terminological extractors (Daille and Morin,2005), parsers (Yu and Tsujii, 2009) or lemma tizers (Rapp, 1999) are sometimes used. $$$$$ One possibility to get the process going would be to learn vocabulary lists as in school, another to simply acquire the names of items in the physical world.
Complex linguistic tools such as terminological extractors (Daille and Morin,2005), parsers (Yu and Tsujii, 2009) or lemma tizers (Rapp, 1999) are sometimes used. $$$$$ Algorithms for the alignment of words in translated texts are well established.

Context length can be based on a number of units, for instance 3 sentences (Daille and Morin, 2005), windows of 3 (Rapp, 1999) or 25 words (Prochasson et al, 2009), etc. $$$$$ If — for example — in a text of one language two words A and B co-occur more often than expected by chance, then in a text of another language those words that are translations of A and B should also co-occur more frequently than expected.
Context length can be based on a number of units, for instance 3 sentences (Daille and Morin, 2005), windows of 3 (Rapp, 1999) or 25 words (Prochasson et al, 2009), etc. $$$$$ Algorithms for the alignment of words in translated texts are well established.
Context length can be based on a number of units, for instance 3 sentences (Daille and Morin, 2005), windows of 3 (Rapp, 1999) or 25 words (Prochasson et al, 2009), etc. $$$$$ For example, for the German word Strafie we had expected street, but the system predicted road, which is a translation quite as good.

As already noted, most authors use the log-likelihood ratio to measure the association between collocates; some, like (Rapp, 1999), informally compare the performance of a small number of association measures, or combine the results obtained with different association measures (Daille and Morin, 2005). $$$$$ Therefore, as a better measure for the accuracy of our system we counted the number of times where an acceptable translation of the source word is ranked first.
As already noted, most authors use the log-likelihood ratio to measure the association between collocates; some, like (Rapp, 1999), informally compare the performance of a small number of association measures, or combine the results obtained with different association measures (Daille and Morin, 2005). $$$$$ Starting with the well-known paper of Brown et al. (1990) on statistical machine translation, there has been much scientific interest in the alignment of sentences and words in translated texts.
As already noted, most authors use the log-likelihood ratio to measure the association between collocates; some, like (Rapp, 1999), informally compare the performance of a small number of association measures, or combine the results obtained with different association measures (Daille and Morin, 2005). $$$$$ In addition, for each word its expected English translation from the test set is given together with its position in the ranked lists of computed translations.

Expand the dictionary of step 3 using comparable corpora as proposed in a study by Rapp (1999). $$$$$ However, only recently new approaches have been proposed to identify word translations from non-parallel or even unrelated texts.
Expand the dictionary of step 3 using comparable corpora as proposed in a study by Rapp (1999). $$$$$ However, in the case of unrelated texts, its usefulness may be near zero.
Expand the dictionary of step 3 using comparable corpora as proposed in a study by Rapp (1999). $$$$$ We now select a word of the source language whose translation is to be determined.

From a previous pilot study (Rapp, 1999) it can be expected that this methodology achieves an accuracy in the order of 70%, which means that only a relatively modest amount of manual post editing is required. $$$$$ Others reduce the size of the co-occurrence matrices by performing a singular value decomposition.
From a previous pilot study (Rapp, 1999) it can be expected that this methodology achieves an accuracy in the order of 70%, which means that only a relatively modest amount of manual post editing is required. $$$$$ Let us now look at some cases where the program did particularly badly.
From a previous pilot study (Rapp, 1999) it can be expected that this methodology achieves an accuracy in the order of 70%, which means that only a relatively modest amount of manual post editing is required. $$$$$ Future work has to approach the difficult problem of ambiguity resolution, which has not been dealt with here.

(Rapp, 1999) and (Koehn and Knight, 2002) extract new word translations from non-parallel corpus. $$$$$ Most of the proposed algorithms first conduct an alignment of sentences, that is, they locate those pairs of sentences that are translations of each other.
(Rapp, 1999) and (Koehn and Knight, 2002) extract new word translations from non-parallel corpus. $$$$$ This task is more difficult, because most statistical clues useful in the processing of parallel texts cannot be applied to non-parallel texts.
(Rapp, 1999) and (Koehn and Knight, 2002) extract new word translations from non-parallel corpus. $$$$$ To conclude with, let us add some speculation by mentioning that the ability to identify word translations from non-parallel texts can be seen as an indicator in favor of the associationist view of human language acquisition (see also Landauer & Dumais, 1997, and Wettler & Rapp, 1993).

 $$$$$ It not only reduces the sparsedata problem but also takes into account that German is a highly inflectional language, whereas English is not.
 $$$$$ However, it must be emphasized that their result has been achieved under very different circumstances.
 $$$$$ One possibility would be to semantically disambiguate the words in the corpora beforehand, another to look at co-occurrences between significant word sequences instead of co-occurrences between single words.

To identify the context terms CT (WS) of a source word WS, as in (Rapp, 1999), we use log likelihood ratio (LL) Dunning (1993). $$$$$ With the resulting vector, we now perform a similarity computation to all vectors in the co-occurrence matrix of the target language.
To identify the context terms CT (WS) of a source word WS, as in (Rapp, 1999), we use log likelihood ratio (LL) Dunning (1993). $$$$$ The method described here — although developed independently of Fung's work — goes in the same direction.
To identify the context terms CT (WS) of a source word WS, as in (Rapp, 1999), we use log likelihood ratio (LL) Dunning (1993). $$$$$ The algorithms are usually based on one or several of the following statistical clues: All these clues usually work well for parallel texts.

Using large, unrelated English and German corpora (with 163m and 135mwords) and a small German-English bilingual dictionary (with 22k entires), Rapp (1999) demonstrated that reasonably accurate translations could be learned for 100 German nouns that were not contained in the seed bilingual dictionary. $$$$$ The reason is that weifi can also be third person singular of the German verb wissen (to know), which in newspaper texts is more frequent than the color white.
Using large, unrelated English and German corpora (with 163m and 135mwords) and a small German-English bilingual dictionary (with 22k entires), Rapp (1999) demonstrated that reasonably accurate translations could be learned for 100 German nouns that were not contained in the seed bilingual dictionary. $$$$$ This was true in 89 cases.5 For comparison, Fung & McKeown (1997) report an accuracy of about 30% when only the top candidate is counted.
Using large, unrelated English and German corpora (with 163m and 135mwords) and a small German-English bilingual dictionary (with 22k entires), Rapp (1999) demonstrated that reasonably accurate translations could be learned for 100 German nouns that were not contained in the seed bilingual dictionary. $$$$$ On the one hand, their task was more difficult because they worked on a pair of unrelated languages (English/Japanese) using smaller corpora and a random selection of test words, many of which were multi-word terms.
Using large, unrelated English and German corpora (with 163m and 135mwords) and a small German-English bilingual dictionary (with 22k entires), Rapp (1999) demonstrated that reasonably accurate translations could be learned for 100 German nouns that were not contained in the seed bilingual dictionary. $$$$$ One possibility would be to semantically disambiguate the words in the corpora beforehand, another to look at co-occurrences between significant word sequences instead of co-occurrences between single words.

We extend the vector space approach of Rapp (1999) to compute similarity between phrases in the source and target languages. $$$$$ This was done on the basis of a list of approximately 600 German and another list of about 200 English function words.
We extend the vector space approach of Rapp (1999) to compute similarity between phrases in the source and target languages. $$$$$ In preliminary experiments it also led to slightly better results than the conditional probability measure.
We extend the vector space approach of Rapp (1999) to compute similarity between phrases in the source and target languages. $$$$$ Nevertheless, the results achieved with these algorithms have been found useful for the cornpilation of dictionaries, for checking the consistency of terminological usage in translations, for assisting the terminological work of translators and interpreters, and for example-based machine translation.

 $$$$$ Thereafter, they were normalized in such a way that for each vector the sum of its entries adds up to one.

One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional co-locational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). $$$$$ On the other hand, when conducting their evaluation, Fung & McKeown limited the vocabulary they considered as translation candidates to a few hundred terms, which obviously facilitates the task.
One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional co-locational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). $$$$$ However, it must be emphasized that their result has been achieved under very different circumstances.
One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional co-locational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). $$$$$ As mentioned above, it is assumed that across languages there is a correlation between the cooccurrences of words that are translations of each other.

Some successful combinations are cos CP (Schuetze and Pedersen, 1997), Lin PMI (Lin, 1998), City LL (Rapp, 1999), and Jensen Shannon divergence of conditional probabilities (JSD CP). $$$$$ The method described can be seen as a simple case of the gradient descent method proposed by Rapp (1995), which does not need an initial lexicon but is computationally prohibitively expensive.
Some successful combinations are cos CP (Schuetze and Pedersen, 1997), Lin PMI (Lin, 1998), City LL (Rapp, 1999), and Jensen Shannon divergence of conditional probabilities (JSD CP). $$$$$ In other cases, typical associates follow the correct translation.
Some successful combinations are cos CP (Schuetze and Pedersen, 1997), Lin PMI (Lin, 1998), City LL (Rapp, 1999), and Jensen Shannon divergence of conditional probabilities (JSD CP). $$$$$ It can also be considered as an extension from the monolingual to the bilingual case of the well-established methods for semantic or syntactic word clustering as proposed by Schiitze (1993), Grefenstette (1994), Ruge (1995), Rapp (1996), Lin (1998), and others.
Some successful combinations are cos CP (Schuetze and Pedersen, 1997), Lin PMI (Lin, 1998), City LL (Rapp, 1999), and Jensen Shannon divergence of conditional probabilities (JSD CP). $$$$$ Others reduce the size of the co-occurrence matrices by performing a singular value decomposition.

The counts can be collected in positional (Rapp, 1999) or non-positional way (count all the word occurrences within the sliding window). $$$$$ However, only recently new approaches have been proposed to identify word translations from non-parallel or even unrelated texts.
The counts can be collected in positional (Rapp, 1999) or non-positional way (count all the word occurrences within the sliding window). $$$$$ All unknown words are discarded from the vector and the vector positions are sorted in order to match the vectors of the target-language matrix.
The counts can be collected in positional (Rapp, 1999) or non-positional way (count all the word occurrences within the sliding window). $$$$$ In a second step a word alignment is performed by analyzing the correspondences of words in each pair of sentences.
