On the other hand, the alternative approach using comparable or unrelated text corpora were studied by Rapp (1999) and Fung et al (1998). $$$$$ It gives us an idea of how it is possible to derive the meaning of unknown words from texts by only presupposing a limited number of known words and then iteratively expanding this knowledge base.
On the other hand, the alternative approach using comparable or unrelated text corpora were studied by Rapp (1999) and Fung et al (1998). $$$$$ One possibility would be to semantically disambiguate the words in the corpora beforehand, another to look at co-occurrences between significant word sequences instead of co-occurrences between single words.
On the other hand, the alternative approach using comparable or unrelated text corpora were studied by Rapp (1999) and Fung et al (1998). $$$$$ This was true in 89 cases.5 For comparison, Fung & McKeown (1997) report an accuracy of about 30% when only the top candidate is counted.

For instance, good results are obtained from large corpora several million words for which the accuracy of the proposed translation is between 76% (Fung, 1998) and 89% (Rapp, 1999) for the first 20 candidates. $$$$$ For all other pairs, it is usually used in combination with the first clue.
For instance, good results are obtained from large corpora several million words for which the accuracy of the proposed translation is between 76% (Fung, 1998) and 89% (Rapp, 1999) for the first 20 candidates. $$$$$ Using our source-language corpus, we compute a co-occurrence vector for this word.
For instance, good results are obtained from large corpora several million words for which the accuracy of the proposed translation is between 76% (Fung, 1998) and 89% (Rapp, 1999) for the first 20 candidates. $$$$$ It gives us an idea of how it is possible to derive the meaning of unknown words from texts by only presupposing a limited number of known words and then iteratively expanding this knowledge base.

This can be accomplished as in Rapp (1999) and Schafer and Yarowsky (2002) by creating bag-of-words context vectors around both the source and target language words and then projecting the source vectors into the (English) target space via the current small translation dictionary. $$$$$ Nevertheless, both methods are of technical value since they lead to a reduction in the size of the co-occurrence matrices.
This can be accomplished as in Rapp (1999) and Schafer and Yarowsky (2002) by creating bag-of-words context vectors around both the source and target language words and then projecting the source vectors into the (English) target space via the current small translation dictionary. $$$$$ This task is more difficult, because most statistical clues useful in the processing of parallel texts cannot be applied to non-parallel texts.
This can be accomplished as in Rapp (1999) and Schafer and Yarowsky (2002) by creating bag-of-words context vectors around both the source and target language words and then projecting the source vectors into the (English) target space via the current small translation dictionary. $$$$$ It is expected that the correct translation is ranked first in the sorted list.

Here, a standard technique of estimating bilingual term correspondences from com parable corpora (e.g., Fung and Yee (1998) and Rapp (1999)) is employed. $$$$$ It gives us an idea of how it is possible to derive the meaning of unknown words from texts by only presupposing a limited number of known words and then iteratively expanding this knowledge base.
Here, a standard technique of estimating bilingual term correspondences from com parable corpora (e.g., Fung and Yee (1998) and Rapp (1999)) is employed. $$$$$ Since it has been empirically observed that word order of content words is often similar between languages (even between unrelated languages such as English and Chinese), and since this may be a useful statistical clue, we decided to modify the common approach in the way proposed by Rapp (1996, p. 162).
Here, a standard technique of estimating bilingual term correspondences from com parable corpora (e.g., Fung and Yee (1998) and Rapp (1999)) is employed. $$$$$ The current study, which is based on the assumption that there is a correlation between the patterns of word co-occurrences in corpora of different languages, makes a significant improvement to about 72% of word translations identified correctly.

For example, Rapp (1999) filtered out bilingual term pairs with low monolingual frequencies (those below 100 times), while Fung and Yee (1998) restricted candidate bilingual term pairs to be pairs of the most frequent 118 unknown words. $$$$$ After an attempt with a context heterogeneity measure (Fung, 1995) for identifying word translations, Fung based her later work also on the co-occurrence assumption (Fung & Yee, 1998; Fung & McKeown, 1997).
For example, Rapp (1999) filtered out bilingual term pairs with low monolingual frequencies (those below 100 times), while Fung and Yee (1998) restricted candidate bilingual term pairs to be pairs of the most frequent 118 unknown words. $$$$$ In another test, we checked whether an acceptable translation appeared among the top 10 of the ranked lists.
For example, Rapp (1999) filtered out bilingual term pairs with low monolingual frequencies (those below 100 times), while Fung and Yee (1998) restricted candidate bilingual term pairs to be pairs of the most frequent 118 unknown words. $$$$$ Some of these authors perform a shallow or full syntactical analysis before constructing the cooccurrence vectors.

The approach we investigate for identifying term translations in comparable corpora is similar to (Rapp, 1999) and many others. $$$$$ For vector comparison, different similarity measures can be considered.
The approach we investigate for identifying term translations in comparable corpora is similar to (Rapp, 1999) and many others. $$$$$ Some of these authors perform a shallow or full syntactical analysis before constructing the cooccurrence vectors.
The approach we investigate for identifying term translations in comparable corpora is similar to (Rapp, 1999) and many others. $$$$$ Future work has to approach the difficult problem of ambiguity resolution, which has not been dealt with here.

Complex linguistic tools such as terminological extractors (Daille and Morin,2005), parsers (Yu and Tsujii, 2009) or lemma tizers (Rapp, 1999) are sometimes used. $$$$$ Since semantic patterns are more reliable than syntactic patterns across language families, we hoped that eliminating the function words would give our method more generality.
Complex linguistic tools such as terminological extractors (Daille and Morin,2005), parsers (Yu and Tsujii, 2009) or lemma tizers (Rapp, 1999) are sometimes used. $$$$$ Starting with the well-known paper of Brown et al. (1990) on statistical machine translation, there has been much scientific interest in the alignment of sentences and words in translated texts.
Complex linguistic tools such as terminological extractors (Daille and Morin,2005), parsers (Yu and Tsujii, 2009) or lemma tizers (Rapp, 1999) are sometimes used. $$$$$ Some of these authors perform a shallow or full syntactical analysis before constructing the cooccurrence vectors.
Complex linguistic tools such as terminological extractors (Daille and Morin,2005), parsers (Yu and Tsujii, 2009) or lemma tizers (Rapp, 1999) are sometimes used. $$$$$ Using our source-language corpus, we compute a co-occurrence vector for this word.

Context length can be based on a number of units, for instance 3 sentences (Daille and Morin, 2005), windows of 3 (Rapp, 1999) or 25 words (Prochasson et al, 2009), etc. $$$$$ In other cases, typical associates follow the correct translation.
Context length can be based on a number of units, for instance 3 sentences (Daille and Morin, 2005), windows of 3 (Rapp, 1999) or 25 words (Prochasson et al, 2009), etc. $$$$$ Table 1 shows the results for 20 of the 100 German test words.
Context length can be based on a number of units, for instance 3 sentences (Daille and Morin, 2005), windows of 3 (Rapp, 1999) or 25 words (Prochasson et al, 2009), etc. $$$$$ Because we had decided on our test word list beforehand, and since it would not make much sense to apply our method to words that are already in the base lexicon, we also removed all entries belonging to the 100 test words.

As already noted, most authors use the log-likelihood ratio to measure the association between collocates; some, like (Rapp, 1999), informally compare the performance of a small number of association measures, or combine the results obtained with different association measures (Daille and Morin, 2005). $$$$$ This task is more difficult, because most statistical clues useful in the processing of parallel texts cannot be applied to non-parallel texts.
As already noted, most authors use the log-likelihood ratio to measure the association between collocates; some, like (Rapp, 1999), informally compare the performance of a small number of association measures, or combine the results obtained with different association measures (Daille and Morin, 2005). $$$$$ The algorithms are usually based on one or several of the following statistical clues: All these clues usually work well for parallel texts.
As already noted, most authors use the log-likelihood ratio to measure the association between collocates; some, like (Rapp, 1999), informally compare the performance of a small number of association measures, or combine the results obtained with different association measures (Daille and Morin, 2005). $$$$$ This was true for 72 of the 100 test words, which gives us an accuracy of 72%.

Expand the dictionary of step 3 using comparable corpora as proposed in a study by Rapp (1999). $$$$$ Some of these authors perform a shallow or full syntactical analysis before constructing the cooccurrence vectors.
Expand the dictionary of step 3 using comparable corpora as proposed in a study by Rapp (1999). $$$$$ The second clue is generally less powerful than the first, since most words are ambiguous in natural languages, and many ambiguities are different across languages.
Expand the dictionary of step 3 using comparable corpora as proposed in a study by Rapp (1999). $$$$$ Using a corpus of the target language, we first compute a co-occurrence matrix whose rows are all word types occurring in the corpus and whose columns are all target words appearing in the base lexicon.
Expand the dictionary of step 3 using comparable corpora as proposed in a study by Rapp (1999). $$$$$ Using a corpus of the target language, we first compute a co-occurrence matrix whose rows are all word types occurring in the corpus and whose columns are all target words appearing in the base lexicon.

From a previous pilot study (Rapp, 1999) it can be expected that this methodology achieves an accuracy in the order of 70%, which means that only a relatively modest amount of manual post editing is required. $$$$$ This task is more difficult, because most statistical clues useful in the processing of parallel texts cannot be applied to non-parallel texts.
From a previous pilot study (Rapp, 1999) it can be expected that this methodology achieves an accuracy in the order of 70%, which means that only a relatively modest amount of manual post editing is required. $$$$$ Table 1 shows the results for 20 of the 100 German test words.
From a previous pilot study (Rapp, 1999) it can be expected that this methodology achieves an accuracy in the order of 70%, which means that only a relatively modest amount of manual post editing is required. $$$$$ It is further assumed that there is a small dictionary available at the beginning, and that our aim is to expand this base lexicon.
From a previous pilot study (Rapp, 1999) it can be expected that this methodology achieves an accuracy in the order of 70%, which means that only a relatively modest amount of manual post editing is required. $$$$$ On the one hand, their task was more difficult because they worked on a pair of unrelated languages (English/Japanese) using smaller corpora and a random selection of test words, many of which were multi-word terms.

(Rapp, 1999) and (Koehn and Knight, 2002) extract new word translations from non-parallel corpus. $$$$$ Since preliminary experiments showed that a window size of 3 with consideration of word order seemed to give somewhat better results than other window types, the results reported here are based on vectors of this kind.
(Rapp, 1999) and (Koehn and Knight, 2002) extract new word translations from non-parallel corpus. $$$$$ Whereas for parallel texts in some studies up to 99% of the word alignments have been shown to be correct, the accuracy for non-parallel texts has been around 30% up to now.
(Rapp, 1999) and (Koehn and Knight, 2002) extract new word translations from non-parallel corpus. $$$$$ Of course, the translators and interpreters can understand the texts, whereas our programs are only considering a few statistical clues.
(Rapp, 1999) and (Koehn and Knight, 2002) extract new word translations from non-parallel corpus. $$$$$ It is further assumed that there is a small dictionary available at the beginning, and that our aim is to expand this base lexicon.

 $$$$$ 4 This means that alternative translations of a word were not considered.
 $$$$$ However, in yet unpublished work we found that at least for the computation of synonyms and related words neither syntactical analysis nor singular value decomposition lead to significantly better results than the approach described here when applied to the monolingual case (see also Grefenstette, 1993), so we did not try to include these methods in our system.
 $$$$$ In preliminary experiments it also led to slightly better results than the conditional probability measure.
 $$$$$ Most of the proposed algorithms first conduct an alignment of sentences, that is, they locate those pairs of sentences that are translations of each other.

To identify the context terms CT (WS) of a source word WS, as in (Rapp, 1999), we use log likelihood ratio (LL) Dunning (1993). $$$$$ It can also be considered as an extension from the monolingual to the bilingual case of the well-established methods for semantic or syntactic word clustering as proposed by Schiitze (1993), Grefenstette (1994), Ruge (1995), Rapp (1996), Lin (1998), and others.
To identify the context terms CT (WS) of a source word WS, as in (Rapp, 1999), we use log likelihood ratio (LL) Dunning (1993). $$$$$ As mentioned above, it is assumed that across languages there is a correlation between the cooccurrences of words that are translations of each other.

Using large, unrelated English and German corpora (with 163m and 135mwords) and a small German-English bilingual dictionary (with 22k entires), Rapp (1999) demonstrated that reasonably accurate translations could be learned for 100 German nouns that were not contained in the seed bilingual dictionary. $$$$$ In these vectors, all entries belonging to words not found in the English part of the base lexicon were deleted.
Using large, unrelated English and German corpora (with 163m and 135mwords) and a small German-English bilingual dictionary (with 22k entires), Rapp (1999) demonstrated that reasonably accurate translations could be learned for 100 German nouns that were not contained in the seed bilingual dictionary. $$$$$ Others reduce the size of the co-occurrence matrices by performing a singular value decomposition.
Using large, unrelated English and German corpora (with 163m and 135mwords) and a small German-English bilingual dictionary (with 22k entires), Rapp (1999) demonstrated that reasonably accurate translations could be learned for 100 German nouns that were not contained in the seed bilingual dictionary. $$$$$ This task is more difficult, because most statistical clues useful in the processing of parallel texts cannot be applied to non-parallel texts.
Using large, unrelated English and German corpora (with 163m and 135mwords) and a small German-English bilingual dictionary (with 22k entires), Rapp (1999) demonstrated that reasonably accurate translations could be learned for 100 German nouns that were not contained in the seed bilingual dictionary. $$$$$ Nevertheless, both methods are of technical value since they lead to a reduction in the size of the co-occurrence matrices.

We extend the vector space approach of Rapp (1999) to compute similarity between phrases in the source and target languages. $$$$$ One possibility would be to semantically disambiguate the words in the corpora beforehand, another to look at co-occurrences between significant word sequences instead of co-occurrences between single words.
We extend the vector space approach of Rapp (1999) to compute similarity between phrases in the source and target languages. $$$$$ We translate all known words in this vector to the target language.
We extend the vector space approach of Rapp (1999) to compute similarity between phrases in the source and target languages. $$$$$ It can also be considered as an extension from the monolingual to the bilingual case of the well-established methods for semantic or syntactic word clustering as proposed by Schiitze (1993), Grefenstette (1994), Ruge (1995), Rapp (1996), Lin (1998), and others.
We extend the vector space approach of Rapp (1999) to compute similarity between phrases in the source and target languages. $$$$$ For vector comparison, different similarity measures can be considered.

 $$$$$ It can also be considered as an extension from the monolingual to the bilingual case of the well-established methods for semantic or syntactic word clustering as proposed by Schiitze (1993), Grefenstette (1994), Ruge (1995), Rapp (1996), Lin (1998), and others.
 $$$$$ Future work has to approach the difficult problem of ambiguity resolution, which has not been dealt with here.
 $$$$$ However, this approach does not take word order within a window into account.
 $$$$$ For counting word co-occurrences, in most other studies a fixed window size is chosen and it is determined how often each pair of words occurs within a text window of this size.

One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional co-locational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). $$$$$ Since our base lexicon is small, only some of the translations are known.
One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional co-locational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). $$$$$ For Kohl we had expected its dictionary translation cabbage, but — given that a substantial part of our newspaper corpora consists of political texts — we do not need to further explain why our program lists Major, Kohl, Thatcher, Gorbachev, and Bush, state leaders who were in office during the time period the texts were written.
One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional co-locational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). $$$$$ Results based on mutual information or co-occurrence counts were significantly worse.

Some successful combinations are cos CP (Schuetze and Pedersen, 1997), Lin PMI (Lin, 1998), City LL (Rapp, 1999), and Jensen Shannon divergence of conditional probabilities (JSD CP). $$$$$ When comparing an English and a German co-occurrence matrix of corresponding words, he found a high correlation between the co-occurrence patterns of the two matrices when the rows and columns of both matrices were in corresponding word order, and a low correlation when the rows and columns were in random order.
Some successful combinations are cos CP (Schuetze and Pedersen, 1997), Lin PMI (Lin, 1998), City LL (Rapp, 1999), and Jensen Shannon divergence of conditional probabilities (JSD CP). $$$$$ Using a corpus of the target language, we first compute a co-occurrence matrix whose rows are all word types occurring in the corpus and whose columns are all target words appearing in the base lexicon.
Some successful combinations are cos CP (Schuetze and Pedersen, 1997), Lin PMI (Lin, 1998), City LL (Rapp, 1999), and Jensen Shannon divergence of conditional probabilities (JSD CP). $$$$$ After an attempt with a context heterogeneity measure (Fung, 1995) for identifying word translations, Fung based her later work also on the co-occurrence assumption (Fung & Yee, 1998; Fung & McKeown, 1997).
Some successful combinations are cos CP (Schuetze and Pedersen, 1997), Lin PMI (Lin, 1998), City LL (Rapp, 1999), and Jensen Shannon divergence of conditional probabilities (JSD CP). $$$$$ However, only recently new approaches have been proposed to identify word translations from non-parallel or even unrelated texts.

The counts can be collected in positional (Rapp, 1999) or non-positional way (count all the word occurrences within the sliding window). $$$$$ The main contribution of this paper is to describe a practical implementation based on the co-occurrence clue that yields good results.
The counts can be collected in positional (Rapp, 1999) or non-positional way (count all the word occurrences within the sliding window). $$$$$ This task is more difficult, because most statistical clues useful in the processing of parallel texts cannot be applied to non-parallel texts.
The counts can be collected in positional (Rapp, 1999) or non-positional way (count all the word occurrences within the sliding window). $$$$$ We now select a word of the source language whose translation is to be determined.
