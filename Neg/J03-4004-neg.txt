They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). $$$$$ The probability distributions are conditioned on a verb or adjective class and a grammatical relationship.
They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). $$$$$ We quantified coverage and accuracy of sense disambiguation of verbs, adjectives, and nouns in the SENSEVAL-2 English all-words test corpus, using automatically acquired selectional preferences.
They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). $$$$$ This gives a probability of 0.009 x 0.226 = 0.002 for the noun class probability given the verb class (with maximum probability) and grammatical context.

Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ These results were obtained for three systems (Resnik 1997; Abney and Light 1999; Ciaramita and Johnson 2000) on a training and test data set constructed by Resnik containing nouns occurring as direct objects of 100 verbs that select strongly for their objects.
Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ The first application is text simplification, as outlined by Carroll, Minnen, Pearce et al. (1999).
Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ One problem is coverage: Most previous work has focused on acquiring selectional preferences for verbs and applying them to disambiguate nouns occurring at subject and direct object slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson 2000; Stevenson and Wilks 2001).
Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ For the distribution over AC, the frequency credit for each adjective is divided by the number of synsets to which the adjective belongs, and the credit for an ac is the sum over all the synsets that are members by virtue of the similar-to WordNet link.

Selectional preferences do not only play an important role in human sentence processing (McRae et al, 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ For the WSD task, we compare the probability estimates at each nc E Cn, so if a noun belongs to several synsets, we compare the probability estimates, given the context, of these synsets.
Selectional preferences do not only play an important role in human sentence processing (McRae et al, 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ Selectional preferences work well for some word combinations and grammatical relationships, but not well for others.
Selectional preferences do not only play an important role in human sentence processing (McRae et al, 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ This gives a probability of 0.009 x 0.226 = 0.002 for the noun class probability given the verb class (with maximum probability) and grammatical context.
Selectional preferences do not only play an important role in human sentence processing (McRae et al, 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ This work was supported by UK EPSRC project GR/N36493 “Robust Accurate Statistical Parsing (RASP)” and EU FW5 project IST-2001-34460 “MEANING.” We are grateful to Rob Koeling and three anonymous reviewers for their helpful comments on earlier drafts.

Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ We would also like to thank David Weir and Mark Mclauchlan for useful discussions.
Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ We would also like to thank David Weir and Mark Mclauchlan for useful discussions.
Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ The frequency credit for a tuple is divided by |Cn |for any n, and by the number of synsets of v, Cv (or Ca if the gr is adjective-noun): A hypernym nc' includes the frequency credit attributed to all its hyponyms ({nc ∈ nc'}).
Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ We believe that selectional preferences would perform best if they were acquired from similar training data to that for which disambiguation is required.

In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al, 2007). $$$$$ This article is aimed at quantifying the disambiguation performance of automatically acquired selectional preferences in regard to nouns, verbs, and adjectives with respect to a standard test corpus and evaluation setup (SENSEVAL-2) and to identify strengths and weaknesses.
In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al, 2007). $$$$$ For many words, however, the predominant sense varies across domains, and so we contend that it is worth concentrating on detecting when the first sense is not relevant, and where the selectional-preference models provide a high probability for a secondary sense.
In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al, 2007). $$$$$ The first-sense heuristic, based on sense-tagged data such as that available in SemCor, seems to beat unsupervised models such as ours.
In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al, 2007). $$$$$ We also investigate use of the one-senseper-discourse heuristic to propagate a sense tag for a word to other occurrences of the same word within the current document in order to increase coverage.

Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levels in future research (c.f. Li and Abe (1998), McCarthy and Carroll (2003), Xiong et al (2005), Fujita et al (2007)). $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.
Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levels in future research (c.f. Li and Abe (1998), McCarthy and Carroll (2003), Xiong et al (2005), Fujita et al (2007)). $$$$$ This work was supported by UK EPSRC project GR/N36493 “Robust Accurate Statistical Parsing (RASP)” and EU FW5 project IST-2001-34460 “MEANING.” We are grateful to Rob Koeling and three anonymous reviewers for their helpful comments on earlier drafts.
Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levels in future research (c.f. Li and Abe (1998), McCarthy and Carroll (2003), Xiong et al (2005), Fujita et al (2007)). $$$$$ In normal running text, however, a large proportion of word tokens do not fall at these slots.
Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levels in future research (c.f. Li and Abe (1998), McCarthy and Carroll (2003), Xiong et al (2005), Fujita et al (2007)). $$$$$ We improved coverage and recall by applying the one-sense-per-discourse heuristic.

Agirre and Martinez (2001) and Zheng et al (2007) adopted the Class-only Model in research, while in McCarthy and Carroll (2003) and Merlo and Stevenson (2001) the Word-Class Model is employed. $$$$$ Nouns, verbs, and adjectives all outperform their random baseline for precision, and the difference is more marked when monosemous instances are dropped.
Agirre and Martinez (2001) and Zheng et al (2007) adopted the Class-only Model in research, while in McCarthy and Carroll (2003) and Merlo and Stevenson (2001) the Word-Class Model is employed. $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.
Agirre and Martinez (2001) and Zheng et al (2007) adopted the Class-only Model in research, while in McCarthy and Carroll (2003) and Merlo and Stevenson (2001) the Word-Class Model is employed. $$$$$ We intend to investigate this issue further with the SENSEVAL-2 lexical sample data, which contains more instances of a smaller number of words.

McCarthy and Carroll (2003) reports that the Word-Class Model performs well in unsupervised WSD. $$$$$ For three of the most polysemous verbs that overlapped between the English lexical sample for SENSEVAL and SENSEVAL-2, the performance was comparable.
McCarthy and Carroll (2003) reports that the Word-Class Model performs well in unsupervised WSD. $$$$$ This work was supported by UK EPSRC project GR/N36493 “Robust Accurate Statistical Parsing (RASP)” and EU FW5 project IST-2001-34460 “MEANING.” We are grateful to Rob Koeling and three anonymous reviewers for their helpful comments on earlier drafts.
McCarthy and Carroll (2003) reports that the Word-Class Model performs well in unsupervised WSD. $$$$$ We intend to investigate this issue further with the SENSEVAL-2 lexical sample data, which contains more instances of a smaller number of words.

One known unsupervised learning approach for WSD in SP is McCarthy and Carroll (2003) which addresses the issue via conditional probability. $$$$$ This work was supported by UK EPSRC project GR/N36493 “Robust Accurate Statistical Parsing (RASP)” and EU FW5 project IST-2001-34460 “MEANING.” We are grateful to Rob Koeling and three anonymous reviewers for their helpful comments on earlier drafts.
One known unsupervised learning approach for WSD in SP is McCarthy and Carroll (2003) which addresses the issue via conditional probability. $$$$$ One subtask in this application involves substituting words with thier more frequent synonyms, for example, substituting letter for missive.

This experiment is used as baseline as the approach is also used in McCarthy and Carroll (2003) for verb and adjective disambiguation. $$$$$ The selectional preferences are specific to verb or adjective classes, rather than individual word forms, so they can be used to disambiguate the co-occurring adjectives and verbs, rather than just the nominal argument heads.
This experiment is used as baseline as the approach is also used in McCarthy and Carroll (2003) for verb and adjective disambiguation. $$$$$ We therefore do not include in the following the coarse-grained results; they are just slightly better than the fine-grained results, which seems to be typical of other systems.
This experiment is used as baseline as the approach is also used in McCarthy and Carroll (2003) for verb and adjective disambiguation. $$$$$ The probability distributions are conditioned on a verb or adjective class and a grammatical relationship.
This experiment is used as baseline as the approach is also used in McCarthy and Carroll (2003) for verb and adjective disambiguation. $$$$$ For many words, however, the predominant sense varies across domains, and so we contend that it is worth concentrating on detecting when the first sense is not relevant, and where the selectional-preference models provide a high probability for a secondary sense.

McCarthy and Carroll (2003) also uses this type of redundancy for disambiguation in SP. $$$$$ The probability distributions are conditioned on a verb or adjective class and a grammatical relationship.
McCarthy and Carroll (2003) also uses this type of redundancy for disambiguation in SP. $$$$$ We would also like to thank David Weir and Mark Mclauchlan for useful discussions.
McCarthy and Carroll (2003) also uses this type of redundancy for disambiguation in SP. $$$$$ We would also like to thank David Weir and Mark Mclauchlan for useful discussions.

The approach is also used in (McCarthy and Carroll 2003) to disambiguate verbs and adjectives in collocations. $$$$$ Verbs and adjectives are disambiguated by using the probability distributions and Bayes’ rule to obtain an estimate of the probability of the adjective or verb class, given the noun and the grammatical relationship.
The approach is also used in (McCarthy and Carroll 2003) to disambiguate verbs and adjectives in collocations. $$$$$ No particular patterns were evident in this respect, perhaps because of the small size of the test data.
The approach is also used in (McCarthy and Carroll 2003) to disambiguate verbs and adjectives in collocations. $$$$$ We improved coverage and recall by applying the one-sense-per-discourse heuristic.
The approach is also used in (McCarthy and Carroll 2003) to disambiguate verbs and adjectives in collocations. $$$$$ The selectional preferences are specific to verb or adjective classes, rather than individual word forms, so they can be used to disambiguate the co-occurring adjectives and verbs, rather than just the nominal argument heads.

To be particular, the method used by McCarthy and Carroll (2003) is formula (6). $$$$$ One subtask in this application involves substituting words with thier more frequent synonyms, for example, substituting letter for missive.
To be particular, the method used by McCarthy and Carroll (2003) is formula (6). $$$$$ We did not use the SENSEVAL-2 coarse-grained classification, as this was not available at the time when we were acquiring the selectional preferences.
To be particular, the method used by McCarthy and Carroll (2003) is formula (6). $$$$$ For adjectives there seems to be a lot less ambiguity (this has also been noted by Krovetz [1998]; the data in SENSEVAL-2 bear this out, with many adjectives occurring only in their first sense.
To be particular, the method used by McCarthy and Carroll (2003) is formula (6). $$$$$ For three of the most polysemous verbs that overlapped between the English lexical sample for SENSEVAL and SENSEVAL-2, the performance was comparable.

For example $$$$$ Although the preferences perform well in comparison with other unsupervised WSD systems on the same corpus, the results show that for many applications, further knowledge sources would be required to achieve an adequate level of accuracy and coverage.
For example $$$$$ The selectional preferences are specific to verb or adjective classes, rather than individual word forms, so they can be used to disambiguate the co-occurring adjectives and verbs, rather than just the nominal argument heads.
For example $$$$$ The overall structure of the system is illustrated in Figure 1.
For example $$$$$ Then the estimate p(n) is obtained using the estimates for the hypernym classes on P for all the Cn that n belongs to: The probability at any particular nc' is divided by nsnc' to give the estimate for each p(ns) under that nc'.

Examples include information retrieval (Manning et al, 2008), word sense discrimination (Schtze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. $$$$$ Although a modest amount of English sense-tagged data is available, we nevertheless believe it is important to investigate methods that do not require such data, because there will be languages or texts for which sense-tagged data for a given word is not available or relevant.
Examples include information retrieval (Manning et al, 2008), word sense discrimination (Schtze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. $$$$$ A TCM is a set of noun classes that partition NS disjointly.
Examples include information retrieval (Manning et al, 2008), word sense discrimination (Schtze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.
Examples include information retrieval (Manning et al, 2008), word sense discrimination (Schtze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. $$$$$ These results are not comparable with ours, however, for three reasons.

McCarthy and Carroll (2003) have shown the effectiveness of the selectional preference information for WSD. $$$$$ The frequency threshold of 20 is intended to remove noisy data.
McCarthy and Carroll (2003) have shown the effectiveness of the selectional preference information for WSD. $$$$$ Selectional preferences can be used for WSD in combination with other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascertain when they work well so that they can be utilized to their full advantage.
McCarthy and Carroll (2003) have shown the effectiveness of the selectional preference information for WSD. $$$$$ This work was supported by UK EPSRC project GR/N36493 “Robust Accurate Statistical Parsing (RASP)” and EU FW5 project IST-2001-34460 “MEANING.” We are grateful to Rob Koeling and three anonymous reviewers for their helpful comments on earlier drafts.
McCarthy and Carroll (2003) have shown the effectiveness of the selectional preference information for WSD. $$$$$ We evaluate WSD using selectional preferences acquired for English adjective–noun, subject, and direct object grammatical relationships with respect to a standard test corpus.

McCarthy and Carroll (2003) also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes. $$$$$ Thus, the TCMs define a probability distribution over NS that is conditioned on a verb class (vc) or adjective class (ac) and a particular grammatical relation (gr): Acquisition of a TCM for a given vc and gr proceeds as follows.
McCarthy and Carroll (2003) also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes. $$$$$ We are also investigating the disambiguation of verb senses in running text before subcategorization information for the verbs is acquired, in order to produce a subcategorization lexicon specific to sense (Preiss and Korhonen 2002).
McCarthy and Carroll (2003) also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes. $$$$$ In addition to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance.
McCarthy and Carroll (2003) also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes. $$$$$ We evaluate WSD using selectional preferences acquired for English adjective–noun, subject, and direct object grammatical relationships with respect to a standard test corpus.

The most similar approach to the one we describe, that has been tested on Senseval-2, is the one described in (McCarthy and Carroll, 2003). $$$$$ We evaluate WSD using selectional preferences acquired for English adjective–noun, subject, and direct object grammatical relationships with respect to a standard test corpus.
The most similar approach to the one we describe, that has been tested on Senseval-2, is the one described in (McCarthy and Carroll, 2003). $$$$$ The results for the English SENSEVAL-2 tasks were generally much lower than those for the original SENSEVAL competition.
The most similar approach to the one we describe, that has been tested on Senseval-2, is the one described in (McCarthy and Carroll, 2003). $$$$$ Also, verbs are best disambiguated by their direct objects, whereas nouns appear to be better disambiguated as subjects and when modified by adjectives.
The most similar approach to the one we describe, that has been tested on Senseval-2, is the one described in (McCarthy and Carroll, 2003). $$$$$ In addition to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance.
