They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). $$$$$ The results show that disambiguation models using only selectional preferences can perform with accuracy well above the random baseline, although accuracy would not be high enough for applications in the absence of other knowledge sources (Stevenson and Wilks 2001).
They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). $$$$$ We also investigate use of the one-senseper-discourse heuristic to propagate a sense tag for a word to other occurrences of the same word within the current document in order to increase coverage.
They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). $$$$$ No particular patterns were evident in this respect, perhaps because of the small size of the test data.
They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.

Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ Although the preferences perform well in comparison with other unsupervised WSD systems on the same corpus, the results show that for many applications, further knowledge sources would be required to achieve an adequate level of accuracy and coverage.
Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ These are combined with prior estimates for p(nc|gr) and p(vc|gr) (or p(ac|gr)) using Bayes’ rule to give: and for adjective–noun relations: The prior distributions for p(nc|gr), p(vc|gr) and p(ac|adjnoun) are obtained during the training phase.
Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ A noun is disambiguated by using the preferences to give probability estimates for each of its senses in WordNet, that is, for WordNet synsets.
Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ We evaluate WSD using selectional preferences acquired for English adjective–noun, subject, and direct object grammatical relationships with respect to a standard test corpus.

Selectional preferences do not only play an important role in human sentence processing (McRae et al, 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ For example, if subcategorization were acquired specific to sense, rather than verb form, then distinct senses of fire could have different subcategorization entries: Selectional preferences could also then be acquired automatically from sense-tagged data in an iterative approach (McCarthy 2001).
Selectional preferences do not only play an important role in human sentence processing (McRae et al, 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ In addition to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance.
Selectional preferences do not only play an important role in human sentence processing (McRae et al, 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ This work was supported by UK EPSRC project GR/N36493 “Robust Accurate Statistical Parsing (RASP)” and EU FW5 project IST-2001-34460 “MEANING.” We are grateful to Rob Koeling and three anonymous reviewers for their helpful comments on earlier drafts.
Selectional preferences do not only play an important role in human sentence processing (McRae et al, 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ Although a modest amount of English sense-tagged data is available, we nevertheless believe it is important to investigate methods that do not require such data, because there will be languages or texts for which sense-tagged data for a given word is not available or relevant.

Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ Table 2 shows our precision results including use of the OSPD heuristic, broken down by part of speech.
Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ For example, in the following sentence we would like to use strategy, rather than dodge, as a substitute for scheme: A recent government study singled out the scheme as an example to others.
Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.
Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ Verbs and adjectives are disambiguated using TCMs to give estimates for p(nc|vc,gr) and p(nc|ac,gr), respectively.

In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al, 2007). $$$$$ We also investigate use of the one-senseper-discourse heuristic to propagate a sense tag for a word to other occurrences of the same word within the current document in order to increase coverage.
In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al, 2007). $$$$$ Although the preferences perform well in comparison with other unsupervised WSD systems on the same corpus, the results show that for many applications, further knowledge sources would be required to achieve an adequate level of accuracy and coverage.
In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al, 2007). $$$$$ This work was supported by UK EPSRC project GR/N36493 “Robust Accurate Statistical Parsing (RASP)” and EU FW5 project IST-2001-34460 “MEANING.” We are grateful to Rob Koeling and three anonymous reviewers for their helpful comments on earlier drafts.
In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al, 2007). $$$$$ This work was supported by UK EPSRC project GR/N36493 “Robust Accurate Statistical Parsing (RASP)” and EU FW5 project IST-2001-34460 “MEANING.” We are grateful to Rob Koeling and three anonymous reviewers for their helpful comments on earlier drafts.

Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levels in future research (c.f. Li and Abe (1998), McCarthy and Carroll (2003), Xiong et al (2005), Fujita et al (2007)). $$$$$ Selectional preferences can be used for WSD in combination with other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascertain when they work well so that they can be utilized to their full advantage.
Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levels in future research (c.f. Li and Abe (1998), McCarthy and Carroll (2003), Xiong et al (2005), Fujita et al (2007)). $$$$$ In these experiments we applied the OSPD heuristic to increase coverage.
Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levels in future research (c.f. Li and Abe (1998), McCarthy and Carroll (2003), Xiong et al (2005), Fujita et al (2007)). $$$$$ We would also like to thank David Weir and Mark Mclauchlan for useful discussions.

Agirre and Martinez (2001) and Zheng et al (2007) adopted the Class-only Model in research, while in McCarthy and Carroll (2003) and Merlo and Stevenson (2001) the Word-Class Model is employed. $$$$$ One subtask in this application involves substituting words with thier more frequent synonyms, for example, substituting letter for missive.
Agirre and Martinez (2001) and Zheng et al (2007) adopted the Class-only Model in research, while in McCarthy and Carroll (2003) and Merlo and Stevenson (2001) the Word-Class Model is employed. $$$$$ For the distribution over AC, the frequency credit for each adjective is divided by the number of synsets to which the adjective belongs, and the credit for an ac is the sum over all the synsets that are members by virtue of the similar-to WordNet link.
Agirre and Martinez (2001) and Zheng et al (2007) adopted the Class-only Model in research, while in McCarthy and Carroll (2003) and Merlo and Stevenson (2001) the Word-Class Model is employed. $$$$$ This work was supported by UK EPSRC project GR/N36493 “Robust Accurate Statistical Parsing (RASP)” and EU FW5 project IST-2001-34460 “MEANING.” We are grateful to Rob Koeling and three anonymous reviewers for their helpful comments on earlier drafts.
Agirre and Martinez (2001) and Zheng et al (2007) adopted the Class-only Model in research, while in McCarthy and Carroll (2003) and Merlo and Stevenson (2001) the Word-Class Model is employed. $$$$$ There is a significant limitation to the word tokens that can be disambiguated using selectional preferences, in that they are restricted to those that occur in the specified grammatical relations and in argument head position.

McCarthy and Carroll (2003) reports that the Word-Class Model performs well in unsupervised WSD. $$$$$ The first sense heuristic, with respect to SemCor, outperforms the selectional preferences when it is averaged over a given text.
McCarthy and Carroll (2003) reports that the Word-Class Model performs well in unsupervised WSD. $$$$$ In these cases evidence for a sense can be taken from multiple occurrences of the word in the document, using the one-sense-per-discourse heuristic.
McCarthy and Carroll (2003) reports that the Word-Class Model performs well in unsupervised WSD. $$$$$ We are also investigating the disambiguation of verb senses in running text before subcategorization information for the verbs is acquired, in order to produce a subcategorization lexicon specific to sense (Preiss and Korhonen 2002).
McCarthy and Carroll (2003) reports that the Word-Class Model performs well in unsupervised WSD. $$$$$ Selectional preferences work well for some word combinations and grammatical relationships, but not well for others.

One known unsupervised learning approach for WSD in SP is McCarthy and Carroll (2003) which addresses the issue via conditional probability. $$$$$ Although the preferences perform well in comparison with other unsupervised WSD systems on the same corpus, the results show that for many applications, further knowledge sources would be required to achieve an adequate level of accuracy and coverage.
One known unsupervised learning approach for WSD in SP is McCarthy and Carroll (2003) which addresses the issue via conditional probability. $$$$$ In Figure 6 we show how our system fared at the time of SENSEVAL-2 compared to other unsupervised systems.9 We have also plotted the results of the supervised systems and the precision and recall achieved by using the most frequent sense (as listed in WordNet).10 In the work reported here, we attempted disambiguation for head nouns and verbs in subject and direct object relationships, and for adjectives and nouns in adjectivenoun relationships.
One known unsupervised learning approach for WSD in SP is McCarthy and Carroll (2003) which addresses the issue via conditional probability. $$$$$ Thus, to disambiguate a noun occurring in a given relationship with a given verb, the nc E Cn that gives the largest estimate for p(nc|vc,gr) is taken, where the verb class (vc) is that which maximizes this estimate from Cv.
One known unsupervised learning approach for WSD in SP is McCarthy and Carroll (2003) which addresses the issue via conditional probability. $$$$$ The selectional preferences are specific to verb or adjective classes, rather than individual word forms, so they can be used to disambiguate the co-occurring adjectives and verbs, rather than just the nominal argument heads.

This experiment is used as baseline as the approach is also used in McCarthy and Carroll (2003) for verb and adjective disambiguation. $$$$$ The estimate for p(nc|vc,gr) is taken as in equation (16) but selecting the vc to maximize the estimate for p(vc|nc,gr) rather than p(nc|vc,gr).
This experiment is used as baseline as the approach is also used in McCarthy and Carroll (2003) for verb and adjective disambiguation. $$$$$ The selectional preferences are specific to verb or adjective classes, rather than individual word forms, so they can be used to disambiguate the co-occurring adjectives and verbs, rather than just the nominal argument heads.
This experiment is used as baseline as the approach is also used in McCarthy and Carroll (2003) for verb and adjective disambiguation. $$$$$ Although the preferences perform well in comparison with other unsupervised WSD systems on the same corpus, the results show that for many applications, further knowledge sources would be required to achieve an adequate level of accuracy and coverage.
This experiment is used as baseline as the approach is also used in McCarthy and Carroll (2003) for verb and adjective disambiguation. $$$$$ We also investigate use of the one-senseper-discourse heuristic to propagate a sense tag for a word to other occurrences of the same word within the current document in order to increase coverage.

McCarthy and Carroll (2003) also uses this type of redundancy for disambiguation in SP. $$$$$ For example, if subcategorization were acquired specific to sense, rather than verb form, then distinct senses of fire could have different subcategorization entries: Selectional preferences could also then be acquired automatically from sense-tagged data in an iterative approach (McCarthy 2001).
McCarthy and Carroll (2003) also uses this type of redundancy for disambiguation in SP. $$$$$ The probability at the hypernym nc' will necessarily total the probability at all hyponyms, since the frequency credit of hyponyms is propagated to hypernyms.
McCarthy and Carroll (2003) also uses this type of redundancy for disambiguation in SP. $$$$$ We define a TCM as follows.
McCarthy and Carroll (2003) also uses this type of redundancy for disambiguation in SP. $$$$$ In addition to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance.

The approach is also used in (McCarthy and Carroll 2003) to disambiguate verbs and adjectives in collocations. $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.
The approach is also used in (McCarthy and Carroll 2003) to disambiguate verbs and adjectives in collocations. $$$$$ The probability at the hypernym nc' will necessarily total the probability at all hyponyms, since the frequency credit of hyponyms is propagated to hypernyms.
The approach is also used in (McCarthy and Carroll 2003) to disambiguate verbs and adjectives in collocations. $$$$$ Although the preferences perform well in comparison with other unsupervised WSD systems on the same corpus, the results show that for many applications, further knowledge sources would be required to achieve an adequate level of accuracy and coverage.
The approach is also used in (McCarthy and Carroll 2003) to disambiguate verbs and adjectives in collocations. $$$$$ One subtask in this application involves substituting words with thier more frequent synonyms, for example, substituting letter for missive.

To be particular, the method used by McCarthy and Carroll (2003) is formula (6). $$$$$ For example, one target noun was letter, which occurred as the direct object of sign in our parses of the SENSEVAL-2 data.
To be particular, the method used by McCarthy and Carroll (2003) is formula (6). $$$$$ The probability estimate for each n is obtained using the estimates for all the nss that n has.
To be particular, the method used by McCarthy and Carroll (2003) is formula (6). $$$$$ The main differences between that work and ours are that we acquire adjective as well as verb models, and also that our models are with respect to verb and adjective classes, rather than forms.
To be particular, the method used by McCarthy and Carroll (2003) is formula (6). $$$$$ The selectional preferences are specific to verb or adjective classes, rather than individual word forms, so they can be used to disambiguate the co-occurring adjectives and verbs, rather than just the nominal argument heads.

For example: McCarthy and Carroll (2003) use Li and Abe's method in a word sense disambiguation setting; Schulteim Walde et al (2008) use their MDL approach as part of a system for syntactic and semantic subcategorisation frame learning; Shutova (2010) deploys Resnik's method for metaphor interpretation. $$$$$ For each test instance, we applied subject preferences before direct object preferences, and direct object preferences before adjective–noun preferences.
For example: McCarthy and Carroll (2003) use Li and Abe's method in a word sense disambiguation setting; Schulteim Walde et al (2008) use their MDL approach as part of a system for syntactic and semantic subcategorisation frame learning; Shutova (2010) deploys Resnik's method for metaphor interpretation. $$$$$ We would also like to thank David Weir and Mark Mclauchlan for useful discussions.
For example: McCarthy and Carroll (2003) use Li and Abe's method in a word sense disambiguation setting; Schulteim Walde et al (2008) use their MDL approach as part of a system for syntactic and semantic subcategorisation frame learning; Shutova (2010) deploys Resnik's method for metaphor interpretation. $$$$$ Li and Abe used TCMs for the task of structural disambiguation.

Examples include information retrieval (Manning et al, 2008), word sense discrimination (Schtze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. $$$$$ The estimate for p(nc|vc,gr) is taken as in equation (16) but selecting the vc to maximize the estimate for p(vc|nc,gr) rather than p(nc|vc,gr).
Examples include information retrieval (Manning et al, 2008), word sense discrimination (Schtze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. $$$$$ An adjective is likewise disambiguated to the ac from all those to which the adjective belongs, using the estimate for p(nc|ac,gr) and selecting the nc that maximizes the p(ac|nc,gr) estimate.
Examples include information retrieval (Manning et al, 2008), word sense discrimination (Schtze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. $$$$$ We would also like to thank David Weir and Mark Mclauchlan for useful discussions.
Examples include information retrieval (Manning et al, 2008), word sense discrimination (Schtze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. $$$$$ There are two applications for WSD that we have in mind and are directing our research.

McCarthy and Carroll (2003) have shown the effectiveness of the selectional preference information for WSD. $$$$$ The results show that disambiguation models using only selectional preferences can perform with accuracy well above the random baseline, although accuracy would not be high enough for applications in the absence of other knowledge sources (Stevenson and Wilks 2001).
McCarthy and Carroll (2003) have shown the effectiveness of the selectional preference information for WSD. $$$$$ In Figure 6 we show how our system fared at the time of SENSEVAL-2 compared to other unsupervised systems.9 We have also plotted the results of the supervised systems and the precision and recall achieved by using the most frequent sense (as listed in WordNet).10 In the work reported here, we attempted disambiguation for head nouns and verbs in subject and direct object relationships, and for adjectives and nouns in adjectivenoun relationships.
McCarthy and Carroll (2003) have shown the effectiveness of the selectional preference information for WSD. $$$$$ We also investigate use of the one-senseper-discourse heuristic to propagate a sense tag for a word to other occurrences of the same word within the current document in order to increase coverage.
McCarthy and Carroll (2003) have shown the effectiveness of the selectional preference information for WSD. $$$$$ We would also like to thank David Weir and Mark Mclauchlan for useful discussions.

McCarthy and Carroll (2003) also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes. $$$$$ Although the precision for nouns is greater than that for verbs, the difference is much less when we remove the trivial monosemous cases.
McCarthy and Carroll (2003) also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes. $$$$$ Although the preferences perform well in comparison with other unsupervised WSD systems on the same corpus, the results show that for many applications, further knowledge sources would be required to achieve an adequate level of accuracy and coverage.
McCarthy and Carroll (2003) also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes. $$$$$ This work was supported by UK EPSRC project GR/N36493 “Robust Accurate Statistical Parsing (RASP)” and EU FW5 project IST-2001-34460 “MEANING.” We are grateful to Rob Koeling and three anonymous reviewers for their helpful comments on earlier drafts.
McCarthy and Carroll (2003) also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes. $$$$$ We entered a previous version of this system for the SENSEVAL-2 exercise, in three variants, under the names “sussex-sel” (selectional preferences), “sussex-sel-ospd” (with the OSPD heuristic), and “sussex-sel-ospd-ana” (with anaphora resolution).

The most similar approach to the one we describe, that has been tested on Senseval-2, is the one described in (McCarthy and Carroll, 2003). $$$$$ Li and Abe used TCMs for the task of structural disambiguation.
The most similar approach to the one we describe, that has been tested on Senseval-2, is the one described in (McCarthy and Carroll, 2003). $$$$$ In normal running text, however, a large proportion of word tokens do not fall at these slots.
The most similar approach to the one we describe, that has been tested on Senseval-2, is the one described in (McCarthy and Carroll, 2003). $$$$$ The first application is text simplification, as outlined by Carroll, Minnen, Pearce et al. (1999).
