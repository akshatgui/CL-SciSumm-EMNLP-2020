This is a simple way of including syntactic information in a phrase-based model, and has also been suggested by Hassan et al (2007). $$$$$ These significant improvements indicate that the rich information in supertags helps select better translation candidates.
This is a simple way of including syntactic information in a phrase-based model, and has also been suggested by Hassan et al (2007). $$$$$ All systems were trained on the same parallel data.
This is a simple way of including syntactic information in a phrase-based model, and has also been suggested by Hassan et al (2007). $$$$$ Both supertaggers achieve a supertagging accuracy of 90–92%.
This is a simple way of including syntactic information in a phrase-based model, and has also been suggested by Hassan et al (2007). $$$$$ In this work we show that incorporating lexical syntactic descriptions in the form of supertags can yield significantly better PBSMT systems.

For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation. $$$$$ This result compares favourably with the best systems on the NIST 2005 Arabic–English task.
For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation. $$$$$ However, unlike in rule- and example-based MT, it has proven difficult to date to incorporate linguistic, syntactic knowledge in order to improve translation quality.
For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation. $$$$$ The LTAG system improves by 1.17 BLEU points (2.6% relative), but the CCG system gives an even larger increase: 1.91 BLEU points (4.3% relative).

Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. $$$$$ In order to LTAG (Chen et al., 2006) and the CCG supertag extract xRS rules, the word-to-word alignment insets (Hockenmaier, 2003) were acquired from the duced from the parallel training corpus is used to WSJ section of the Penn-II Treebank using hand- guide heuristic tree ‘cutting’ criteria. built extraction rules.
Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. $$$$$ We compared the translation quality of the baseline systems with the LTAG and CCG supertags systems (LTAG-SMALL and CCG-SMALL).
Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. $$$$$ Obviously, more informed ways of combining the two could result in better performance than a simple log-linear interpolation of the components.

Supertagging (Hassan et al, 2007b) $$$$$ In this section we present a number of experiments that demonstrate the effect of lexical syntax on translation quality.
Supertagging (Hassan et al, 2007b) $$$$$ We expect more work on system integration to improve results still further, and anticipate that similar increases are to be seen for other language pairs.
Supertagging (Hassan et al, 2007b) $$$$$ In section 4, we detail our ap- a parse).

We have previously shown this approach to be very effective for both case and punctuation restoration (Hassan et al, 2007a). $$$$$ Table 1 presents the BLEU scores (Papineni et al., 2002) of both systems on the NIST 2005 MT Evaluation test set.
We have previously shown this approach to be very effective for both case and punctuation restoration (Hassan et al, 2007a). $$$$$ Hence, we may separate order from content:
We have previously shown this approach to be very effective for both case and punctuation restoration (Hassan et al, 2007a). $$$$$ We would like to thank Srinivas Bangalore and the anonymous reviewers for useful comments on earlier versions of this paper.

(Huang and Knight, 2006) and (Hassan et al, 2007) introduce relabeling and supertagging on the target side, respectively. $$$$$ As commonly done in PBSMT, we interpolate these models log-linearly (using different A weights) together with a word penalty weight which allows for control over the length of the target sentence t: For convenience of notation, the interpolation factor for the bag of phrases translation model is shown in formula (3) at the phrase level (but that does not entail any difference).
(Huang and Knight, 2006) and (Hassan et al, 2007) introduce relabeling and supertagging on the target side, respectively. $$$$$ Data and Settings The experiments were conducted for Arabic to English translation and tested on the NIST 2005 evaluation set.
(Huang and Knight, 2006) and (Hassan et al, 2007) introduce relabeling and supertagging on the target side, respectively. $$$$$ This work is partially funded by Science Foundation Ireland Principal Investigator Award 05/IN/1732, and Netherlands Organization for Scientific Research (NWO) VIDI Award.

Two kinds of supertags, from Lexicalized Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG), have been used as lexical syntactic descriptions (Hassan et al, 2007) for phrase based SMT (Koehn et al, 2007). $$$$$ Until quite recently, extending Phrase-based Statistical Machine Translation (PBSMT) with syntactic structure caused system performance to deteriorate.
Two kinds of supertags, from Lexicalized Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG), have been used as lexical syntactic descriptions (Hassan et al, 2007) for phrase based SMT (Koehn et al, 2007). $$$$$ This work is partially funded by Science Foundation Ireland Principal Investigator Award 05/IN/1732, and Netherlands Organization for Scientific Research (NWO) VIDI Award.
Two kinds of supertags, from Lexicalized Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG), have been used as lexical syntactic descriptions (Hassan et al, 2007) for phrase based SMT (Koehn et al, 2007). $$$$$ In this work, we have presented a novel model of PBSMT which integrates supertags into the target language model and the target side of the translation model.

Birch et al (2007) and Hassan et al (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). $$$$$ SMT practitioners have on the whole found it difficult to integrate syntax into their systems.
Birch et al (2007) and Hassan et al (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). $$$$$ We expect more work on system integration to improve results still further, and anticipate that similar increases are to be seen for other language pairs.
Birch et al (2007) and Hassan et al (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). $$$$$ Baseline System The baseline system is a stateof-the-art PBSMT system as described in section 3.
Birch et al (2007) and Hassan et al (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). $$$$$ The LTAG-based supertagger of (Bangalore & Joshi, 1999) is a standard HMM tagger and consists of a (second-order) Markov language model over supertags and a lexical model conditioning the probability of every word on its own supertag (just like standard HMM-based POS taggers).

Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. $$$$$ We perform various experiments on the Arabic to English NIST 2005 test set addressing issues such as sparseness, scalability and the utility of system subcomponents.
Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. $$$$$ We would like to thank Srinivas Bangalore and the anonymous reviewers for useful comments on earlier versions of this paper.

Hassan et al (2007) improve the statistical phrase based MT model by injecting supertags, lexical information such as the POS tag of the word and its subcategorization information, into the phrase table, resulting in generalized phrases with placeholders in them. $$$$$ We expect more work on system integration to improve results still further, and anticipate that similar increases are to be seen for other language pairs.
Hassan et al (2007) improve the statistical phrase based MT model by injecting supertags, lexical information such as the POS tag of the word and its subcategorization information, into the phrase table, resulting in generalized phrases with placeholders in them. $$$$$ We performed a number of experiments to examine the effect of supertagging approaches (CCG or LTAG) with varying data sizes.
Hassan et al (2007) improve the statistical phrase based MT model by injecting supertags, lexical information such as the POS tag of the word and its subcategorization information, into the phrase table, resulting in generalized phrases with placeholders in them. $$$$$ Interestingly, combining phrase-pair a set of target language syntactic structhe two taggers together diminishes the benefits of tures based on supertag sequences.

Hassan et al (2007) and Birch et al (2007) use supertag n-gram LMs. $$$$$ SMT practitioners have on the whole found it difficult to integrate syntax into their systems.
Hassan et al (2007) and Birch et al (2007) use supertag n-gram LMs. $$$$$ We would like to thank Srinivas Bangalore and the anonymous reviewers for useful comments on earlier versions of this paper.
Hassan et al (2007) and Birch et al (2007) use supertag n-gram LMs. $$$$$ Table 6 compares the results of the baseline, the CCG with an n-gram LM-only system (CCG-LARGE) and CCG-LARGE with this ‘grammaticalized’ LM system (CCG-LARGE-GRAM).

This analysis then lets us abstract and encode many local and some nonlocal syntactic structures as complex tags (dynamically, as opposed to the static complex tags as proposed by Birch et al (2007) and Hassan et al (2007)). $$$$$ Data and Settings The experiments were conducted for Arabic to English translation and tested on the NIST 2005 evaluation set.
This analysis then lets us abstract and encode many local and some nonlocal syntactic structures as complex tags (dynamically, as opposed to the static complex tags as proposed by Birch et al (2007) and Hassan et al (2007)). $$$$$ Figure 3 shows some example system output.
This analysis then lets us abstract and encode many local and some nonlocal syntactic structures as complex tags (dynamically, as opposed to the static complex tags as proposed by Birch et al (2007) and Hassan et al (2007)). $$$$$ SMT practitioners have on the whole found it difficult to integrate syntax into their systems.
This analysis then lets us abstract and encode many local and some nonlocal syntactic structures as complex tags (dynamically, as opposed to the static complex tags as proposed by Birch et al (2007) and Hassan et al (2007)). $$$$$ In this work, we have presented a novel model of PBSMT which integrates supertags into the target language model and the target side of the translation model.

A similar approach based on supertagging was proposed by Hassan et al (2007). They used both CCG supertags and LTAG supertags in Arabic-to-English phrase-based translation and have reported about 6% relative improvement in BLEU scores. $$$$$ Both supertaggers achieve a supertagging accuracy of 90–92%.
A similar approach based on supertagging was proposed by Hassan et al (2007). They used both CCG supertags and LTAG supertags in Arabic-to-English phrase-based translation and have reported about 6% relative improvement in BLEU scores. $$$$$ The LTAG-based supertagger of (Bangalore & Joshi, 1999) is a standard HMM tagger and consists of a (second-order) Markov language model over supertags and a lexical model conditioning the probability of every word on its own supertag (just like standard HMM-based POS taggers).
A similar approach based on supertagging was proposed by Hassan et al (2007). They used both CCG supertags and LTAG supertags in Arabic-to-English phrase-based translation and have reported about 6% relative improvement in BLEU scores. $$$$$ The bidirectional word alignment is used to obtain lexical phrase translation pairs using heuristics presented in (Och & Ney, 2003) and (Koehn et al., 2003).

Hassan et al (2007) noticed that the target side POS sequences could be scored, much as we do in this work. $$$$$ This result compares favourably with the best systems on the NIST 2005 Arabic–English task.
Hassan et al (2007) noticed that the target side POS sequences could be scored, much as we do in this work. $$$$$ This result compares favourably with the best systems on the NIST 2005 Arabic–English task.
Hassan et al (2007) noticed that the target side POS sequences could be scored, much as we do in this work. $$$$$ All systems were trained on the same parallel data.
