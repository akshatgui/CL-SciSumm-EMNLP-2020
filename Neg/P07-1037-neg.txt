This is a simple way of including syntactic information in a phrase-based model, and has also been suggested by Hassan et al (2007). $$$$$ For a supertag sequence of length (L) which has (V ) operator violations (as measured by the CCG system), the language model P will be adjusted as P* = P x (1 − �i ).
This is a simple way of including syntactic information in a phrase-based model, and has also been suggested by Hassan et al (2007). $$$$$ Relative to using supertagging seen with the individual LTAG and arbitrary parse-chunks, the power of supertags lies CCG systems.
This is a simple way of including syntactic information in a phrase-based model, and has also been suggested by Hassan et al (2007). $$$$$ Chiang’s derived grammar does not rely on extend with supertags in the next section.

For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation. $$$$$ This result compares favourably with the best systems on the NIST 2005 Arabic–English task.
For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation. $$$$$ This work is partially funded by Science Foundation Ireland Principal Investigator Award 05/IN/1732, and Netherlands Organization for Scientific Research (NWO) VIDI Award.
For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation. $$$$$ While (Chiang, 2005) avails of structure which is not linguistically motivated, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table.
For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation. $$$$$ For example, our supertag-enriched target phrases need not be (Koehn et al., 2003) demonstrated that adding syn- generalized into (xRS or any other) rules that work tax actually harmed the quality of their SMT system. with abstract categories.

Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. $$$$$ SMT practitioners have on the whole found it difficult to integrate syntax into their systems.
Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. $$$$$ We expect more work on system integration to improve results still further, and anticipate that similar increases are to be seen for other language pairs.
Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. $$$$$ Firstly, supertags are rich syntactic constructs that exist for individual words and so they are easy to integrate into SMT models that can be based on any level of granularity, be it wordor phrase-based.
Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. $$$$$ Our best result (0.4688 BLEU) improves by 6.1% relative to a state-of-theart PBSMT model, which compares very favourably with the leading systems on the NIST 2005 task.

Supertagging (Hassan et al, 2007b): incorporating lexical syntactic descriptions, in the form of supertags, to the language model and target side of the translation model in order to better inform decoding. $$$$$ In other words, x = (0x� Ox), where 0x stands for the bag of phrases that constitute x, and Ox for the order of the phrases as given in x (Ox can be implemented as a function from a bag of tokens 0x to a set with a finite number of positions).
Supertagging (Hassan et al, 2007b): incorporating lexical syntactic descriptions, in the form of supertags, to the language model and target side of the translation model in order to better inform decoding. $$$$$ SMT practitioners have on the whole found it difficult to integrate syntax into their systems.
Supertagging (Hassan et al, 2007b): incorporating lexical syntactic descriptions, in the form of supertags, to the language model and target side of the translation model in order to better inform decoding. $$$$$ In this work, we have presented a novel model of PBSMT which integrates supertags into the target language model and the target side of the translation model.

We have previously shown this approach to be very effective for both case and punctuation restoration (Hassan et al, 2007a). $$$$$ Our best result (0.4688 BLEU) improves by 6.1% relative to a state-of-theart PBSMT model, which compares very favourably with the leading systems on the NIST 2005 task.
We have previously shown this approach to be very effective for both case and punctuation restoration (Hassan et al, 2007a). $$$$$ As in most state-of-the-art PBSMT systems, we use GIZA++ to obtain word-level alignments in both language directions.
We have previously shown this approach to be very effective for both case and punctuation restoration (Hassan et al, 2007a). $$$$$ In this work we employ the lexical entries but exchange the algebraic combinatory operators with the more robust and efficient supertagging approach: like standard English.
We have previously shown this approach to be very effective for both case and punctuation restoration (Hassan et al, 2007a). $$$$$ We would like to thank Srinivas Bangalore and the anonymous reviewers for useful comments on earlier versions of this paper.

(Huang and Knight, 2006) and (Hassan et al, 2007) introduce relabeling and supertagging on the target side, respectively. $$$$$ Using LTAG supertags gives the best improvement over a state-of-the-art PBSMT system for a smaller data set, while CCG supertags work best on a large 2 million-sentence pair training set.
(Huang and Knight, 2006) and (Hassan et al, 2007) introduce relabeling and supertagging on the target side, respectively. $$$$$ We would like to thank Srinivas Bangalore and the anonymous reviewers for useful comments on earlier versions of this paper.
(Huang and Knight, 2006) and (Hassan et al, 2007) introduce relabeling and supertagging on the target side, respectively. $$$$$ In our model we employ the CCG supertagger to obtain the best sequences of supertags for a corpus of sentences from which we obtain language model statistics.

Two kinds of supertags, from Lexicalized Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG), have been used as lexical syntactic descriptions (Hassan et al, 2007) for phrase based SMT (Koehn et al, 2007). $$$$$ This work is partially funded by Science Foundation Ireland Principal Investigator Award 05/IN/1732, and Netherlands Organization for Scientific Research (NWO) VIDI Award.
Two kinds of supertags, from Lexicalized Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG), have been used as lexical syntactic descriptions (Hassan et al, 2007) for phrase based SMT (Koehn et al, 2007). $$$$$ This work is partially funded by Science Foundation Ireland Principal Investigator Award 05/IN/1732, and Netherlands Organization for Scientific Research (NWO) VIDI Award.
Two kinds of supertags, from Lexicalized Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG), have been used as lexical syntactic descriptions (Hassan et al, 2007) for phrase based SMT (Koehn et al, 2007). $$$$$ Both supertaggers achieve a supertagging accuracy of 90–92%.

Birch et al (2007) and Hassan et al (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). $$$$$ While this is slightly lower than the 4.9% relative improvement with the smaller data sets, the sustained increase is probably due to observing more data with different supertag contexts, which enables the model to select better target language phrases.
Birch et al (2007) and Hassan et al (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). $$$$$ Section 6 cessing (using Markov models, for instance) which concludes, and provides avenues for further work. does not necessarily aim at building a fully con2 Related Work nected graph.
Birch et al (2007) and Hassan et al (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). $$$$$ Then we extract phrase-pairs together with the cooccuring English supertag sequence from this corpus via the same phrase extraction method used in the baseline model.
Birch et al (2007) and Hassan et al (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). $$$$$ We would like to thank Srinivas Bangalore and the anonymous reviewers for useful comments on earlier versions of this paper.

Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. $$$$$ We perform various experiments on the Arabic to English NIST 2005 test set addressing issues such as sparseness, scalability and the utility of system subcomponents.
Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. $$$$$ This is the main difference with full parsing: supertagging the input utterance need not result in a fully connected graph.
Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. $$$$$ The language model is further smoothed by log-linear interpolation with the baseline language model over word sequences.

Hassan et al (2007) improve the statistical phrase based MT model by injecting supertags, lexical information such as the POS tag of the word and its subcategorization information, into the phrase table, resulting in generalized phrases with placeholders in them. $$$$$ Figure 2).
Hassan et al (2007) improve the statistical phrase based MT model by injecting supertags, lexical information such as the POS tag of the word and its subcategorization information, into the phrase table, resulting in generalized phrases with placeholders in them. $$$$$ We see that bringing the grammaticality tests to bear onto the supertagged system gives a further improvement of 0.79 BLEU points, a 1.7% relative increase, culminating in an overall increase of 2.7 BLEU points, or a 6.1% relative improvement over the baseline system.
Hassan et al (2007) improve the statistical phrase based MT model by injecting supertags, lexical information such as the POS tag of the word and its subcategorization information, into the phrase table, resulting in generalized phrases with placeholders in them. $$$$$ Adding grammaticality factors based on algebraic compositional operators gives the best result, namely 0.4688 BLEU, or a 6.1% relative increase over the baseline.
Hassan et al (2007) improve the statistical phrase based MT model by injecting supertags, lexical information such as the POS tag of the word and its subcategorization information, into the phrase table, resulting in generalized phrases with placeholders in them. $$$$$ SMT practitioners have on the whole found it difficult to integrate syntax into their systems.

Hassan et al (2007) and Birch et al (2007) use supertag n-gram LMs. $$$$$ This work is partially funded by Science Foundation Ireland Principal Investigator Award 05/IN/1732, and Netherlands Organization for Scientific Research (NWO) VIDI Award.
Hassan et al (2007) and Birch et al (2007) use supertag n-gram LMs. $$$$$ Interestingly, combining phrase-pair a set of target language syntactic structhe two taggers together diminishes the benefits of tures based on supertag sequences.
Hassan et al (2007) and Birch et al (2007) use supertag n-gram LMs. $$$$$ We perform various experiments on the Arabic to English NIST 2005 test set addressing issues such as sparseness, scalability and the utility of system subcomponents.
Hassan et al (2007) and Birch et al (2007) use supertag n-gram LMs. $$$$$ Any (target or source) sentence x will consist of two parts: a bag of elements (words/phrases etc.) and an order over that bag.

This analysis then lets us abstract and encode many local and some nonlocal syntactic structures as complex tags (dynamically, as opposed to the static complex tags as proposed by Birch et al (2007) and Hassan et al (2007)). $$$$$ This work is partially funded by Science Foundation Ireland Principal Investigator Award 05/IN/1732, and Netherlands Organization for Scientific Research (NWO) VIDI Award.
This analysis then lets us abstract and encode many local and some nonlocal syntactic structures as complex tags (dynamically, as opposed to the static complex tags as proposed by Birch et al (2007) and Hassan et al (2007)). $$$$$ Our best result (0.4688 BLEU) improves by 6.1% relative to a state-of-theart PBSMT model, which compares very favourably with the leading systems on the NIST 2005 task.
This analysis then lets us abstract and encode many local and some nonlocal syntactic structures as complex tags (dynamically, as opposed to the static complex tags as proposed by Birch et al (2007) and Hassan et al (2007)). $$$$$ Next we define the probabilistic model that accompanies this syntactic enrichment of the baseline model.
This analysis then lets us abstract and encode many local and some nonlocal syntactic structures as complex tags (dynamically, as opposed to the static complex tags as proposed by Birch et al (2007) and Hassan et al (2007)). $$$$$ This work is partially funded by Science Foundation Ireland Principal Investigator Award 05/IN/1732, and Netherlands Organization for Scientific Research (NWO) VIDI Award.

A similar approach based on supertagging was proposed by Hassan et al (2007). They used both CCG supertags and LTAG supertags in Arabic-to-English phrase-based translation and have reported about 6% relative improvement in BLEU scores. $$$$$ Using LTAG supertags gives the best improvement over a state-of-the-art PBSMT system for a smaller data set, while CCG supertags work best on a large 2 million-sentence pair training set.
A similar approach based on supertagging was proposed by Hassan et al (2007). They used both CCG supertags and LTAG supertags in Arabic-to-English phrase-based translation and have reported about 6% relative improvement in BLEU scores. $$$$$ We would like to thank Srinivas Bangalore and the anonymous reviewers for useful comments on earlier versions of this paper.
A similar approach based on supertagging was proposed by Hassan et al (2007). They used both CCG supertags and LTAG supertags in Arabic-to-English phrase-based translation and have reported about 6% relative improvement in BLEU scores. $$$$$ Secondly, supertags specify the local syntactic constraints for a word, which resonates well with sequential (finite state) statistical (e.g.

Hassan et al (2007) noticed that the target side POS sequences could be scored, much as we do in this work. $$$$$ Until quite recently, extending Phrase-based Statistical Machine Translation (PBSMT) with syntactic structure caused system performance to deteriorate.
Hassan et al (2007) noticed that the target side POS sequences could be scored, much as we do in this work. $$$$$ SMT practitioners have on the whole found it difficult to integrate syntax into their systems.
Hassan et al (2007) noticed that the target side POS sequences could be scored, much as we do in this work. $$$$$ We would like to thank Srinivas Bangalore and the anonymous reviewers for useful comments on earlier versions of this paper.
Hassan et al (2007) noticed that the target side POS sequences could be scored, much as we do in this work. $$$$$ Chiang’s derived grammar does not rely on extend with supertags in the next section.
